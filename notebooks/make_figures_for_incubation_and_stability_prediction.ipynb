{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e237023",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing requirements\n",
    "import pandas as pd\n",
    "# 1. Copy all .h5 and .mzML files from the job folders to a single output directory for easier access.\n",
    "# 2. Run the deconvolution to make <filename>_deconvoluted.parquet files.\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "# import ztest\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import pdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4eb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_data = annotation_engine.load_envnet_reference(\n",
    "#     graphml_file=os.path.join(ref_dir, \"network_with_sirius.graphml\"),\n",
    "#     mgf_base_name=os.path.join(ref_dir, \"envnet\")\n",
    "# )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588438ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHONPATH = \"/global/homes/b/bpb/repos/envnet\"\n",
    "if PYTHONPATH not in sys.path:\n",
    "    sys.path.insert(0, PYTHONPATH)\n",
    "\n",
    "from envnet.annotation.core import AnnotationEngine\n",
    "annotation_engine = AnnotationEngine()\n",
    "ref_dir = '/global/homes/b/bpb/repos/envnet/results/full_build_20250908_181404/'\n",
    "node_data = annotation_engine.load_envnet_reference(\n",
    "    graphml_file=os.path.join(ref_dir, \"network_with_sirius.graphml\"),\n",
    "    mgf_base_name=os.path.join(ref_dir, \"envnet\")\n",
    ")   \n",
    "cols = ['original_index','precursor_mz','inchi_key', 'compound_name', 'smiles','NPC#pathway', 'NPC#superclass', 'NPC#class','predicted_formula', 'dbe', 'dbe_ai',\n",
    "       'dbe_ai_mod', 'ai_mod', 'ai', 'nosc', 'h_to_c', 'o_to_c', 'n_to_c',\n",
    "       'p_to_c', 'c', 'h', 'o', 'n', 's', 'p']\n",
    "node_data = node_data['nodes'][cols]\n",
    "\n",
    "model_cols = ['original_index', 'predicted_unchanged_in_soil_prob', 'predicted_unchanged_in_soil']\n",
    "model_data = pd.read_csv('../envnet/data/node_data_with_predicted_unchanged_20251107.csv', usecols=model_cols)\n",
    "node_data = node_data.merge(model_data, on='original_index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65821cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# job_folders = ['egsb/20250415_EB_MdR_109570-002_LigDiv6_20250122_QE119_C18-EP_USDAY92790',\n",
    "#                'egsb/20231018_EB_MdR_109570-002_WAVEstab_20231017_EXP120A_C18-EP_USDAY72349_vols',\n",
    "#                'egsb/20240125_EB_MdR_101544-059_WAVESTAB3_20231222_EXP120A_C18-EP_USDAY72349',\n",
    "#                'jgi/20220707_JGI_SB_503799_Permafrost_pilot_QE-HF_C18_USDAY63663',\n",
    "#                'jgi/20231004_JGI_SB_503799_Pfrost_final_QEHF_C18_USDAY81384']\n",
    "\n",
    "# data = []\n",
    "# output_dir = '/global/cfs/cdirs/metatlas/projects/envnet_build_files/analysis_for_manuscript'\n",
    "# h5_files = []\n",
    "# for job_folder in job_folders:\n",
    "#     full_dir = os.path.join('/global/cfs/cdirs/metatlas/raw_data', job_folder)\n",
    "#     files = os.listdir(full_dir)\n",
    "#     files = [f for f in files if '_NEG_' in f]\n",
    "#     for f in files:\n",
    "#         if f.endswith('.h5') | f.endswith('.mzML'):\n",
    "#             old_location = os.path.join(full_dir, f)\n",
    "#             new_location = os.path.join(output_dir, f)\n",
    "#             if new_location.endswith('.h5'):\n",
    "#                 h5_files.append(new_location)\n",
    "#             # copy to output dir\n",
    "#             if not os.path.exists(new_location):\n",
    "#                 os.system(f'cp {old_location} {new_location}')\n",
    "#             # data.append({'job_folder': job_folder, 'h5': os.path.join(full_dir, f)})\n",
    "\n",
    "# df = pd.DataFrame(h5_files, columns=['h5'])\n",
    "# df['parquet'] = df['h5'].str.replace('.h5', '_deconvoluted.parquet')\n",
    "# df['environmental_subclass'] = 'not applicable'\n",
    "# df['lcmsrun_observed'] = df['h5'].apply(lambda x: os.path.basename(x).replace('.h5', ''))\n",
    "# df['original_file_type'] = 'h5'\n",
    "# output_filename = '/global/homes/b/bpb/repos/envnet/scripts/input_for_ms1-ms2_annotation-experiments-for-paper.csv'\n",
    "# df.to_csv(output_filename, index=False)\n",
    "# # 3. For each subfolder run the ms1 and ms2 annotation scripts\n",
    "# # This is done by making a input.csv for each experiment that looks like this\n",
    "# # parquet,h5,environmental_subclass,lcmsrun_observed,original_file_type\n",
    "# # /global/cfs/cdirs/metatlas/projects/envnet_build_files/metatlas/20230223_EB_MdR_101544-059_SynDAC_20230223_QE144_C18-EP_USDAY72350_NEG_MS2_29_RS-HA-NA_1__18_deconvoluted.parquet,/global/cfs/cdirs/metatlas/projects/envnet_build_files/metatlas/20230223_EB_MdR_101544-059_SynDAC_20230223_QE144_C18-EP_USDAY72350_NEG_MS2_29_RS-HA-NA_1__18.h5,Suwannee River Humic Acid Standard III,metatlas/20230223_EB_MdR_101544-059_SynDAC_20230223_QE144_C18-EP_USDAY72350_NEG_MS2_29_RS-HA-NA_1__18,h5\n",
    "# # /global/cfs/cdirs/metatlas/projects/envnet_build_files/metatlas/20221219_EB_MdR_101544-059_PyrToDAC_20221219_EXP120A_C18-EP_USDAY63672_NEG_MS2_8_FAIII-1mg-NA_1__48_deconvoluted.parquet,/global/cfs/cdirs/metatlas/projects/envnet_build_files/metatlas/20221219_EB_MdR_101544-059_PyrToDAC_20221219_EXP120A_C18-EP_USDAY63672_NEG_MS2_8_FAIII-1mg-NA_1__48.h5,Suwannee River Fulvic Acid Standard III,metatlas/20221219_EB_MdR_101544-059_PyrToDAC_20221219_EXP120A_C18-EP_USDAY63672_NEG_MS2_8_FAIII-1mg-NA_1__48,h5\n",
    "# #\n",
    "# # and a ms1 annotation sbatch for each that looks like this for each experiment\n",
    "# # /global/homes/b/bpb/repos/envnet/scripts/run_env_sample_ms1-annotation.sbatch\n",
    "# # /global/homes/b/bpb/repos/envnet/scripts/run_env_sample_ms2-annotation.sbatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c314e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1_filename = '/pscratch/sd/b/bpb/envnet_annotation_results/ms1_results_experiments_for_paper/ms1_annotations.parquet'\n",
    "ms2_filename = '/pscratch/sd/b/bpb/envnet_annotation_results/ms2_results_experiments_for_paper/ms2_deconvoluted_annotations.parquet'\n",
    "ms1_df = pd.read_parquet(ms1_filename)\n",
    "# get rid of hte 3uL WAVEstab samples\n",
    "idx1 = ms1_df['lcmsrun_observed'].str.contains('3uL')\n",
    "idx2 = ms1_df['lcmsrun_observed'].str.contains('WAVEstab')\n",
    "idx = idx1 & idx2\n",
    "ms1_df = ms1_df[~idx]\n",
    "\n",
    "# get rid of the LigDiv6 project samples\n",
    "idx = ms1_df['lcmsrun_observed'].str.contains('LigDiv6')\n",
    "ms1_df = ms1_df[~idx]\n",
    "\n",
    "# get rid of the QC samples\n",
    "idx = ms1_df['lcmsrun_observed'].str.contains('QC')\n",
    "ms1_df = ms1_df[~idx]\n",
    "\n",
    "# get rid of the ctrl samples\n",
    "idx = ms1_df['lcmsrun_observed'].str.contains('ctrl', case=False)\n",
    "ms1_df = ms1_df[~idx]\n",
    "\n",
    "print(ms1_df.shape)\n",
    "ms2_cols = ['score_deconvoluted_match', 'matches_deconvoluted_match',\n",
    "       'original_index_deconvoluted_match',  'filename',\n",
    "        'mz_diff']\n",
    "ms2_df = pd.read_parquet(ms2_filename, columns=ms2_cols)\n",
    "ms1_df = pd.merge(ms1_df, ms2_df, left_on=['lcmsrun_observed','original_index'], right_on=['filename','original_index_deconvoluted_match'], how='left')\n",
    "ms1_df.drop(columns=['original_index_deconvoluted_match','filename','confidence_level','h5'], inplace=True)\n",
    "ms1_df['has_ms2_evidence'] = pd.notna(ms1_df['score_deconvoluted_match'])\n",
    "sample_metadata = pd.DataFrame(ms1_df['lcmsrun_observed'].unique(), columns=['lcmsrun_observed'])\n",
    "sample_metadata['basename'] = sample_metadata['lcmsrun_observed'].apply(lambda x: os.path.basename(x))\n",
    "sample_metadata['treatment'] = sample_metadata['basename'].apply(lambda x: x.split('_')[12])\n",
    "\n",
    "\n",
    "print(ms1_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960410b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1_df['original_index'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_metadata = pd.DataFrame(ms1_df['lcmsrun_observed'].unique(), columns=['lcmsrun_observed'])\n",
    "sample_metadata['basename'] = sample_metadata['lcmsrun_observed'].apply(lambda x: os.path.basename(x))\n",
    "sample_metadata['project'] = sample_metadata['basename'].apply(lambda x: x.split('_')[4])\n",
    "sample_metadata['treatment'] = sample_metadata['basename'].apply(lambda x: x.split('_')[12])\n",
    "\n",
    "sample_metadata['timepoint'] = None\n",
    "idx = sample_metadata['treatment'].str.contains('-d7', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'timepoint'] = 7\n",
    "idx = sample_metadata['treatment'].str.contains('-day7', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'timepoint'] = 7\n",
    "idx = sample_metadata['treatment'].str.contains('-d0', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'timepoint'] = 0\n",
    "idx = sample_metadata['treatment'].str.contains('-day0', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'timepoint'] = 0\n",
    "idx = sample_metadata['treatment'].str.contains('-T0-MeOH', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'timepoint'] = 0\n",
    "idx = sample_metadata['treatment'].str.contains('-T2-MeOH', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'timepoint'] = 7\n",
    "\n",
    "sample_metadata['priming'] = None\n",
    "idx = sample_metadata['treatment'].str.endswith('-NA')\n",
    "sample_metadata.loc[idx,'priming'] = 'Unprimed'\n",
    "idx = sample_metadata['treatment'].str.contains('-na-', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'priming'] = 'Unprimed'\n",
    "idx = sample_metadata['treatment'].str.contains('-natcom-salts', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'priming'] = 'Unprimed'\n",
    "idx = sample_metadata['treatment'].str.contains('-natcom-nldm', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'priming'] = 'High'\n",
    "idx = sample_metadata['treatment'].str.endswith('-Lo')\n",
    "sample_metadata.loc[idx,'priming'] = 'Low'\n",
    "idx = sample_metadata['treatment'].str.contains('-0p05xnldm-', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'priming'] = 'Low'\n",
    "idx = sample_metadata['treatment'].str.endswith('-Hi')\n",
    "sample_metadata.loc[idx,'priming'] = 'High'\n",
    "idx = sample_metadata['treatment'].str.contains('-0p5xnldm-', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'priming'] = 'High'\n",
    "idx = sample_metadata['treatment'].str.contains('M-T')\n",
    "sample_metadata.loc[idx,'priming'] = 'Unprimed'\n",
    "\n",
    "sample_metadata['soil_type'] = None\n",
    "idx = sample_metadata['treatment'].str.contains('supern-wave-natcom', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'soil_type'] = 'Potting Soil'\n",
    "idx = sample_metadata['treatment'].str.contains('omt1d2', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'soil_type'] = 'Agricultural Soil'\n",
    "idx = sample_metadata['treatment'].str.contains('h4171', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'soil_type'] = 'H4171 lignin'\n",
    "idx = sample_metadata['treatment'].str.contains('h4161', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'soil_type'] = 'H4161 lignin'\n",
    "idx = sample_metadata['treatment'].str.contains('20M-', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'soil_type'] = '20M permafrost'\n",
    "idx = sample_metadata['treatment'].str.contains('42M-', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'soil_type'] = '42M permafrost'\n",
    "idx = sample_metadata['treatment'].str.contains('55M-', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'soil_type'] = '55M permafrost'\n",
    "idx = sample_metadata['treatment'].str.contains('66M-', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'soil_type'] = '66M permafrost'\n",
    "idx = sample_metadata['treatment'].str.contains('82M-', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'soil_type'] = '82M permafrost'\n",
    "\n",
    "cols = ['timepoint','priming','soil_type']\n",
    "idx = pd.notna(sample_metadata[cols]).all(axis=1)\n",
    "sample_metadata = sample_metadata[idx].copy()\n",
    "sample_metadata.drop(columns=['basename','treatment'], inplace=True)\n",
    "ms1_df = pd.merge(ms1_df, sample_metadata, on='lcmsrun_observed', how='inner')\n",
    "idx = ms1_df['soil_type'].isin(['Potting Soil','Agricultural Soil'])\n",
    "# idx2 = ms1_df['soil_type'].str.contains('permafrost',case=False)\n",
    "# idx = idx1 | idx2\n",
    "ms1_df = ms1_df[idx]\n",
    "print(ms1_df.columns)\n",
    "print(ms1_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99977a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_filter(df,project,column_name):\n",
    "    df = df[~df[column_name].str.contains('exctrl|qc|txctrl',case=False)]\n",
    "    if project == 'Potting Soil':\n",
    "        return df[df[column_name].str.contains('6uL') & df[column_name].str.contains('NatCom') & df[column_name].str.contains('Day0|Day7')]\n",
    "    if project == 'Agricultural Soil NA':\n",
    "        return df[df[column_name].str.contains('NatCom') & df[column_name].str.contains('NA')]\n",
    "    if project == 'Agricultural Soil Lo':\n",
    "        return df[df[column_name].str.contains('NatCom') & df[column_name].str.contains('Lo')]\n",
    "    if project == 'Agricultural Soil':\n",
    "        return df[df[column_name].str.contains('NatCom') & df[column_name].str.contains('Hi')]\n",
    "    elif project == 'soil-ppl':\n",
    "        return df[df[column_name].str.contains('Run15')]\n",
    "    # elif project == 'syncom-exudates':\n",
    "        # return \n",
    "    elif project == 'century-exp':\n",
    "        return df[df[column_name].str.contains('omt|cmt',case=False)]\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "def tost(control, treatment, margin=0.25):\n",
    "    # Perform two one-sided tests.  Note that this\n",
    "    # is specific for testing if treatment is not\n",
    "    # the same as control.  If you control mean is\n",
    "    # near zero this will not work.\n",
    "    m = np.mean(control)\n",
    "    lower_margin =  -1*margin*m\n",
    "    upper_margin = margin*m\n",
    "    _, p_value_lower = ztest(control, treatment, value=lower_margin, alternative='larger')\n",
    "    _, p_value_upper = ztest(control, treatment, value=upper_margin, alternative='smaller')\n",
    "    return max(p_value_lower, p_value_upper)\n",
    "\n",
    "def do_ttest(df,control_group,treatment_group,do_split=True,min_intensity=1e6,margin=0.2):\n",
    "    p = pd.pivot_table(df,index=['lcmsrun_observed','timepoint'],values='peak_area',columns='original_index')\n",
    "    p.fillna(1e5,inplace=True)     \n",
    "    cols = p.columns\n",
    "    p.reset_index(inplace=True,drop=False)\n",
    "\n",
    "    ttest = []\n",
    "    for c in cols:\n",
    "        idx1 = p['timepoint']==treatment_group\n",
    "        idx2 = p['timepoint']==control_group\n",
    "        treatment = p[c][idx1].values\n",
    "        control = p[c][idx2].values\n",
    "        mean_treatment = 1+treatment.mean()\n",
    "        mean_control = 1+control.mean()\n",
    "        if (mean_treatment<min_intensity) & (mean_control<min_intensity):\n",
    "            continue\n",
    "        fold_change = np.log2(mean_treatment/mean_control)\n",
    "        t,p_value = ttest_ind(treatment, control)\n",
    "        # Two One-Sided Tests (TOST) procedure using 50% margin\n",
    "        p_tost = tost(control,treatment, margin=0.5)\n",
    "        ttest.append({'original_index':c,'t':t,'p':p_value,'fc':fold_change,'tost':p_tost,\n",
    "                      'mean_treatment':mean_treatment,'mean_control':mean_control,\n",
    "                      'treatment_vals':treatment,\n",
    "                      'control_vals':control})\n",
    "    ttest = pd.DataFrame(ttest)\n",
    "    return ttest\n",
    "\n",
    "def create_volcano_plot(plot_df, ax, title):\n",
    "    \"\"\"Creates a single volcano plot with clipped fold change.\"\"\"\n",
    "    plot_df = plot_df.copy()\n",
    "    \n",
    "    # Clip fold change to -8 to 8\n",
    "    plot_df['fc'] = plot_df['fc'].clip(-5, 5)\n",
    "    \n",
    "    plot_df['-log10_p'] = -np.log10(plot_df['p'].clip(lower=1e-8))\n",
    "\n",
    "    for classification, color in CLASSIFICATION_COLORS.items():\n",
    "        subset = plot_df[plot_df['classification'] == classification]\n",
    "        if not subset.empty:\n",
    "            ax.scatter(subset['fc'], subset['-log10_p'], c=color, alpha=1, s=35,\n",
    "                       label=classification, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "    unclassified = plot_df[plot_df['classification'].isna()]\n",
    "    if not unclassified.empty:\n",
    "        ax.scatter(unclassified['fc'], unclassified['-log10_p'], c=UNCLASSIFIED_COLOR,\n",
    "                   alpha=1, s=35, label='Unclassified', edgecolors='white', linewidth=0.3)\n",
    "\n",
    "    ax.axhline(y=-np.log10(0.05), color='black', linestyle='--', alpha=0.7, linewidth=1.2)\n",
    "    ax.axvline(x=0.5, color='black', linestyle='--', alpha=0.7, linewidth=1.2)\n",
    "    ax.axvline(x=-0.5, color='black', linestyle='--', alpha=0.7, linewidth=1.2)\n",
    "\n",
    "    # Set x-axis limits to -8 to 8\n",
    "    ax.set_xlim(-5.1, 5.1)\n",
    "    \n",
    "    ax.set_xlabel('Log₂ Fold Change', fontsize=30)\n",
    "    ax.set_ylabel('-Log₁₀(p-value)', fontsize=30)\n",
    "    ax.set_title(title, fontsize=20, pad=5)\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "def truncate_class_names(class_names, max_length=35):\n",
    "    \"\"\"Truncates long compound class names.\"\"\"\n",
    "    return [str(name)[:max_length-3] + '...' if len(str(name)) > max_length else str(name) for name in class_names]\n",
    "\n",
    "\n",
    "def get_combined_class_order(df, node_data, min_compounds):\n",
    "    \"\"\"Gets a consistent order of compound classes using hierarchical clustering.\n",
    "    Only includes classes that have at least min_compounds in EVERY condition.\"\"\"\n",
    "    \n",
    "    # First pass: count compounds per class per condition\n",
    "    class_counts_by_condition = {}\n",
    "    all_classes = set()\n",
    "    \n",
    "    for cond_idx, cond in enumerate(CONDITIONS):\n",
    "        idx = (df['soil_type'] == cond['soil_type'])\n",
    "        if cond['priming'] is not None:\n",
    "            idx &= (df['priming'] == cond['priming'])\n",
    "        plot_df = df[idx]\n",
    "        plot_df = pd.merge(plot_df, node_data, on='original_index', how='left')\n",
    "        \n",
    "        class_counts = plot_df.groupby(compound_class_col)['classification'].count()\n",
    "        class_counts_by_condition[cond_idx] = class_counts.to_dict()\n",
    "        all_classes.update(class_counts.keys())\n",
    "    \n",
    "    # Filter to classes that have min_compounds in EVERY condition\n",
    "    abundant_classes = []\n",
    "    for class_name in all_classes:\n",
    "        meets_threshold = True\n",
    "        for cond_idx in range(len(CONDITIONS)):\n",
    "            count = class_counts_by_condition[cond_idx].get(class_name, 0)\n",
    "            if count < min_compounds:\n",
    "                meets_threshold = False\n",
    "                break\n",
    "        if meets_threshold:\n",
    "            abundant_classes.append(class_name)\n",
    "    \n",
    "    if len(abundant_classes) <= 1:\n",
    "        return abundant_classes\n",
    "    \n",
    "    # Initialize feature matrix: rows = classes, columns = conditions × classifications\n",
    "    n_classes = len(abundant_classes)\n",
    "    n_features = len(CONDITIONS) * 3  # 3 classifications per condition\n",
    "    feature_matrix = np.zeros((n_classes, n_features))\n",
    "    \n",
    "    # Build feature vectors\n",
    "    class_to_idx = {cls: i for i, cls in enumerate(abundant_classes)}\n",
    "    \n",
    "    for cond_idx, cond in enumerate(CONDITIONS):\n",
    "        idx = (df['soil_type'] == cond['soil_type'])\n",
    "        if cond['priming'] is not None:\n",
    "            idx &= (df['priming'] == cond['priming'])\n",
    "        plot_df = df[idx]\n",
    "        plot_df = pd.merge(plot_df, node_data, on='original_index', how='left')\n",
    "        \n",
    "        for class_name in abundant_classes:\n",
    "            class_subset = plot_df[plot_df[compound_class_col] == class_name]\n",
    "            total = len(class_subset)\n",
    "            \n",
    "            if total > 0:\n",
    "                produced_pct = len(class_subset[class_subset['classification'] == 'enriched']) / total\n",
    "                prefered_pct = len(class_subset[class_subset['classification'] == 'depleted']) / total\n",
    "                ignored_pct = len(class_subset[class_subset['classification'] == 'unchanged']) / total\n",
    "            else:\n",
    "                produced_pct, prefered_pct, ignored_pct = 0, 0, 0\n",
    "            \n",
    "            row_idx = class_to_idx[class_name]\n",
    "            col_offset = cond_idx * 3\n",
    "            feature_matrix[row_idx, col_offset] = produced_pct\n",
    "            feature_matrix[row_idx, col_offset + 1] = prefered_pct\n",
    "            feature_matrix[row_idx, col_offset + 2] = ignored_pct\n",
    "    \n",
    "    # Perform hierarchical clustering\n",
    "    linkage_matrix = linkage(feature_matrix, method='ward')\n",
    "    \n",
    "    # Get the order from the dendrogram\n",
    "    dend = dendrogram(linkage_matrix, no_plot=True)\n",
    "    cluster_order = dend['leaves']\n",
    "    \n",
    "    # Return classes in clustered order\n",
    "    return [abundant_classes[i] for i in cluster_order]\n",
    "\n",
    "def prepare_barchart_data(df, global_class_order):\n",
    "    \"\"\"Prepares and normalizes data for a single stacked bar chart.\"\"\"\n",
    "    temp = df.groupby(compound_class_col)['classification'].value_counts().unstack(fill_value=0)\n",
    "    for col in CLASSIFICATION_COLORS:\n",
    "        if col not in temp.columns:\n",
    "            temp[col] = 0\n",
    "    temp = temp[list(CLASSIFICATION_COLORS.keys())]\n",
    "    temp = temp.div(temp.sum(axis=1), axis=0)\n",
    "    temp = temp.reindex(global_class_order).dropna()\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ms1_df = [g for _, g in ms1_df.groupby(['project','priming','soil_type'])]\n",
    "print(len(grouped_ms1_df))\n",
    "out = []\n",
    "for g_df in grouped_ms1_df:\n",
    "    project = g_df['project'].values[0]\n",
    "    priming = g_df['priming'].values[0]\n",
    "    soil_type = g_df['soil_type'].values[0]\n",
    "    print(f'Processing {project} {priming} {soil_type}')\n",
    "    ttest = do_ttest(g_df,0,7,min_intensity=1e7)\n",
    "    ttest['project'] = project\n",
    "    ttest['priming'] = priming\n",
    "    ttest['soil_type'] = soil_type\n",
    "    idx_prefered = (ttest['fc']<-0.5) & (ttest['p']<0.05)\n",
    "    idx_produced = (ttest['fc']>0.5) & (ttest['p']<0.05)\n",
    "    idx_ignored = ttest['tost']<0.05\n",
    "    ttest['classification'] = None\n",
    "    ttest.loc[idx_ignored,'classification'] = 'unchanged'\n",
    "    # overwrite any that are overlapping, prioritize prefered and produced\n",
    "    ttest.loc[idx_prefered,'classification'] = 'depleted'\n",
    "    ttest.loc[idx_produced,'classification'] = 'enriched'\n",
    "    out.append(ttest)\n",
    "final_df = pd.concat(out, ignore_index=True)\n",
    "\n",
    "# merge in ms2 evidence at the project/soil_type level\n",
    "cols = ['has_ms2_evidence', 'original_index', 'project', 'soil_type','priming']\n",
    "ms2_evidence = ms1_df[cols].drop_duplicates(subset=cols[1:])\n",
    "final_df = pd.merge(final_df, ms2_evidence, on=cols[1:], how='left')\n",
    "final_df.to_csv('training data for stability model 20251106.csv', index=False)\n",
    "print(final_df.columns)\n",
    "print(final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830722c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[['project', 'priming',\n",
    "       'soil_type']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e678518",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['original_index'].nunique(),final_df.loc[final_df['has_ms2_evidence']==True,'original_index'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda991d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[['soil_type','classification','priming']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d22388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = p[unchanged_all].copy()\n",
    "temp = temp.loc[pd.notna(temp[c]),c].value_counts()\n",
    "temp['condition'] = c\n",
    "# temp = temp.value_counts()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9775f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p['66M permafrost_Unprimed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070797ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.pivot_table(final_df,columns=['soil_type','priming'],index='original_index',values='classification',fill_value=None,aggfunc='first')\n",
    "p.columns = ['_'.join([str(c) for c in col]).strip() for col in p.columns.values]\n",
    "my_class = 'depleted'\n",
    "idx1 = ((p==my_class) | (p.isna())).sum(axis=1)==10\n",
    "idx2 = (p==my_class).sum(axis=1)>=4\n",
    "idx = idx1 & idx2\n",
    "node_data.loc[node_data['original_index'].isin(p[idx].index),'NPC#class'].value_counts().head(20)\n",
    "node_data[node_data['original_index'].isin(p[idx].index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1919e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.pivot_table(final_df,columns=['soil_type','priming'],index='original_index',values='classification',fill_value=None,aggfunc='first')\n",
    "p.columns = ['_'.join([str(c) for c in col]).strip() for col in p.columns.values]\n",
    "p.reset_index(inplace=True,drop=False)\n",
    "soil_cols = [c for c in p.columns if 'Soil' in c]\n",
    "unchanged_all = p[soil_cols].apply(lambda row: all(row == 'unchanged'), axis=1)\n",
    "pfrost_cols  = [c for c in p.columns if 'permafrost' in c]\n",
    "out = []\n",
    "out_all = []\n",
    "for c in pfrost_cols:\n",
    "    # for unchanged in pfrost sample, check if unchanged in soil samples\n",
    "    temp = p[unchanged_all].copy()\n",
    "    print(p.shape,temp.shape)\n",
    "    temp = temp.loc[pd.notna(temp[c]),c].value_counts()\n",
    "    temp['condition'] = c\n",
    "    out.append(temp)\n",
    "    temp = p.copy()\n",
    "    print(p.shape,temp.shape)\n",
    "    temp = temp.loc[pd.notna(temp[c]),c].value_counts()\n",
    "    temp['condition'] = c\n",
    "    out_all.append(temp)\n",
    "df_unchanged = pd.DataFrame(out)\n",
    "df_all = pd.DataFrame(out_all)\n",
    "df_summary = df_unchanged.merge(df_all, on='condition', suffixes=('_unchanged','_all'))\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20329571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pairwise_available_euclidean_vectorized(X):\n",
    "    n, m = X.shape\n",
    "    \n",
    "    # Expand dimensions for broadcasting: (n, 1, m) and (1, n, m)\n",
    "    X1 = X[:, np.newaxis, :]  # shape (n, 1, m)\n",
    "    X2 = X[np.newaxis, :, :]  # shape (1, n, m)\n",
    "    \n",
    "    # Mask for valid (non-NaN) pairs\n",
    "    valid_mask = ~(np.isnan(X1) | np.isnan(X2))  # shape (n, n, m)\n",
    "    \n",
    "    # Squared differences (NaNs will be ignored)\n",
    "    diff_sq = np.where(valid_mask, (X1 - X2)**2, 0)  # shape (n, n, m)\n",
    "    \n",
    "    # Count valid dimensions for each pair\n",
    "    count = valid_mask.sum(axis=2)  # shape (n, n)\n",
    "    \n",
    "    # Sum of squared differences\n",
    "    sum_sq = diff_sq.sum(axis=2)  # shape (n, n)\n",
    "    \n",
    "    # Mean squared difference, then sqrt\n",
    "    # Avoid division by zero\n",
    "    D = np.sqrt(np.divide(sum_sq, count, out=np.zeros_like(sum_sq), where=count>0))\n",
    "    \n",
    "    return D\n",
    "\n",
    "# Usage\n",
    "# final_df['numerical_classification'] = final_df['classification'].replace({'enriched':2,'depleted':-2,'unchanged':0})\n",
    "# p = pd.pivot_table(final_df,columns=['soil_type','priming'],index='original_index',values='numerical_classification',fill_value=None)\n",
    "p = pd.pivot_table(final_df[final_df['has_ms2_evidence']==True],columns=['soil_type','priming'],index='original_index',values='fc',fill_value=None)\n",
    "\n",
    "sum_valid = p.notna().sum(axis=1)\n",
    "p = p[sum_valid>=2]\n",
    "print(p.shape)\n",
    "# p = p.sample(frac=0.1, random_state=42)  # for testing purposes, use a subset\n",
    "distance_matrix = pairwise_available_euclidean_vectorized(p.values)\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 2. Hierarchical clustering\n",
    "linkage_matrix = linkage(distance_matrix[np.triu_indices(len(distance_matrix), k=1)], method='ward')\n",
    "\n",
    "# 3. Get cluster labels (adjust t for number of clusters you want)\n",
    "clusters = fcluster(linkage_matrix, t=5, criterion='maxclust')\n",
    "\n",
    "# 4. t-SNE projection\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "coords = tsne.fit_transform(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c7b3b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21129"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Common Setup and Data Loading ---\n",
    "# This part is run only once to load and prepare the data.\n",
    "data_path = '../results/full_build_20250908_181404/permafrost results gnps2'\n",
    "analysis_filename = os.path.join(data_path, 'analysis_results', 'statistical_results.csv')\n",
    "\n",
    "# Load the data\n",
    "output = pd.read_csv(analysis_filename)\n",
    "output['original_index'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for stacked bar chart\n",
    "bar_data = []\n",
    "\n",
    "for soil_type in final_df['soil_type'].unique():\n",
    "    df_soil = final_df[final_df['soil_type']==soil_type].copy()\n",
    "    for priming in df_soil['priming'].unique():\n",
    "        df_subset = df_soil[df_soil['priming']==priming]\n",
    "        nodes = df_subset[(df_subset['classification']=='unchanged')]['original_index'].tolist()\n",
    "        # nodes = df_subset[(df_subset['classification']=='depleted') | (df_subset['classification']=='enriched')]['original_index'].tolist()     \n",
    "        # Get the probability values\n",
    "        prob_values = node_data[node_data['original_index'].isin(nodes)]['predicted_unchanged_in_soil_prob']\n",
    "        \n",
    "        # Count True (>=0.5) and False (<0.5)\n",
    "        # true_count = (prob_values < 0.5).sum()\n",
    "        # false_count = (prob_values >= 0.5).sum()\n",
    "        true_count = (prob_values >= 0.5).sum()\n",
    "        false_count = (prob_values < 0.5).sum()\n",
    "        total = len(prob_values)\n",
    "        \n",
    "        # Store data\n",
    "        bar_data.append({\n",
    "            'label': f'{soil_type} {priming}',\n",
    "            'true': true_count,\n",
    "            'false': false_count,\n",
    "            'total': total,\n",
    "            'true_prop': true_count / total if total > 0 else 0,\n",
    "            'false_prop': false_count / total if total > 0 else 0\n",
    "        })\n",
    "\n",
    "# Create the normalized stacked horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "labels = []\n",
    "for d in bar_data:\n",
    "    if 'Unprimed' in d['label']:\n",
    "        label = d['label'].replace(' Unprimed', ' (Unprimed)')\n",
    "    elif 'Low' in d['label']:\n",
    "        label = d['label'].replace(' Low', ' (Low Priming)')\n",
    "    elif 'High' in d['label']:\n",
    "        label = d['label'].replace(' High', ' (High Priming)')\n",
    "    labels.append(label)\n",
    "\n",
    "false_props = [d['false_prop'] for d in bar_data]\n",
    "true_props = [d['true_prop'] for d in bar_data]\n",
    "\n",
    "y_pos = np.arange(len(labels))\n",
    "\n",
    "# Create horizontal stacked bars\n",
    "ax.barh(y_pos, false_props, label='Model prediction: Not unchanged', color='#d62728')\n",
    "ax.barh(y_pos, true_props, left=false_props, label='Model prediction: Unchanged', color='#2ca02c')\n",
    "\n",
    "# Add text annotations showing true/total fraction\n",
    "for i, d in enumerate(bar_data):\n",
    "    # Position text in the middle of the bar\n",
    "    x_pos = 0.8\n",
    "    val = d['true']/d['total']\n",
    "    val = val*100\n",
    "    ax.text(x_pos, i, f\"{val:.1f}% Correct\", color='white',\n",
    "            ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel('Fraction of experimentally \"unchanged\" ENVnet nodes')\n",
    "ax.set_xlim(0, 1)\n",
    "# locate legend above the plot\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.2, 1.15), ncol=2,frameon=False)#, title='Prediction')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4a4680",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,ncols=5, figsize=(25,10))\n",
    "ax = ax.flatten()\n",
    "\n",
    "counter = 0\n",
    "for soil_type in final_df['soil_type'].unique():\n",
    "    df55 = final_df[final_df['soil_type']==soil_type].copy()\n",
    "    for priming in df55['priming'].unique():\n",
    "        df55_primed = df55[df55['priming']==priming]\n",
    "        nodes55 = df55_primed[(df55_primed['classification']=='unchanged')]['original_index'].tolist()\n",
    "        edges = np.linspace(0,1,40)\n",
    "        node_data[node_data['original_index'].isin(nodes55)]['predicted_unchanged_in_soil_prob'].hist(bins=edges,label=f'{soil_type} {priming} unchanged', alpha=0.5,ax=ax[counter])\n",
    "        ax[counter].set_title(f'{soil_type} {priming} unchanged n={len(nodes55)}')\n",
    "        counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70adf4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Assume final_df and node_data are pre-loaded DataFrames\n",
    "UNCLASSIFIED_COLOR = 'gray'\n",
    "# --- Configuration Section ---\n",
    "CLASSIFICATION_COLORS = {\n",
    "    'enriched': '#1f77b4',\n",
    "    'depleted': '#ff7f0e',\n",
    "    'unchanged': '#2ca02c',\n",
    "    'unclassified': UNCLASSIFIED_COLOR\n",
    "}\n",
    "\n",
    "\n",
    "MIN_COMPOUNDS_FOR_DISPLAY = 10\n",
    "\n",
    "CONDITIONS = [\n",
    "    {'soil_type': 'Agricultural Soil', 'priming': 'Unprimed', 'title': 'Agricultural Soil\\nUnprimed'},\n",
    "    {'soil_type': 'Agricultural Soil', 'priming': 'Low',      'title': 'Agricultural Soil\\nLow Priming'},\n",
    "    {'soil_type': 'Agricultural Soil', 'priming': 'High',     'title': 'Agricultural Soil\\nHigh Priming'},\n",
    "    {'soil_type': 'Potting Soil',      'priming': 'Unprimed',       'title': 'Potting Soil\\nUnprimed'},\n",
    "    {'soil_type': 'Potting Soil',      'priming': 'High',       'title': 'Potting Soil\\nHigh Priming'},\n",
    "    {'soil_type': '20M permafrost',      'priming': 'Unprimed',       'title': '20M permafrost\\nUnprimed'},\n",
    "    {'soil_type': '42M permafrost',      'priming': 'Unprimed',       'title': '42M permafrost\\nUnprimed'},\n",
    "    {'soil_type': '55M permafrost',      'priming': 'Unprimed',       'title': '55M permafrost\\nUnprimed'},\n",
    "    {'soil_type': '66M permafrost',      'priming': 'Unprimed',       'title': '66M permafrost\\nUnprimed'},\n",
    "    {'soil_type': '82M permafrost',      'priming': 'Unprimed',       'title': '82M permafrost\\nUnprimed'},\n",
    "\n",
    "]\n",
    "# sample_metadata.loc[idx,'soil_type'] = '20M permafrost'\n",
    "# idx = sample_metadata['treatment'].str.contains('42M-', regex=False,case=False)\n",
    "# sample_metadata.loc[idx,'soil_type'] = '42M permafrost'\n",
    "# idx = sample_metadata['treatment'].str.contains('55M-', regex=False,case=False)\n",
    "# sample_metadata.loc[idx,'soil_type'] = '55M permafrost'\n",
    "# idx = sample_metadata['treatment'].str.contains('66M-', regex=False,case=False)\n",
    "# sample_metadata.loc[idx,'soil_type'] = '66M permafrost'\n",
    "# idx = sample_metadata['treatment'].str.contains('82M-', regex=False,case=False)\n",
    "# sample_metadata.loc[idx,'soil_type'] = '82M permafrost'\n",
    "\n",
    "compound_class_col = 'NPC#superclass'\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def add_ms2_count_labels_v2(ax, ms2_counts, ms1_counts, soil_type, priming, bar_data, x_offset=0.02, fontsize=12):\n",
    "    \"\"\"\n",
    "    Add MS2 count labels using the bar_data index directly.\n",
    "    \"\"\"\n",
    "    for i, compound_class in enumerate(bar_data.index):\n",
    "        try:\n",
    "            count = ms2_counts.loc[compound_class, (soil_type, priming)]\n",
    "            count_ms1 = ms1_counts.loc[compound_class, (soil_type, priming)]\n",
    "            ax.text(0.95 + x_offset, i, f'{int(count)}/{int(count_ms1)}', \n",
    "                   va='center', ha='right', fontsize=fontsize,\n",
    "                   transform=ax.get_yaxis_transform())\n",
    "        except (KeyError, IndexError):\n",
    "            pass\n",
    "\n",
    "\n",
    "# --- Main Plotting Script ---\n",
    "# filtered_final_df = final_df[final_df['has_ms2_evidence'] == True].copy()\n",
    "filtered_final_df = final_df.copy()\n",
    "global_class_order = get_combined_class_order(filtered_final_df, node_data, min_compounds=MIN_COMPOUNDS_FOR_DISPLAY)\n",
    "print(f\"Displaying {len(global_class_order)} most abundant compound classes.\")\n",
    "\n",
    "# Reduce spacing between subplots\n",
    "width = 40\n",
    "height = 30\n",
    "fig = plt.figure(figsize=(width, height))\n",
    "# fig = plt.figure(figsize=(25, 34))\n",
    "gs = gridspec.GridSpec(2, 10, height_ratios=[1, 4], hspace=0.12, wspace=0.2)\n",
    "# gs = gridspec.GridSpec(2, 5, height_ratios=[1, 5], hspace=0.12, wspace=0.15)\n",
    "\n",
    "axes_bar = []\n",
    "axes_volcano = []  # Track volcano plot axes for sharing y-axis\n",
    "\n",
    "for i, cond in enumerate(CONDITIONS):\n",
    "    # Create volcano plot with shared y-axis\n",
    "    ax_volcano = fig.add_subplot(gs[0, i], sharey=axes_volcano[0] if i > 0 else None)\n",
    "    axes_volcano.append(ax_volcano)\n",
    "    \n",
    "    # Create bar chart with shared y-axis\n",
    "    ax_bar = fig.add_subplot(gs[1, i], sharey=axes_bar[0] if i > 0 else None)\n",
    "    axes_bar.append(ax_bar)\n",
    "\n",
    "    idx = (filtered_final_df['soil_type'] == cond['soil_type'])\n",
    "    if cond['priming'] is not None:\n",
    "        idx &= (filtered_final_df['priming'] == cond['priming'])\n",
    "    plot_df = filtered_final_df[idx].copy()\n",
    "    plot_df = pd.merge(plot_df, node_data, on='original_index', how='left')\n",
    "    print(cond,plot_df.shape[0])\n",
    "    create_volcano_plot(plot_df, ax_volcano, cond['title'])\n",
    "    \n",
    "    # Hide y-axis labels for volcano plots after the first one\n",
    "    if i > 0:\n",
    "        ax_volcano.set_ylabel('')\n",
    "        plt.setp(ax_volcano.get_yticklabels(), visible=False)\n",
    "    \n",
    "    bar_data = prepare_barchart_data(plot_df, global_class_order)\n",
    "    if not bar_data.empty:\n",
    "        bar_data.plot(kind='barh', stacked=True, ax=ax_bar, width=0.95, alpha=0.6,\n",
    "                      color=[CLASSIFICATION_COLORS[c] for c in bar_data.columns], legend=False)\n",
    "        ax_bar.set_yticklabels(truncate_class_names(bar_data.index))\n",
    "        add_ms2_count_labels_v2(ax_bar, ms2_counts, ms1_counts, cond['soil_type'], cond['priming'], \n",
    "                        bar_data, x_offset=0.02, fontsize=30)\n",
    "    else:\n",
    "        ax_bar.text(0.5, 0.5, 'No data', ha='center', va='center', fontsize=12, transform=ax_bar.transAxes)\n",
    "\n",
    "    ax_bar.set_title('')\n",
    "    if i==2:\n",
    "        ax_bar.set_xlabel('Fraction of Compounds', fontsize=20)\n",
    "    else:\n",
    "        ax_bar.set_xlabel('')\n",
    "    ax_bar.tick_params(axis='x', labelsize=20)\n",
    "    ax_bar.tick_params(axis='y', labelsize=20)\n",
    "    ax_bar.set_xlim(0,1)\n",
    "    if i == 0:\n",
    "        ax_bar.set_ylabel('Compound Class', fontsize=20)\n",
    "    else:\n",
    "        ax_bar.set_ylabel('')\n",
    "        plt.setp(ax_bar.get_yticklabels(), visible=False)\n",
    "\n",
    "legend_elements = [\n",
    "    plt.Rectangle((0,0), 1, 1, color=CLASSIFICATION_COLORS['enriched'], label='Enriched'),\n",
    "    plt.Rectangle((0,0), 1, 1, color=CLASSIFICATION_COLORS['depleted'], label='Depleted'),\n",
    "    plt.Rectangle((0,0), 1, 1, color=CLASSIFICATION_COLORS['unchanged'], label='Unchanged'),\n",
    "    plt.Rectangle((0,0), 1, 1, color=CLASSIFICATION_COLORS['unclassified'], label='Unclassified'),\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(-0.15, 0.9),\n",
    "           ncol=1, title='Classification', fontsize=20, title_fontsize=20, frameon=False)\n",
    "\n",
    "# add panel labels lowercase letters with period\n",
    "panel_labels = ['a.', 'b.', 'c.', 'd.', 'e.','f.', 'g.', 'h.', 'i.', 'j.']\n",
    "counter = 0\n",
    "for ax in axes_volcano:\n",
    "    ax.text(-0.025, 1.12, panel_labels[counter], transform=ax.transAxes,\n",
    "            fontsize=40, va='top', ha='right')\n",
    "    counter += 1\n",
    "counter =0 \n",
    "for i, ax in enumerate(axes_bar):\n",
    "    # if i>0:\n",
    "    ax.text(-0.03, 1.015, panel_labels[counter], transform=ax.transAxes,\n",
    "                fontsize=20, va='top', ha='right')\n",
    "    # else:\n",
    "    # ax.text(-0.8, 1.03, panel_labels[counter], transform=ax.transAxes,\n",
    "                # fontsize=40, va='top', ha='right')\n",
    "    counter += 1\n",
    "# Adjust layout with reduced margins\n",
    "# plt.tight_layout(rect=[0, 0.04, 1, 0.97])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fa5ea7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can only merge Series or DataFrame objects, a <class 'dict'> was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# --- Main Plotting Script ---\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# filtered_final_df = final_df[final_df['has_ms2_evidence'] == True].copy()\u001b[39;00m\n\u001b[1;32m     48\u001b[0m filtered_final_df \u001b[38;5;241m=\u001b[39m final_df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 49\u001b[0m global_class_order \u001b[38;5;241m=\u001b[39m \u001b[43mget_combined_class_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_final_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_compounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMIN_COMPOUNDS_FOR_DISPLAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisplaying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(global_class_order)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m most abundant compound classes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Reduce spacing between subplots\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 110\u001b[0m, in \u001b[0;36mget_combined_class_order\u001b[0;34m(df, node_data, min_compounds)\u001b[0m\n\u001b[1;32m    108\u001b[0m     idx \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpriming\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m cond[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpriming\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    109\u001b[0m plot_df \u001b[38;5;241m=\u001b[39m df[idx]\n\u001b[0;32m--> 110\u001b[0m plot_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moriginal_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m plot_df\u001b[38;5;241m.\u001b[39mgroupby(compound_class_col)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    113\u001b[0m class_counts_by_condition[cond_idx] \u001b[38;5;241m=\u001b[39m class_counts\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[0;32m/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/pandas/core/reshape/merge.py:110\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m--> 110\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/pandas/core/reshape/merge.py:645\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    630\u001b[0m     left: DataFrame \u001b[38;5;241m|\u001b[39m Series,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m     _left \u001b[38;5;241m=\u001b[39m _validate_operand(left)\n\u001b[0;32m--> 645\u001b[0m     _right \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_operand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_left \u001b[38;5;241m=\u001b[39m _left\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_right \u001b[38;5;241m=\u001b[39m _right\n",
      "File \u001b[0;32m/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/pandas/core/reshape/merge.py:2426\u001b[0m, in \u001b[0;36m_validate_operand\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2424\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   2425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only merge Series or DataFrame objects, a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was passed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2428\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Can only merge Series or DataFrame objects, a <class 'dict'> was passed"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Assume final_df and node_data are pre-loaded DataFrames\n",
    "UNCLASSIFIED_COLOR = 'gray'\n",
    "# --- Configuration Section ---\n",
    "CLASSIFICATION_COLORS = {\n",
    "    'enriched': '#1f77b4',\n",
    "    'depleted': '#ff7f0e',\n",
    "    'unchanged': '#2ca02c',\n",
    "    'unclassified': UNCLASSIFIED_COLOR\n",
    "}\n",
    "\n",
    "\n",
    "MIN_COMPOUNDS_FOR_DISPLAY = 20\n",
    "\n",
    "CONDITIONS = [\n",
    "    {'soil_type': 'Agricultural Soil', 'priming': 'Unprimed', 'title': 'Agricultural Soil\\nUnprimed'},\n",
    "    {'soil_type': 'Agricultural Soil', 'priming': 'Low',      'title': 'Agricultural Soil\\nLow Priming'},\n",
    "    {'soil_type': 'Agricultural Soil', 'priming': 'High',     'title': 'Agricultural Soil\\nHigh Priming'},\n",
    "    {'soil_type': 'Potting Soil',      'priming': 'Unprimed',       'title': 'Potting Soil\\nUnprimed'},\n",
    "    {'soil_type': 'Potting Soil',      'priming': 'High',       'title': 'Potting Soil\\nHigh Priming'}\n",
    "]\n",
    "\n",
    "compound_class_col = 'NPC#superclass'\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def add_ms2_count_labels_v2(ax, ms2_counts, ms1_counts, soil_type, priming, bar_data, x_offset=0.02, fontsize=12):\n",
    "    \"\"\"\n",
    "    Add MS2 count labels using the bar_data index directly.\n",
    "    \"\"\"\n",
    "    for i, compound_class in enumerate(bar_data.index):\n",
    "        try:\n",
    "            count = ms2_counts.loc[compound_class, (soil_type, priming)]\n",
    "            count_ms1 = ms1_counts.loc[compound_class, (soil_type, priming)]\n",
    "            ax.text(0.95 + x_offset, i, f'{int(count)}/{int(count_ms1)}', \n",
    "                   va='center', ha='right', fontsize=fontsize,\n",
    "                   transform=ax.get_yaxis_transform())\n",
    "        except (KeyError, IndexError):\n",
    "            pass\n",
    "\n",
    "\n",
    "# --- Main Plotting Script ---\n",
    "# filtered_final_df = final_df[final_df['has_ms2_evidence'] == True].copy()\n",
    "filtered_final_df = final_df.copy()\n",
    "global_class_order = get_combined_class_order(filtered_final_df, node_data, min_compounds=MIN_COMPOUNDS_FOR_DISPLAY)\n",
    "print(f\"Displaying {len(global_class_order)} most abundant compound classes.\")\n",
    "\n",
    "# Reduce spacing between subplots\n",
    "fig = plt.figure(figsize=(25, 30))\n",
    "# fig = plt.figure(figsize=(25, 34))\n",
    "gs = gridspec.GridSpec(2, 5, height_ratios=[1, 4], hspace=0.12, wspace=0.2)\n",
    "# gs = gridspec.GridSpec(2, 5, height_ratios=[1, 5], hspace=0.12, wspace=0.15)\n",
    "\n",
    "axes_bar = []\n",
    "axes_volcano = []  # Track volcano plot axes for sharing y-axis\n",
    "\n",
    "for i, cond in enumerate(CONDITIONS):\n",
    "    # Create volcano plot with shared y-axis\n",
    "    ax_volcano = fig.add_subplot(gs[0, i], sharey=axes_volcano[0] if i > 0 else None)\n",
    "    axes_volcano.append(ax_volcano)\n",
    "    \n",
    "    # Create bar chart with shared y-axis\n",
    "    ax_bar = fig.add_subplot(gs[1, i], sharey=axes_bar[0] if i > 0 else None)\n",
    "    axes_bar.append(ax_bar)\n",
    "\n",
    "    idx = (filtered_final_df['soil_type'] == cond['soil_type'])\n",
    "    if cond['priming'] is not None:\n",
    "        idx &= (filtered_final_df['priming'] == cond['priming'])\n",
    "    plot_df = filtered_final_df[idx].copy()\n",
    "    plot_df = pd.merge(plot_df, node_data, on='original_index', how='left')\n",
    "    \n",
    "    create_volcano_plot(plot_df, ax_volcano, cond['title'])\n",
    "    \n",
    "    # Hide y-axis labels for volcano plots after the first one\n",
    "    if i > 0:\n",
    "        ax_volcano.set_ylabel('')\n",
    "        plt.setp(ax_volcano.get_yticklabels(), visible=False)\n",
    "    \n",
    "    bar_data = prepare_barchart_data(plot_df, global_class_order)\n",
    "    if not bar_data.empty:\n",
    "        bar_data.plot(kind='barh', stacked=True, ax=ax_bar, width=0.95, alpha=0.6,\n",
    "                      color=[CLASSIFICATION_COLORS[c] for c in bar_data.columns], legend=False)\n",
    "        ax_bar.set_yticklabels(truncate_class_names(bar_data.index))\n",
    "        add_ms2_count_labels_v2(ax_bar, ms2_counts, ms1_counts, cond['soil_type'], cond['priming'], \n",
    "                        bar_data, x_offset=0.02, fontsize=30)\n",
    "    else:\n",
    "        ax_bar.text(0.5, 0.5, 'No data', ha='center', va='center', fontsize=12, transform=ax_bar.transAxes)\n",
    "\n",
    "    ax_bar.set_title('')\n",
    "    if i==2:\n",
    "        ax_bar.set_xlabel('Fraction of Compounds', fontsize=30)\n",
    "    else:\n",
    "        ax_bar.set_xlabel('')\n",
    "    ax_bar.tick_params(axis='x', labelsize=30)\n",
    "    ax_bar.tick_params(axis='y', labelsize=30)\n",
    "    ax_bar.set_xlim(0,1)\n",
    "    if i == 0:\n",
    "        ax_bar.set_ylabel('Compound Class', fontsize=30)\n",
    "    else:\n",
    "        ax_bar.set_ylabel('')\n",
    "        plt.setp(ax_bar.get_yticklabels(), visible=False)\n",
    "\n",
    "legend_elements = [\n",
    "    plt.Rectangle((0,0), 1, 1, color=CLASSIFICATION_COLORS['enriched'], label='Enriched'),\n",
    "    plt.Rectangle((0,0), 1, 1, color=CLASSIFICATION_COLORS['depleted'], label='Depleted'),\n",
    "    plt.Rectangle((0,0), 1, 1, color=CLASSIFICATION_COLORS['unchanged'], label='Unchanged'),\n",
    "    plt.Rectangle((0,0), 1, 1, color=CLASSIFICATION_COLORS['unclassified'], label='Unclassified'),\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(-0.15, 0.9),\n",
    "           ncol=1, title='Classification', fontsize=40, title_fontsize=40, frameon=False)\n",
    "\n",
    "# add panel labels lowercase letters with period\n",
    "panel_labels = ['a.', 'b.', 'c.', 'd.', 'e.','f.', 'g.', 'h.', 'i.', 'j.']\n",
    "counter = 0\n",
    "for ax in axes_volcano:\n",
    "    ax.text(-0.025, 1.12, panel_labels[counter], transform=ax.transAxes,\n",
    "            fontsize=40, va='top', ha='right')\n",
    "    counter += 1\n",
    "for i, ax in enumerate(axes_bar):\n",
    "    # if i>0:\n",
    "    ax.text(-0.03, 1.015, panel_labels[counter], transform=ax.transAxes,\n",
    "                fontsize=40, va='top', ha='right')\n",
    "    # else:\n",
    "    # ax.text(-0.8, 1.03, panel_labels[counter], transform=ax.transAxes,\n",
    "                # fontsize=40, va='top', ha='right')\n",
    "    counter += 1\n",
    "# Adjust layout with reduced margins\n",
    "# plt.tight_layout(rect=[0, 0.04, 1, 0.97])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30642e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_final_df = final_df.copy()\n",
    "global_class_order = get_combined_class_order(filtered_final_df, node_data, min_compounds=MIN_COMPOUNDS_FOR_DISPLAY)\n",
    "filtered_final_df = pd.merge(filtered_final_df, node_data, on='original_index', how='left')\n",
    "filtered_final_df = filtered_final_df[filtered_final_df[compound_class_col].isin(global_class_order)]\n",
    "ms1_counts = filtered_final_df.groupby(['soil_type','priming',compound_class_col])['has_ms2_evidence'].count()\n",
    "ms1_counts = ms1_counts.unstack(fill_value=0).T\n",
    "ms1_counts = ms1_counts.reindex(global_class_order).dropna()\n",
    "ms1_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df13e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_final_df = final_df.copy()\n",
    "global_class_order = get_combined_class_order(filtered_final_df, node_data, min_compounds=MIN_COMPOUNDS_FOR_DISPLAY)\n",
    "filtered_final_df = filtered_final_df[filtered_final_df['has_ms2_evidence'] == True]\n",
    "filtered_final_df = pd.merge(filtered_final_df, node_data, on='original_index', how='left')\n",
    "filtered_final_df = filtered_final_df[filtered_final_df[compound_class_col].isin(global_class_order)]\n",
    "filtered_final_df.columns\n",
    "ms2_counts = filtered_final_df.groupby(['soil_type','priming',compound_class_col])['has_ms2_evidence'].value_counts()\n",
    "ms2_counts = ms2_counts.unstack(fill_value=0).reset_index()\n",
    "ms2_counts.rename(columns={True:'ms2_count'}, inplace=True)\n",
    "ms2_counts = pd.pivot_table(ms2_counts, values='ms2_count', index=[compound_class_col], columns=['soil_type','priming'], fill_value=0)\n",
    "# put in order of global_class_order\n",
    "ms2_counts = ms2_counts.reindex(global_class_order).dropna()\n",
    "ms2_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c12ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same CONDITIONS as the previous figure\n",
    "# CONDITIONS = [\n",
    "#     {'soil_type': 'Agricultural Soil', 'priming': 'Unprimed', 'title': 'Agricultural Soil\\nUnprimed'},\n",
    "#     {'soil_type': 'Agricultural Soil', 'priming': 'Low',      'title': 'Agricultural Soil\\nLow Priming'},\n",
    "#     {'soil_type': 'Agricultural Soil', 'priming': 'High',     'title': 'Agricultural Soil\\nHigh Priming'},\n",
    "#     {'soil_type': 'Potting Soil',      'priming': None,       'title': 'Potting Soil'}\n",
    "# ]\n",
    "\n",
    "plot_df = final_df[final_df['has_ms2_evidence'] == True].copy()\n",
    "fig, all_axes = plt.subplots(figsize=(18, 6), nrows=1, ncols=len(CONDITIONS), sharey=True)\n",
    "\n",
    "class_term = 'NPC#class'\n",
    "\n",
    "# First pass: collect all compound classes across all conditions to determine global order\n",
    "all_class_counts = {}\n",
    "for cond in CONDITIONS:\n",
    "    idx1 = plot_df['classification'].notna()\n",
    "    idx2 = plot_df['soil_type'] == cond['soil_type']\n",
    "    if cond['priming'] is not None:\n",
    "        idx2 &= (plot_df['priming'] == cond['priming'])\n",
    "    idx = idx1 & idx2\n",
    "    \n",
    "    classified_indices = plot_df[idx].groupby('classification')['original_index'].unique().to_dict()\n",
    "    \n",
    "    for key, value in classified_indices.items():\n",
    "        idx_node = node_data['original_index'].isin(value)\n",
    "        temp = node_data.loc[idx_node, class_term].value_counts()\n",
    "        for class_name, count in temp.items():\n",
    "            all_class_counts[class_name] = all_class_counts.get(class_name, 0) + count\n",
    "\n",
    "# Filter classes that appear frequently enough and sort by total abundance\n",
    "class_threshold = 0.05  # Adjust this threshold as needed\n",
    "total_count = sum(all_class_counts.values())\n",
    "global_class_order = [cls for cls, count in all_class_counts.items() \n",
    "                      if count / total_count > class_threshold / len(CONDITIONS)]\n",
    "global_class_order = sorted(global_class_order, key=lambda x: all_class_counts[x], reverse=True)\n",
    "\n",
    "# Second pass: create plots with consistent ordering\n",
    "for counter, cond in enumerate(CONDITIONS):\n",
    "    ax = all_axes[counter]\n",
    "    \n",
    "    idx1 = plot_df['classification'].notna()\n",
    "    idx2 = plot_df['soil_type'] == cond['soil_type']\n",
    "    if cond['priming'] is not None:\n",
    "        idx2 &= (plot_df['priming'] == cond['priming'])\n",
    "    idx = idx1 & idx2\n",
    "    \n",
    "    classified_indices = plot_df[idx].groupby('classification')['original_index'].unique().to_dict()\n",
    "    out = []\n",
    "    \n",
    "    for key, value in classified_indices.items():\n",
    "        idx_node = node_data['original_index'].isin(value)\n",
    "        temp = node_data.loc[idx_node, class_term].value_counts()\n",
    "        temp = temp.to_frame().reset_index().rename(columns={'index': 'class', class_term: 'count'})\n",
    "        temp['classification'] = key\n",
    "        temp['total'] = temp['count'].sum()\n",
    "        temp['fraction'] = temp['count'] / temp['total']\n",
    "        out.append(temp)\n",
    "    \n",
    "    class_df = pd.concat(out, ignore_index=True)\n",
    "    class_df = class_df.pivot(index='classification', columns='class', values='fraction')\n",
    "    class_df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Reindex to use global class order\n",
    "    class_df = class_df.reindex(columns=global_class_order, fill_value=0)\n",
    "    \n",
    "    # Sum each row to one\n",
    "    class_df = class_df.div(class_df.sum(axis=1), axis=0)\n",
    "    \n",
    "    class_df.plot(kind='bar', stacked=True, ax=ax, width=0.8,\n",
    "                  color=sns.color_palette(\"tab20\", n_colors=len(global_class_order)))\n",
    "    ax.set_ylabel('Fraction of Annotations', fontsize=16)\n",
    "    ax.set_xlabel('', fontsize=14)\n",
    "    ax.set_title(cond['title'], fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.set_ylim(0, 1)\n",
    "    # Only show legend on the last subplot\n",
    "    if counter == len(CONDITIONS) - 1:\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1.15), loc='upper left', fontsize=16, title='Compound Class', title_fontsize=18,frameon=False )\n",
    "    else:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6586821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms2_filename = '/pscratch/sd/b/bpb/envnet_annotation_results/ms2_results_experiments_for_paper/ms2_deconvoluted_annotations.parquet'\n",
    "# ms2_cols = ['score_deconvoluted_match', 'matches_deconvoluted_match',\n",
    "#        'original_index_deconvoluted_match',  'filename',\n",
    "#         'mz_diff']\n",
    "# ms2_df = pd.read_parquet(ms2_filename, columns=ms2_cols)\n",
    "\n",
    "# fig,ax = plt.subplots(1,2,figsize=(10,6),sharex=True,sharey=True)\n",
    "\n",
    "# my_file = '/global/cfs/cdirs/metatlas/projects/envnet_build_files/analysis_for_manuscript/20240125_EB_MdR_101544-059_WAVESTAB3_20231222_EXP120A_C18-EP_USDAY72349_NEG_MS2_23_supern-CentExp-OMT1d2-NatCom-d7-NA_3_6uL_054.h5'\n",
    "# idx = ms2_df['filename'].str.contains('WAVEstab')\n",
    "# plot_df = ms2_df[idx].copy()\n",
    "# plot_df.sort_values(by='score_deconvoluted_match', ascending=False, inplace=True)\n",
    "# plot_df.drop_duplicates(subset=['original_index_deconvoluted_match'], keep='first', inplace=True)\n",
    "# plot_df = pd.merge(plot_df,node_data, left_on='original_index_deconvoluted_match', right_on='original_index', how='left')\n",
    "# p = pd.pivot_table(plot_df,index='predicted_formula', values='score_deconvoluted_match',columns='NPC#class')\n",
    "# p = p.notna()\n",
    "# num_ms2 = p.sum(axis=1)\n",
    "# p = pd.pivot_table(node_data,index='predicted_formula', values='original_index',columns='NPC#class')\n",
    "# p = p.notna()\n",
    "# num_ms1 = p.sum(axis=1)\n",
    "# d = num_ms1.value_counts()\n",
    "# ax[0].bar(d.index, d.values)\n",
    "# ax[0].set_yscale('log')\n",
    "# d = num_ms2.value_counts()\n",
    "# ax[1].bar(d.index, d.values)\n",
    "# ax[1].set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(temp['NPC#class_no_ms2']==1)/# temp = final_df[final_df['classification'].notna()]\n",
    "# idx1 = temp['soil_type'] == 'Agricultural Soil'\n",
    "# idx2 = temp['classification'] == 'prefered'\n",
    "# idx3 = temp['classification'] == 'produced'\n",
    "# temp = temp[idx1].copy()# & (idx2 | idx3)].copy()\n",
    "# temp = pd.merge(temp, node_data, on='original_index', how='left')\n",
    "# temp = temp.groupby('predicted_formula')['classification'].value_counts().unstack(fill_value=0)\n",
    "# print(\"There are {} formulas that are prefered, produced or ignored\".format(len(temp)))\n",
    "# idx1 = (temp>0).sum(axis=1)>1\n",
    "# print('There are {} formulas that are prefered and produced or ignored'.format(temp[idx1].shape[0]))\n",
    "# temp = temp[(temp['prefered']>1) & (temp['produced']>1)].copy()\n",
    "# temp\n",
    "# prediction_df = pd.merge(final_df[final_df['soil_type'].str.contains('soil',case=False)], node_data, on='original_index', how='right')\n",
    "# idx = (pd.notna(prediction_df['NPC#class'])) & (prediction_df['predicted_unchanged_in_soil'].notna())\n",
    "# classified_compounds = prediction_df[idx].copy()\n",
    "# # for each NPC#class get 0,1 from predicted_unchanged_in_soil\n",
    "# summary = classified_compounds.groupby('NPC#class').apply(\n",
    "#     lambda x: pd.Series({\n",
    "#         'num_compounds': x.shape[0],\n",
    "#         'num_predicted_unchanged': x['predicted_unchanged_in_soil'].sum(),\n",
    "#         # 'num_predicted_changed': sum(pd.notna(x['predicted_unchanged_in_soil'])) - x['predicted_unchanged_in_soil'].sum(),\n",
    "#         'fraction_predicted_unchanged': x['predicted_unchanged_in_soil'].mean(),\n",
    "#         # 'total_classified': sum(pd.notna(x['predicted_unchanged_in_soil']))\n",
    "#     })\n",
    "# ).reset_index()\n",
    "# summary = summary[summary['num_compounds']>=20].copy()\n",
    "# summary = summary.sort_values('fraction_predicted_unchanged', ascending=False)\n",
    "\n",
    "# fig,ax = plt.subplots(1,1,figsize=(6,4))\n",
    "# summary['fraction_predicted_unchanged'].hist(bins=20, ax=ax)\n",
    "# ax.set_xlabel('Fraction of compounds predicted unchanged in soil')\n",
    "# ax.set_ylabel('Number of compound classes')\n",
    "# plt.tight_layout()\n",
    "# prediction_df = pd.merge(final_df[final_df['soil_type'].str.contains('soil',case=False)], node_data, on='original_index', how='right')\n",
    "# idx = d.notna(prediction_df['smiles'])\n",
    "# classified_compounds = prediction_df[idx].copy()\n",
    "# classified_compounds = classified_compounds.loc[classified_compounds['t'].isna(),['smiles','predicted_unchanged_in_soil']]\n",
    "# classified_compounds.drop_duplicates(inplace=True)\n",
    "\n",
    "# # use rdkit to draw compounds and label them by predicted_unchanged_in_soil\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import Draw\n",
    "# from rdkit.Chem.Draw import rdMolDraw2D\n",
    "# mols = []\n",
    "# legends = []\n",
    "# for _, row in classified_compounds.iterrows():\n",
    "#     mol = Chem.MolFromSmiles(row['smiles'])\n",
    "#     if mol is not None:\n",
    "#         mol.SetProp('predicted_unchanged_in_soil', str(row['predicted_unchanged_in_soil']))\n",
    "#         mols.append(mol)\n",
    "#         legends.append(f\"Predicted unchanged in soil: {row['predicted_unchanged_in_soil']}\")\n",
    "# # sort them by their legends\n",
    "# mols, legends = zip(*sorted(zip(mols, legends), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# img = Draw.MolsToGridImage(mols, molsPerRow=4, useSVG=True, subImgSize=(200,200), legends=legends)\n",
    "# # with open('classified_compounds.svg', 'w') as f:\n",
    "#     # f.write(img.data)\n",
    "\n",
    "# # these are the not observed compounds in soil that have a prediction probability\n",
    "# from scipy.stats import ttest_1samp, wilcoxon\n",
    "# prediction_df = pd.merge(final_df[final_df['soil_type'].str.contains('soil',case=False)], node_data, on='original_index', how='right')\n",
    "# prediction_df = prediction_df[pd.isna(prediction_df['fc'])].copy()\n",
    "\n",
    "# # observed_df = pd.merge(final_df[final_df['soil_type'].str.contains('soil',case=False)], node_data, on='original_index', how='left')\n",
    "# # observed_df['measured_unchanged_in_soil'] = None\n",
    "# # idx = observed_df['classification']=='ignored'\n",
    "# # observed_df.loc[idx,'measured_unchanged_in_soil'] = 1.0\n",
    "# # idx = (observed_df['classification']=='prefered') | (observed_df['classification']=='produced')\n",
    "# # observed_df.loc[idx,'measured_unchanged_in_soil'] = 0.0\n",
    "# # observed_df = observed_df[pd.notna(observed_df['measured_unchanged_in_soil'])].copy()\n",
    "# observed_df = pd.merge(final_df[final_df['soil_type'].str.contains('soil',case=False)], node_data, on='original_index', how='left')\n",
    "# g = observed_df.groupby(['original_index'])['classification'].apply(my_class_fun)\n",
    "# g = g.to_frame()\n",
    "# g.columns = ['measured_unchanged_in_soil']\n",
    "# g.index.name = 'original_index'\n",
    "# g.reset_index()\n",
    "# observed_df = pd.merge(g,node_data,on='original_index',how='left')\n",
    "\n",
    "# def calculate_significance_stats(group):\n",
    "#     \"\"\"Calculate various statistical tests for probability significance\"\"\"\n",
    "#     probs = group.dropna()\n",
    "#     if len(probs) < 3:  # Need minimum samples for statistical tests\n",
    "#         return pd.Series({\n",
    "#             'count': len(probs),\n",
    "#             'mean': np.nan,\n",
    "#             'median': np.nan,\n",
    "#             'ttest_pvalue': np.nan,\n",
    "#             'wilcoxon_pvalue': np.nan,\n",
    "#             'prop_high': np.nan,\n",
    "#             'prop_low': np.nan\n",
    "#         })\n",
    "    \n",
    "#     # Basic statistics\n",
    "#     mean_prob = probs.mean()\n",
    "#     median_prob = probs.median()\n",
    "#     count = len(probs)\n",
    "    \n",
    "#     # One-sample t-test: tests if mean is significantly different from 0.5\n",
    "#     t_stat, t_pvalue = ttest_1samp(probs, 0.5)\n",
    "    \n",
    "#     # Wilcoxon signed-rank test: non-parametric test if median differs from 0.5\n",
    "#     try:\n",
    "#         # Subtract 0.5 to center around 0 for the test\n",
    "#         w_stat, w_pvalue = wilcoxon(probs - 0.5)\n",
    "#     except ValueError:\n",
    "#         # Happens when all values are exactly 0.5\n",
    "#         w_pvalue = 1.0\n",
    "    \n",
    "#     # Proportion-based measures\n",
    "#     prop_high = (probs > 0.7).sum() / count  # Proportion with high confidence\n",
    "#     prop_low = (probs < 0.3).sum() / count   # Proportion with low confidence\n",
    "    \n",
    "#     return pd.Series({\n",
    "#         'count': count,\n",
    "#         'mean': mean_prob,\n",
    "#         'median': median_prob,\n",
    "#         'ttest_pvalue': t_pvalue,\n",
    "#         'wilcoxon_pvalue': w_pvalue,\n",
    "#         'prop_high': prop_high,\n",
    "#         'prop_low': prop_low\n",
    "#     })\n",
    "\n",
    "# # Apply the function and properly handle the result\n",
    "# g = prediction_df.groupby('NPC#class')['predicted_unchanged_in_soil'].apply(calculate_significance_stats)\n",
    "# # Unstack to convert the multi-index Series to a DataFrame\n",
    "# g = g.unstack().reset_index()\n",
    "# g = g[g['count']>=20].copy().sort_values('mean', ascending=False)\n",
    "\n",
    "# # Add significance flags\n",
    "# g['ttest_significant'] = g['ttest_pvalue'] < 0.05\n",
    "# g['wilcoxon_significant'] = g['wilcoxon_pvalue'] < 0.05\n",
    "# g['strongly_biased'] = (g['prop_high'] > 0.8) | (g['prop_low'] > 0.8)\n",
    "\n",
    "# print(\"Compound classes with significant deviations from 0.5 (t-test):\")\n",
    "# print(g[g['ttest_significant']]['NPC#class'].tolist())\n",
    "# print(f\"\\n{g['ttest_significant'].sum()} out of {len(g)} classes show significant bias\")\n",
    "\n",
    "# # get the ones where one of the three ttests are significant\n",
    "# significant_classes = g[g['ttest_significant'] & g['wilcoxon_significant']]\n",
    "\n",
    "# # get the top and bottom 5 classes by mean predicted_unchanged_in_soil\n",
    "# top_5 = significant_classes[significant_classes['mean'] > 0.5].head(5)\n",
    "# bottom_5 = significant_classes[significant_classes['mean'] < 0.5].tail(5)\n",
    "\n",
    "# selected_classes = pd.concat([top_5, bottom_5])\n",
    "# print(\"Selected compound classes for further analysis:\")\n",
    "# # there are 6 final classes\n",
    "# # go back to prediction_df and get all compounds in these classes and pie chart their predicted_unchanged_in_soil\n",
    "# for cls in selected_classes['NPC#class']:\n",
    "#     class_compounds = prediction_df[prediction_df['NPC#class'] == cls]\n",
    "#     counts = class_compounds['predicted_unchanged_in_soil'].value_counts(normalize=True)\n",
    "#     print(f\"\\nClass: {cls}\")\n",
    "#     print(counts)\n",
    "\n",
    "# # Define consistent colors: 0 (changed) = blue, 1 (unchanged) = orange\n",
    "# pie_colors = {0.0: 'blue', 1.0: 'orange'}\n",
    "# pie_colors_data = {0.0: 'green', 1.0: 'purple'}\n",
    "# fig, ax = plt.subplots(2, 6, figsize=(14, 8))\n",
    "# ax = ax.flatten()\n",
    "\n",
    "# counter = 0\n",
    "# for cls in selected_classes['NPC#class']:\n",
    "#     if counter >= 6:  # Only show first 6 classes\n",
    "#         break\n",
    "    \n",
    "#     class_compounds = prediction_df[prediction_df['NPC#class'] == cls]\n",
    "#     counts = class_compounds['predicted_unchanged_in_soil'].value_counts(normalize=True)\n",
    "    \n",
    "#     # Ensure we have consistent ordering and colors\n",
    "#     sizes = []\n",
    "#     colors = []\n",
    "    \n",
    "#     for value in [0.0, 1.0]:  # Ensure consistent order\n",
    "#         if value in counts.index:\n",
    "#             sizes.append(counts[value])\n",
    "#             colors.append(pie_colors[value])\n",
    "    \n",
    "#     # Only create pie chart if we have data\n",
    "#     if sizes:\n",
    "#         # Create pie chart without labels\n",
    "#         ax[counter].pie(sizes, colors=colors, autopct='%d%%', startangle=90)\n",
    "#         # switch count first or second if even or odd\n",
    "#         if counter % 2 == 0:\n",
    "#             ax[counter].set_title(f\"{cls}\\n(n={len(class_compounds)})\")\n",
    "#         else:\n",
    "#             ax[counter].set_title(f\"(n={len(class_compounds)})\\n{cls}\")\n",
    "#     else:\n",
    "#         ax[counter].text(0.5, 0.5, f\"{cls}\\nNo data\", ha='center', va='center', transform=ax[counter].transAxes)\n",
    "#         ax[counter].set_xlim(0, 1)\n",
    "#         ax[counter].set_ylim(0, 1)\n",
    "    \n",
    "#     counter += 1\n",
    "\n",
    "# # now do it for observed compounds in soil\n",
    "# for cls in selected_classes['NPC#class']:\n",
    "\n",
    "#     class_compounds = observed_df[observed_df['NPC#class'] == cls]\n",
    "#     counts = class_compounds['measured_unchanged_in_soil'].value_counts(normalize=True)\n",
    "    \n",
    "#     # Ensure we have consistent ordering and colors\n",
    "#     sizes = []\n",
    "#     colors = []\n",
    "    \n",
    "#     for value in [0.0, 1.0]:  # Ensure consistent order\n",
    "#         if value in counts.index:\n",
    "#             sizes.append(counts[value])\n",
    "#             colors.append(pie_colors_data[value])\n",
    "\n",
    "\n",
    "#     # Only create pie chart if we have data\n",
    "#     if sizes:\n",
    "#         # Create pie chart without labels\n",
    "#         ax[counter].pie(sizes, colors=colors, autopct='%d%%', startangle=90)\n",
    "#         # switch count first or second if even or odd\n",
    "#         if counter % 2 == 0:\n",
    "#             ax[counter].set_title(f\"{cls}\\n(n={len(class_compounds)})\")\n",
    "#         else:\n",
    "#             ax[counter].set_title(f\"(n={len(class_compounds)})\\n{cls}\")\n",
    "#     else:\n",
    "#         ax[counter].text(0.5, 0.5, f\"{cls}\\nNo data\", ha='center', va='center', transform=ax[counter].transAxes)\n",
    "#         ax[counter].set_xlim(0, 1)\n",
    "#         ax[counter].set_ylim(0, 1)\n",
    "    \n",
    "#     counter += 1\n",
    "\n",
    "# # Hide any unused subplots\n",
    "# for i in range(counter, 6):\n",
    "#     ax[i].axis('off')\n",
    "\n",
    "# # Create a single legend for the entire figure\n",
    "# legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', \n",
    "#                              markersize=10, label='Changed'),\n",
    "#                   plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', \n",
    "#                              markersize=10, label='Unchanged'),\n",
    "#                 plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green',\n",
    "#                              markersize=10, label='Measured Changed'),\n",
    "#                   plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='purple',\n",
    "#                              markersize=10, label='Measured Unchanged')]\n",
    "\n",
    "# fig.legend(handles=legend_elements, loc='center right', bbox_to_anchor=(1.2, 0.65), \n",
    "#            title='Prediction', frameon=True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# prediction_df = pd.merge(final_df[final_df['soil_type'].str.contains('soil',case=False)], node_data, on='original_index', how='left')\n",
    "\n",
    "# fig,ax = plt.subplots(1,1,figsize=(6,5))\n",
    "# # make a 2d histogram of prediction_df['predicted_unchanged_in_soil_prob'] vs prediction_df['fc']\n",
    "# hb = ax.hexbin(prediction_df['predicted_unchanged_in_soil_prob'], prediction_df['fc'],\n",
    "#                gridsize=20, cmap='plasma', mincnt=1, bins='log', \n",
    "#                reduce_C_function=np.sqrt)\n",
    "# # fill the empty bins with black\n",
    "# ax.set_facecolor('black')\n",
    "# ax.set_xlabel('Prediction Probability (unchanged in soil)')\n",
    "# ax.set_ylabel('Log2 Fold Change (Day7/Day0)')\n",
    "# cb = fig.colorbar(hb, ax=ax)\n",
    "# cb.set_label('Sqrt(Counts)')\n",
    "# plt.tight_layout()\n",
    "# prediction_df = pd.merge(final_df[final_df['soil_type'].str.contains('soil',case=False)], node_data, on='original_index', how='left')\n",
    "\n",
    "# fig,ax = plt.subplots(1,1,figsize=(6,3))\n",
    "# idx = (prediction_df['predicted_unchanged_in_soil']==1) & (pd.notna(prediction_df['classification']))\n",
    "# edges = np.linspace(-5,5,50)\n",
    "# sns.histplot(prediction_df[idx], x='fc', bins=edges, color='blue', label='predicted unchanged', ax=ax, stat='density', alpha=0.5)\n",
    "# idx = (prediction_df['predicted_unchanged_in_soil']==0) & (pd.notna(prediction_df['classification']))\n",
    "# sns.histplot(prediction_df[idx], x='fc', bins=edges, color='orange', label='predicted changed', ax=ax, stat='density', alpha=0.5)\n",
    "# ax.set_xlabel('Log2 Fold Change (Day 7 / Day 0)')\n",
    "# ax.set_ylabel('Density')\n",
    "# # ax.set_yscale('log')\n",
    "# blue_patch = mpatches.Patch(color='blue', label='Predicted Unchanged')\n",
    "# orange_patch = mpatches.Patch(color='orange', label='Predicted Changed')\n",
    "# ax.legend(handles=[blue_patch, orange_patch])\n",
    "# plt.show()\n",
    "\n",
    "# top_classes = prediction_df['NPC#class'].value_counts().head(10).index.tolist()\n",
    "# temp = prediction_df[prediction_df['NPC#class'].isin(top_classes)].copy()\n",
    "# fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "# # make a violin plot of fold change for each class colored by predicted_unchanged_in_soil\n",
    "# # Remove inner box plots to reduce clutter\n",
    "# sns.violinplot(data=temp, x='NPC#class', y='fc', hue='predicted_unchanged_in_soil', split=True, ax=ax, inner=None)\n",
    "# ax.set_ylim(-2,2)\n",
    "# ax.set_xlabel('NPC#class')\n",
    "# ax.set_ylabel('Log2 Fold Change (Day 7 / Day 0)')\n",
    "# ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "# # move legend outside of plot\n",
    "# ax.legend(title='Predicted Unchanged in Soil', loc='upper left', bbox_to_anchor=(1, 1)) \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# true_positives = prediction_df[(prediction_df['classification']=='ignored') & (prediction_df['predicted_unchanged_in_soil']==1)]\n",
    "# print(\"There are {} true positives\".format(true_positives.shape[0]))\n",
    "# true_negatives = prediction_df[(prediction_df['classification']!='ignored') & (prediction_df['predicted_unchanged_in_soil']==0)]\n",
    "# print(\"There are {} true negatives\".format(true_negatives.shape[0]))\n",
    "# false_positives = prediction_df[(prediction_df['classification']!='ignored') & (prediction_df['predicted_unchanged_in_soil']==1)]\n",
    "# print(\"There are {} false positives\".format(false_positives.shape[0]))\n",
    "# false_negatives = prediction_df[(prediction_df['classification']=='ignored') & (prediction_df['predicted_unchanged_in_soil']==0)]\n",
    "# print(\"There are {} false negatives\".format(false_negatives.shape[0]))\n",
    "# y_true = prediction_df['classification'].apply(lambda x: 1 if x=='ignored' else 0).values\n",
    "# y_pred = prediction_df['predicted_unchanged_in_soil'].values\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# cm = confusion_matrix(y_true, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['changed','unchanged'])\n",
    "# disp.plot(cmap=plt.cm.Blues)\n",
    "# plt.title('Prediction of unchanged compounds in soil incubations')\n",
    "# plt.show()\n",
    "\n",
    "# # precision and recall\n",
    "# tp = true_positives.shape[0]\n",
    "# fp = false_positives.shape[0]\n",
    "# fn = false_negatives.shape[0]\n",
    "# precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "# recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "# f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "# print(f'Precision: {precision:.2f}')\n",
    "# print(f'Recall: {recall:.2f}')\n",
    "# print(f'F1 Score: {f1_score:.2f}')  \n",
    "\n",
    "# # matthews correlation coefficient\n",
    "# tn = true_negatives.shape[0]\n",
    "# mcc_numerator = (tp * tn) - (fp * fn)\n",
    "# mcc_denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "# mcc = mcc_numerator / mcc_denominator if mcc_denominator > 0 else 0\n",
    "# print(f'Matthews Correlation Coefficient: {mcc:.2f}')\n",
    "# import pandas as pd\n",
    "# usecols = ['original_index', 't', 'p', 'fc', 'tost',\n",
    "#        'mean_treatment', 'mean_control', 'treatment_vals', 'control_vals',\n",
    "#        'project', 'priming', 'soil_type', 'classification',\n",
    "#        'summary_classification', 'node_order_index', 'deconvoluted_spectrum',\n",
    "#        'all_features_prediction', 'formula_based_prediction']\n",
    "# prediction_df = pd.read_csv('training_data_with_predictions_full.csv', usecols=usecols)\n",
    "\n",
    "# prediction_df = prediction_df[prediction_df['classification'].notna()]\n",
    "# idx1 = prediction_df['soil_type'] == 'Agricultural Soil'\n",
    "\n",
    "\n",
    "# prediction_df = pd.merge(prediction_df, node_data, on='original_index', how='left')\n",
    "\n",
    "# g = prediction_df.groupby('predicted_formula')['classification'].value_counts().unstack(fill_value=0)\n",
    "# print(\"There are {} formulas that are prefered, produced or ignored\".format(len(g)))\n",
    "# idx1 = (g>0).sum(axis=1)>1\n",
    "# complex_ones = g[idx1].index.tolist()\n",
    "# print('There are {} formulas that are prefered and produced or ignored'.format(g[idx1].shape[0]))\n",
    "# # temp = temp[(temp['prefered']>1) & (temp['produced']>1)].copy()\n",
    "# g = prediction_df.groupby('predicted_formula')[['classification','formula_based_prediction','all_features_prediction']].agg(lambda x: tuple(x)).reset_index()\n",
    "# def replace_words_with_numbers(x):\n",
    "#     out = []\n",
    "#     for xx in x:\n",
    "#         if xx=='prefered':\n",
    "#             out.append(0)\n",
    "#         elif xx=='produced':\n",
    "#             out.append(0)\n",
    "#         elif xx=='ignored':\n",
    "#             out.append(1)\n",
    "#     return tuple(out)\n",
    "    \n",
    "# g['classification'] = g['classification'].apply(replace_words_with_numbers)\n",
    "# g = g[g['predicted_formula'].isin(complex_ones)].copy()\n",
    "# # g = g[['predicted_formula','classification','formula_based_prediction','all_features_prediction']]\n",
    "# # idx = g['formula_based_prediction'] != g['all_features_prediction']\n",
    "# g['disagreement'] = g['classification'].apply(lambda x: len(set(x))>1)\n",
    "# idx = g['disagreement'] == True\n",
    "# g = g[idx].copy()\n",
    "\n",
    "# # melt and pivot classification, formula_based_prediction, all_features_prediction\n",
    "# # melted = g.melt(id_vars=['predicted_formula'], value_vars=['classification','formula_based_prediction','all_features_prediction'], var_name='type', value_name='values')\n",
    "# cols = ['classification','formula_based_prediction','all_features_prediction']\n",
    "# melted = g.explode(cols)\n",
    "# # melted =pd.pivot_table(melted,index=['predicted_formula'], columns=['type'], values='values')\n",
    "# # melted.reset_index(inplace=True)\n",
    "# correct_formula = melted['classification'] == melted['formula_based_prediction']\n",
    "# correct_all = melted['classification'] == melted['all_features_prediction']\n",
    "# incorrect_formula = melted['classification'] != melted['formula_based_prediction']\n",
    "# incorrect_all = melted['classification'] != melted['all_features_prediction']\n",
    "# print('Formula based prediction')\n",
    "# print('TP: {}, FP: {}, TN: {}, FN: {}'.format(correct_formula.sum(), incorrect_formula.sum(), 0, 0))\n",
    "# print('All features prediction')\n",
    "# print('TP: {}, FP: {}, TN: {}, FN: {}'.format(correct_all.sum(), incorrect_all.sum(), 0, 0))\n",
    "# podman-hpc run --rm -v -it /pscratch/sd/b/bpb/20230127_JGI_ER_508059_POM_final_IDX_C18_USDAY63675:/data docker.io/proteowizard/pwiz-skyline-i-agree-to-the-vendor-licenses wine msconvert /data/*.rawlen(temp), sum(temp['NPC#class_with_ms2']==1)/len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f5551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(1,2,figsize=(12,5))\n",
    "# idx = (final_df['soil_type']=='Agricultural Soil') & (final_df['priming']=='High')\n",
    "# plot_df = final_df[idx].copy()\n",
    "# temp = pd.merge(plot_df,node_data,on='original_index',how='left')\n",
    "# temp = temp[temp['NPC#class'].notna()]\n",
    "# temp.drop_duplicates('original_index',inplace=True)\n",
    "# temp_no_ms2 = temp.groupby('predicted_formula')['NPC#class'].count()\n",
    "\n",
    "# idx = (final_df['soil_type']=='Agricultural Soil') & (final_df['priming']=='High') & (final_df['has_ms2_evidence']==True)\n",
    "# plot_df = final_df[idx].copy()\n",
    "# temp = pd.merge(plot_df,node_data,on='original_index',how='left')\n",
    "# temp = temp[temp['NPC#class'].notna()]\n",
    "# temp.drop_duplicates('original_index',inplace=True)\n",
    "# temp_with_ms2 = temp.groupby('predicted_formula')['NPC#class'].count()\n",
    "\n",
    "# temp = pd.merge(temp_no_ms2, temp_with_ms2, left_index=True, right_index=True, how='inner', suffixes=('_no_ms2','_with_ms2'))\n",
    "# # .plot.scatter(x='NPC#class_no_ms2', y='NPC#class_with_ms2', ax=ax[0])\n",
    "# ax[0].scatter(x=temp['NPC#class_no_ms2'], y=temp['NPC#class_with_ms2'])\n",
    "# # ax[0].set_xlabel('Number of unique compound classes (all MS1 hits)')\n",
    "# # ax[0].set_ylabel('Number of unique compound classes (MS1 hits with MS2 evidence)')\n",
    "# ax[0].set_title('Compound classes per molecular formula\\nAgricultural Soil - High Priming')\n",
    "# # make it a perfect square and equal limits\n",
    "# ax[0].set_aspect('equal', adjustable='box')\n",
    "# max_val = max(temp['NPC#class_no_ms2'].max(), temp['NPC#class_with_ms2'].max())\n",
    "# ax[0].set_xlim(0, max_val+1)\n",
    "# ax[0].set_ylim(0, max_val+1)\n",
    "# ax[0].plot([0, max_val+1], [0, max_val+1], color='gray', linestyle='--')\n",
    "# ax[0].set_xlabel('MS1 evidence only')\n",
    "# ax[0].set_ylabel('With MS2 evidence')\n",
    "\n",
    "# ax[1].hist(temp['NPC#class_no_ms2'],bins=np.arange(1,21)-0.5, alpha=0.5, label='MS1 evidence only')\n",
    "# ax[1].hist(temp['NPC#class_with_ms2'],bins=np.arange(1,21)-0.5, alpha=0.5, label='With MS2 evidence')\n",
    "# ax[1].set_title('Distribution of Unique Compound Classes\\nAgricultural Soil - High Priming')\n",
    "# ax[1].set_xlabel('Number of Unique Compound Classes')\n",
    "# ax[1].set_ylabel('Frequency')\n",
    "# ax[1].legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a21bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fig,ax = plt.subplots(1,2,figsize=(10,5),sharex=True,sharey=True)\n",
    "# idx = (final_df['soil_type']=='Agricultural Soil') & (final_df['priming']=='High')\n",
    "# plot_df = final_df[idx].copy()\n",
    "# temp = pd.merge(plot_df,node_data,on='original_index',how='left')\n",
    "# temp = temp[temp['NPC#class'].notna()]\n",
    "# temp.drop_duplicates('original_index',inplace=True)\n",
    "# temp = temp.groupby('predicted_formula')['NPC#class'].count()\n",
    "# ax[0].hist(temp.values,bins=np.arange(1,31,1)-0.5)\n",
    "# my_str = f\"{(temp>1).sum()/len(temp):.1%} of formulas are associated\\nwith more than 1 compound class\"\n",
    "# ax[0].set_title(my_str)\n",
    "# ax[0].set_yscale('log')\n",
    "# ax[0].set_xlabel('Number of unique compound classes')\n",
    "# ax[0].set_ylabel('Number of predicted formulas')\n",
    "\n",
    "# idx = (final_df['soil_type']=='Agricultural Soil') & (final_df['priming']=='High') & (final_df['has_ms2_evidence']==True)\n",
    "# plot_df = final_df[idx].copy()\n",
    "# temp = pd.merge(plot_df,node_data,on='original_index',how='left')\n",
    "# temp = temp[temp['NPC#class'].notna()]\n",
    "# temp.drop_duplicates('original_index',inplace=True)\n",
    "# temp = temp.groupby('predicted_formula')['NPC#class'].count()\n",
    "# ax[1].hist(temp.values,bins=np.arange(1,31,1)-0.5)\n",
    "# my_str = f\"{(temp>1).sum()/len(temp):.1%} of formulas are associated\\nwith more than 1 compound class\"\n",
    "# ax[1].set_title(my_str)\n",
    "# ax[1].set_yscale('log')\n",
    "# ax[1].set_xlabel('Number of unique compound classes')\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea82caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fig,ax = plt.subplots(1,2,figsize=(10,5),sharex=True,sharey=True)\n",
    "# idx = (final_df['soil_type']=='Agricultural Soil') & (final_df['priming']=='High')\n",
    "# plot_df = final_df[idx].copy()\n",
    "# temp = pd.merge(plot_df,node_data,on='original_index',how='left')\n",
    "# temp = temp[temp['NPC#class'].notna()]\n",
    "# # temp.sort_values('has_ms2_evidence',ascending=False,inplace=True)\n",
    "# temp.drop_duplicates('original_index',inplace=True)\n",
    "# temp = temp.groupby('predicted_formula')['NPC#class'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# g = temp.sum(axis=1).sort_values(ascending=False)\n",
    "# ax[0].hist(g.values,bins=50)\n",
    "# my_str = f\"{(g>1).sum()/len(g):.1%} of formulas are associated\\nwith more than 1 compound class\"\n",
    "# ax[0].set_title(my_str)\n",
    "# ax[0].set_yscale('log')\n",
    "# ax[0].set_xlabel('Number of unique compound classes')\n",
    "# ax[0].set_ylabel('Number of predicted formulas')\n",
    "\n",
    "# idx = (final_df['soil_type']=='Agricultural Soil') & (final_df['priming']=='High') & (final_df['has_ms2_evidence']==True)\n",
    "# plot_df = final_df[idx].copy()\n",
    "# temp = pd.merge(plot_df,node_data,on='original_index',how='left')\n",
    "# temp = temp[temp['NPC#class'].notna()]\n",
    "# # temp.drop_duplicates('original_index',inplace=True)\n",
    "# temp = temp.groupby('predicted_formula')['NPC#class'].value_counts().unstack(fill_value=0)\n",
    "# g = temp.sum(axis=1).sort_values(ascending=False)\n",
    "# ax[1].hist(g.values,bins=50)\n",
    "# my_str = f\"{(g>1).sum()/len(g):.1%} of formulas are associated\\nwith more than 1 compound class\"\n",
    "# ax[1].set_title(my_str)\n",
    "# ax[1].set_yscale('log')\n",
    "# ax[1].set_xlabel('Number of unique compound classes')\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# row_terms = final_df['soil_type'].unique()\n",
    "# num_rows = len(row_terms)\n",
    "# col_terms = ['Unprimed','Low','High']\n",
    "# num_cols = len(col_terms)\n",
    "\n",
    "# classification_colors = {\n",
    "#     'produced': '#1f77b4',  # Muted blue\n",
    "#     'prefered': '#d62728',  # Muted red\n",
    "#     'ignored': '#7f7f7f'    # Gray\n",
    "# }\n",
    "\n",
    "# fig,ax = plt.subplots(num_rows,num_cols, figsize=(4*num_cols,4*num_rows), sharex=True, sharey=True)\n",
    "# for i,row in enumerate(row_terms):\n",
    "#     for j,col in enumerate(col_terms):\n",
    "#         ax_ij = ax[i,j]\n",
    "#         idx = (final_df['soil_type']==row) & (final_df['priming']==col)\n",
    "#         plot_df = final_df[idx].copy()\n",
    "#         # idx = plot_df['p']<0.005\n",
    "#         # plot_df.loc[idx,'p'] = 0.005 + np.random.rand(sum(idx))*0.001\n",
    "#         # idx = plot_df['fc']>5\n",
    "#         # plot_df.loc[idx,'fc'] = 5 + np.random.rand(sum(idx))\n",
    "#         # idx = plot_df['fc']<-5\n",
    "#         # plot_df.loc[idx,'fc'] = -5 - np.random.rand(sum(idx))\n",
    "#         sns.scatterplot(data=plot_df, x='fc', y=-np.log10(plot_df['p']), \n",
    "#                         hue='classification', \n",
    "#                         palette=classification_colors, \n",
    "#                         ax=ax_ij, alpha=0.7,legend=False)\n",
    "\n",
    "#         ax_ij.axhline(-np.log10(0.05), color='red', linestyle='--')\n",
    "#         ax_ij.axvline(0.5, color='blue', linestyle='--')\n",
    "#         ax_ij.axvline(-0.5, color='blue', linestyle='--')\n",
    "#         ax_ij.set_title(f'{row} {col}')\n",
    "#         if i == num_rows-1:\n",
    "#             ax_ij.set_xlabel('Log2 Fold Change (Day 7 / Day 0)', fontsize=14)\n",
    "#         else:\n",
    "#             ax_ij.set_xlabel('')\n",
    "#         if j == 0:\n",
    "#             ax_ij.set_ylabel('-Log10(p-value)')\n",
    "#         else:\n",
    "#             ax_ij.set_ylabel('')\n",
    "\n",
    "\n",
    "# handles = [mpatches.Patch(color=color, label=label) for label, color in classification_colors.items()]\n",
    "\n",
    "# # Place the legend outside the plot area\n",
    "# fig.legend(handles=handles, loc='center left', bbox_to_anchor=(1, 0.5), title='Classification')\n",
    "\n",
    "# # Adjust layout to make room for the legend\n",
    "# plt.tight_layout(rect=[0, 0, 0.9, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b627ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = pd.merge(ms1_df,node_data, on='original_index', how='left')\n",
    "# fig,ax = plt.subplots(1,2,figsize=(10,5),sharey=True)\n",
    "# mz_diff = temp['precursor_mz'] - temp['mz_centroid']\n",
    "# ax[0].hist(mz_diff,bins=100)\n",
    "# ax[0].set_xlabel('Mass difference (Da)')\n",
    "# ax[0].set_ylabel('Number of annotations')\n",
    "# mz_diff_ppm = mz_diff / temp['precursor_mz'] * 1e6\n",
    "# ax[1].hist(mz_diff_ppm,bins=100)\n",
    "# ax[1].set_xlabel('Mass difference (ppm)')\n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c47926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge in ms2 evidence\n",
    "# cols = ['original_index', 'has_ms2_evidence', 'project', \n",
    "#        'priming', 'soil_type']\n",
    "# temp = ms1_df[cols].copy()\n",
    "# temp.sort_values(by=['has_ms2_evidence'], ascending=False, inplace=True)\n",
    "# merge_cols = ['original_index', 'project','priming','soil_type']\n",
    "# temp = temp.drop_duplicates(subset=merge_cols, keep='first')\n",
    "# final_df = pd.merge(final_df, temp, on=merge_cols, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f46ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = pd.merge(node_data,final_df, on='original_index', how='left')\n",
    "# temp = temp[temp['soil_type']=='Agricultural Soil'].copy()\n",
    "# temp = temp[temp['priming']=='High'].copy()\n",
    "# temp = temp[pd.notna(temp['classification'])]\n",
    "\n",
    "# # for each formula, get the most common classification\n",
    "# most_common_classification = temp.groupby('predicted_formula')['classification'].apply(lambda x: x.mode()[0])\n",
    "# cols = ['original_index','predicted_formula','has_ms2_evidence','NPC#class']\n",
    "# temp = pd.merge(temp[cols], most_common_classification.rename('most_common_classification'), on='predicted_formula', how='left')\n",
    "\n",
    "# temp = pd.pivot_table(temp,index=['predicted_formula','most_common_classification'],columns='NPC#class', values='has_ms2_evidence', fill_value=None)\n",
    "# out = []\n",
    "# for i,row in temp.iterrows():\n",
    "#     if (sum(row==True)>0) & (sum(row==False)>0):\n",
    "#         d = {'predicted_formula': i[0], 'most_common_classification': i[1], 'num_classes_with_ms2_evidence': sum(row==True), 'num_classes_without_ms2_evidence': sum(row==False)}\n",
    "#         out.append(d)\n",
    "# final_temp = pd.DataFrame(out)\n",
    "# # final_temp.to_csv('formulas_with_multiple_classes_and_ms2_evidence.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = pd.merge(final_df, node_data, on='original_index', how='left')\n",
    "# temp = temp[temp['soil_type']=='Agricultural Soil'].copy()\n",
    "# temp = temp[temp['priming']=='High'].copy()\n",
    "# df_list = [d for _, d in temp.groupby(['predicted_formula','project', \n",
    "#        'priming', 'soil_type']) if (d.shape[0]>1) & (d['p'].min()<0.05)]\n",
    "# df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ddb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # for each df in df_list, I want to know if ms2 evidence informs the compound classification\n",
    "# results = []\n",
    "# for df in df_list:\n",
    "#     formula = df['predicted_formula'].values[0]\n",
    "#     project = df['project'].values[0]\n",
    "#     priming = df['priming'].values[0]\n",
    "#     soil_type = df['soil_type'].values[0]\n",
    "#     idx_ms2 = df['has_ms2_evidence']==True\n",
    "#     idx_no_ms2 = df['has_ms2_evidence']==False\n",
    "#     num_with_ms2 = idx_ms2.sum()\n",
    "#     num_without_ms2 = idx_no_ms2.sum()\n",
    "#     class_with_ms2 = df.loc[idx_ms2,'NPC#class'].unique()\n",
    "#     class_without_ms2 = df.loc[idx_no_ms2,'NPC#class'].unique()\n",
    "#     # same or different?\n",
    "#     same_classes = set(class_with_ms2).intersection(set(class_without_ms2))\n",
    "#     if len(same_classes)==0:\n",
    "#         ms2_informs_classification = True\n",
    "#     else:\n",
    "#         ms2_informs_classification = False\n",
    "#     results.append({'predicted_formula':formula,\n",
    "#                     'project':project,\n",
    "#                     'priming':priming,\n",
    "#                     'soil_type':soil_type,\n",
    "#                     'num_with_ms2':num_with_ms2,\n",
    "#                     'num_without_ms2':num_without_ms2,\n",
    "#                     'class_with_ms2':class_with_ms2,\n",
    "#                     'class_without_ms2':class_without_ms2,\n",
    "#                     'ms2_informs_classification':ms2_informs_classification})\n",
    "                    \n",
    "# results = pd.DataFrame(results)\n",
    "# results['ms2_informs_classification'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msbuddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
