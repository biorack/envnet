{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcfb431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Set up paths\n",
    "PYTHONPATH = \"/global/homes/b/bpb/repos/envnet\"\n",
    "os.environ['PYTHONPATH'] = PYTHONPATH\n",
    "\n",
    "# Import ENVnet\n",
    "import sys\n",
    "sys.path.insert(0, PYTHONPATH)\n",
    "from envnet.build import quick_envnet\n",
    "from envnet.config.build_config import BuildConfig\n",
    "from envnet.build import ENVnetBuilder\n",
    "\n",
    "output_dir = Path(os.path.join(PYTHONPATH, \"results\", \"temp_data\"))\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create config\n",
    "config = BuildConfig()\n",
    "# config.remblink_cutoff = 0.05\n",
    "# config.network_max_mz_difference = 5000.0\n",
    "config.min_cluster_size = 3\n",
    "print(config.max_rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe89b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "/global/homes/b/bpb/repos/envnet/scripts/my_files_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9cca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1389, 3)\n",
      "/global/cfs/cdirs/metatlas/projects/envnet_build_files/metatlas/20230223_EB_MdR_101544-059_SynDAC_20230223_QE144_C18-EP_USDAY72350_NEG_MS2_29_RS-HA-NA_1__18_deconvoluted.parquet\n",
      "True\n",
      "/global/cfs/cdirs/metatlas/projects/envnet_build_files/metatlas/20221219_EB_MdR_101544-059_PyrToDAC_20221219_EXP120A_C18-EP_USDAY63672_NEG_MS2_8_FAIII-1mg-NA_1__48_deconvoluted.parquet\n",
      "True\n",
      "/global/cfs/cdirs/metatlas/projects/envnet_build_files/metatlas/20230113_EB_MdR_101544-059_FRCgwater_20230105_EXP120A_C18-EP_USDAY72349_NEG_MS2_13_SRFA3-H2O-fil-NA-NA_1__18_deconvoluted.parquet\n",
      "True\n",
      "/global/cfs/cdirs/metatlas/projects/envnet_build_files/metatlas/20230113_EB_MdR_101544-059_FRCgwater_20230105_EXP120A_C18-EP_USDAY72349_NEG_MS2_14_SRFA3-H2O-unfil-NA-NA_1__36_deconvoluted.parquet\n",
      "True\n",
      "/global/cfs/cdirs/metatlas/projects/envnet_build_files/metatlas/20230113_EB_MdR_101544-059_FRCgwater_20230105_EXP120A_C18-EP_USDAY72349_NEG_MS2_15_SRFA3-MeOH-fil-NA-NA_1__12_deconvoluted.parquet\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "filename = '/global/homes/b/bpb/repos/envnet/scripts/my_files.csv'\n",
    "df = pd.read_csv(filename)\n",
    "# replace all text having 'carbon_network/raw_data' with 'envnet_build_files' in the entire dataframe\n",
    "for c in df.columns:\n",
    "    df[c] = df[c].str.replace('carbon_network/raw_data', 'envnet_build_files', regex=False)\n",
    "df = df[~df['parquet'].str.contains('MSV000081030')]\n",
    "idx = df['parquet'].str.contains('_deconvoluted.parquet')\n",
    "df.loc[~idx,'parquet'] = df.loc[~idx,'parquet'].str.replace('.parquet','_deconvoluted.parquet', regex=False)\n",
    "\n",
    "print(df.shape)\n",
    "df.to_csv('/global/homes/b/bpb/repos/envnet/scripts/build_files.csv', index=False)\n",
    "for f in df.head()['parquet'].tolist():\n",
    "    print(f)\n",
    "    print(os.path.exists(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6502eff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original_index', 'temp_index', 'rt', 'count', 'precursor_mz',\n",
       "       'sum_frag_intensity', 'max_frag_intensity', 'obs',\n",
       "       'isolated_precursor_mz', 'filename', 'basename',\n",
       "       'coisolated_precursor_count', 'coisolated_precursor_mz_list',\n",
       "       'assumed_adduct', 'predicted_formula', 'estimated_fdr',\n",
       "       'predicted_mass', 'mass_error', 'num_datapoints', 'peak_area',\n",
       "       'peak_height', 'mz_centroid', 'rt_peak', 'score', 'matches',\n",
       "       'library_formula', 'library_precursor_mz', 'inchi_key', 'compound_name',\n",
       "       'smiles', 'match_type', 'duplicate_cluster_index', 'num_duplicates',\n",
       "       'dbe', 'dbe_ai', 'dbe_ai_mod', 'ai_mod', 'ai', 'nosc', 'h_to_c',\n",
       "       'o_to_c', 'n_to_c', 'p_to_c', 'n_to_p', 'c', 'h', 'o', 'n', 's', 'p',\n",
       "       'has_library_match', 'is_singleton'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305658c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53833/3805686495.py:3: DtypeWarning: Columns (25,27,28,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  node_data = pd.read_csv(filename)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "709"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "filename = '/global/u2/b/bpb/repos/envnet/results/full_build_20250908_181404/envnet_node_data.csv'\n",
    "node_data = pd.read_csv(filename)\n",
    "node_data['inchi_key'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baddd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use recursive find to get all parquet files\n",
    "# use linux find since it is much faster than glob\n",
    "cmd = \"find /global/cfs/cdirs/metatlas/projects/envnet_build_files/ -name '*_deconvoluted.parquet'\"\n",
    "# run command\n",
    "import subprocess\n",
    "result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "files = result.stdout.splitlines()\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d265f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove MSV000081030 files.  These seem to be not environmental\n",
    "files = [f for f in files if \"MSV000081030\" not in f] # MetaboLights MTBLS144 - GNPS Dissolved organic matter produced by Thalassiosira pseudonana\n",
    "\n",
    "print(len(files))\n",
    "# files = [f for f in files if \"MSV000088008\" not in f]\n",
    "\n",
    "# # files = [f for f in files if \"DOM_Interlab-LCMS_Lab024\" not in f]\n",
    "# len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c083a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for f in files:\n",
    "    mzml_filename = f.replace(\"_deconvoluted.parquet\", \".mzML\")\n",
    "    h5_filename = f.replace(\"_deconvoluted.parquet\", \".h5\")\n",
    "    if not os.path.exists(mzml_filename):\n",
    "        print(f)\n",
    "    if not os.path.exists(h5_filename):\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# path_to_replace = '/global/cfs/cdirs/metatlas/projects/carbon_network/raw_data'\n",
    "# path_replace_with = '/global/cfs/cdirs/metatlas/projects/envnet_build_files'\n",
    "# for f in files:\n",
    "#     mzml_filename = f.replace(\"_deconvoluted.parquet\", \".mzML\")\n",
    "#     h5_filename = f.replace(\"_deconvoluted.parquet\", \".h5\")\n",
    "#     new_mzml_filename = mzml_filename.replace(path_to_replace, path_replace_with)\n",
    "#     new_h5_filename = h5_filename.replace(path_to_replace, path_replace_with)\n",
    "#     # copy the mzml and h5 file from the original path to the new path\n",
    "#     # make the sub directories\n",
    "#     os.makedirs(os.path.dirname(new_mzml_filename), exist_ok=True)\n",
    "#     shutil.copy(mzml_filename, new_mzml_filename)\n",
    "#     shutil.copy(h5_filename, new_h5_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for parquet_file in files:\n",
    "    temp_df = pd.read_parquet(parquet_file)\n",
    "    # max_rt = temp_df['rt'].max()\n",
    "    # filtered_rt_max = max_rt*0.7\n",
    "    # temp_df = temp_df[temp_df['rt']<filtered_rt_max]\n",
    "\n",
    "    # temp_df = temp_df[temp_df['rt']<config.max_rt]\n",
    "    df.append(temp_df)\n",
    "\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "# df = df[df['rt']<config.max_rt]\n",
    "# df = df[df['rt']>config.min_rt]\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parquet_file = '/global/cfs/cdirs/metatlas/projects/carbon_network/raw_data/metatlas/20220707_JGI_SB_503799_Permafrost_pilot_QE-HF_C18_USDAY63663_NEG_MSMS_10_55M-Subsection-MeOH_1_Rg80to1200-CE102040-soil-S1_Run26_deconvoluted.parquet'\n",
    "# # print(f\"Loading: {parquet_file}\")\n",
    "# # df = pd.read_parquet(parquet_file)\n",
    "\n",
    "\n",
    "# import glob\n",
    "# files1 = glob.glob('/global/cfs/cdirs/metatlas/projects/carbon_network/raw_data/metatlas/20220707_JGI_SB_503799_*_deconvoluted.parquet')\n",
    "# files2 = glob.glob('/global/cfs/cdirs/metatlas/projects/carbon_network/raw_data/massive/v06/MSV000092520/peak/*_deconvoluted.parquet')\n",
    "# files3 = glob.glob('/global/cfs/cdirs/metatlas/projects/carbon_network/raw_data/metatlas/20240112_JGI_MdR_109570-002_OMTSoil50g_Pilot_QEHF_C18_USDAY86082_NEG_MS2_1_soil-*_deconvoluted.parquet')\n",
    "# files = files1 + files2 + files3\n",
    "# df = []\n",
    "\n",
    "# for parquet_file in files:\n",
    "#     print(f\"Loading: {parquet_file}\")\n",
    "#     temp_df = pd.read_parquet(parquet_file)\n",
    "#     df.append(temp_df)\n",
    "\n",
    "# df = pd.concat(df, ignore_index=True)\n",
    "# df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c238ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(f\"Total spectra in file: {len(df)}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Precursor m/z range: {df['precursor_mz'].min():.3f} - {df['precursor_mz'].max():.3f}\")\n",
    "\n",
    "df.rename(columns={\n",
    "    'mdm_mz_vals': 'deconvoluted_spectrum_mz_vals',\n",
    "    'mdm_i_vals': 'deconvoluted_spectrum_intensity_vals',\n",
    "    'original_mz_vals':'original_spectrum_mz_vals',\n",
    "    'original_i_vals':'original_spectrum_intensity_vals'\n",
    "}, inplace=True)\n",
    "\n",
    "df['basename'] = df['filename'].apply(lambda x: os.path.basename(x))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb042604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 2. Select strategic spectra for testing\n",
    "# # Goal: Pick some that should cluster together and some that shouldn't\n",
    "\n",
    "# # Strategy 1: Pick spectra with similar m/z (should connect if similar fragmentation)\n",
    "# similar_mz_group = df[\n",
    "#     (df['precursor_mz'] >= 200) & (df['precursor_mz'] <= 210)\n",
    "# ].head(5)\n",
    "\n",
    "# # Strategy 2: Pick spectra with very different m/z (should NOT connect)  \n",
    "# different_mz_1 = df[\n",
    "#     (df['precursor_mz'] >= 300) & (df['precursor_mz'] <= 310)\n",
    "# ].head(3)\n",
    "\n",
    "# different_mz_2 = df[\n",
    "#     (df['precursor_mz'] >= 500) & (df['precursor_mz'] <= 510)\n",
    "# ].head(3)\n",
    "\n",
    "# # Strategy 3: Pick a few random ones\n",
    "# random_spectra = df.sample(n=min(5, len(df)), random_state=42)\n",
    "\n",
    "# # Combine all test spectra\n",
    "# test_spectra = pd.concat([\n",
    "#     similar_mz_group,\n",
    "#     different_mz_1, \n",
    "#     different_mz_2,\n",
    "#     random_spectra\n",
    "# ]).head(15).reset_index(drop=True)  # Limit to 15 spectra\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nSelected {len(test_spectra)} test spectra:\")\n",
    "# print(\"Similar m/z group (should potentially connect):\")\n",
    "# for _, row in similar_mz_group.iterrows():\n",
    "#     print(f\"  m/z: {row['precursor_mz']:.3f}\")\n",
    "\n",
    "# print(\"Different m/z groups (should be separate):\")\n",
    "# for _, row in different_mz_1.iterrows():\n",
    "#     print(f\"  m/z: {row['precursor_mz']:.3f}\")\n",
    "# for _, row in different_mz_2.iterrows():\n",
    "#     print(f\"  m/z: {row['precursor_mz']:.3f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b79ff",
   "metadata": {},
   "source": [
    "# this is from the log of running the full build from what is in the sbatch file\n",
    "\n",
    "```bash\n",
    "Found 6312 unique precursor m/z groups\n",
    "Counter: 0, Processing 59886 entries for m/z range 352.9960 to 353.1623, Found 664 unique spectra\n",
    "Counter: 1, Processing 56025 entries for m/z range 324.9888 to 325.1676, Found 510 unique spectra\n",
    "Counter: 2, Processing 55959 entries for m/z range 381.0351 to 381.1588, Found 551 unique spectra\n",
    "Counter: 3, Processing 55516 entries for m/z range 367.0203 to 367.1440, Found 615 unique spectra\n",
    "Counter: 4, Processing 54880 entries for m/z range 339.0132 to 339.1479, Found 472 unique spectra\n",
    "Counter: 5, Processing 50100 entries for m/z range 369.0166 to 369.1595, Found 623 unique spectra\n",
    "Counter: 6, Processing 49832 entries for m/z range 395.0509 to 395.1767, Found 633 unique spectra\n",
    "Counter: 7, Processing 49046 entries for m/z range 379.0115 to 379.1803, Found 543 unique spectra\n",
    "Counter: 8, Processing 44056 entries for m/z range 365.0063 to 365.1624, Found 450 unique spectra\n",
    "Counter: 9, Processing 43186 entries for m/z range 311.0153 to 311.2248, Found 644 unique spectra\n",
    "Counter: 10, Processing 40995 entries for m/z range 409.0668 to 409.1936, Found 616 unique spectra\n",
    "Counter: 11, Processing 39164 entries for m/z range 337.0303 to 337.1704, Found 391 unique spectra\n",
    "Counter: 12, Processing 37273 entries for m/z range 423.0173 to 423.1874, Found 651 unique spectra\n",
    "Counter: 13, Processing 34520 entries for m/z range 341.0252 to 341.1286, Found 439 unique spectra\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e23fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_spectra = df.copy()\n",
    "\n",
    "builder = ENVnetBuilder(config=config, verbose=True)\n",
    "# IMPORTANT: Preprocess the spectra\n",
    "print(\"Preprocessing spectra...\")\n",
    "test_spectra = builder.workflows.data_loader._preprocess_spectra(test_spectra)\n",
    "\n",
    "# Mock the data loading by directly setting the spectra\n",
    "# builder.workflows.data_loader.all_spectra = test_spectra.copy()\n",
    "\n",
    "# Run library matching\n",
    "print(\"Running library matching...\")\n",
    "# deconv_matches = builder.workflows.library_matcher.score_all_spectra(test_spectra, scoring_type='deconvoluted')\n",
    "# orig_matches = builder.workflows.library_matcher.score_all_spectra(test_spectra, scoring_type='original')\n",
    "# deconv_matches = pd.DataFrame(columns=['spectrum_id', 'library_id', 'score', 'scoring_type'])\n",
    "# orig_matches = pd.DataFrame(columns=['spectrum_id', 'library_id', 'score', 'scoring_type'])\n",
    "# Run clustering\n",
    "print(\"Running clustering...\")\n",
    "clustered_spectra = builder.workflows.clusterer.cluster_duplicate_spectra(test_spectra)\n",
    "\n",
    "## the cluster_duplicate_spectra is dead on identical to the full workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b463960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export clustered spectra to parquet\n",
    "filename = os.path.join( config.metadata_folder, \"toy_network_clustered_spectra-freshconversion.parquet\")\n",
    "clustered_spectra.to_parquet(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f65aaa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join( config.metadata_folder, \"toy_network_clustered_spectra-freshconversion.parquet\")\n",
    "clustered_spectra = pd.read_parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a58257a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENVnet Builder initialized\n",
      "Eliminating redundant spectra...\n",
      "Info: No deconvoluted library matches provided.\n",
      "Info: No original library matches provided.\n",
      "Warning: No library matches found at all\n",
      "Final dataset: 22128 unique spectra\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config.min_cluster_size = 4\n",
    "builder = ENVnetBuilder(config=config, verbose=True)\n",
    "\n",
    "\n",
    "# Eliminate redundant spectra\n",
    "print(\"Eliminating redundant spectra...\")\n",
    "node_data = builder.workflows.clusterer.eliminate_redundant_spectra(clustered_spectra)#.sample(frac=0.1))\n",
    "    # clustered_spectra, deconv_matches, orig_matches\n",
    "# )\n",
    "# This will be slightly different, but only in which spectra result in being chosen.  Below, they are chosen based on library matches, but here only on peak area\n",
    "# this is what the full workflow logged\n",
    "#    Identified 739 unique spectral clusters\n",
    "\n",
    "# 4. Eliminating redundant spectra...\n",
    "# Final dataset: 24713 unique spectra\n",
    "#    Reduced to 24713 unique representative spectra\n",
    "\n",
    "# 5. Building molecular network...\n",
    "# Building network from 24713 spectra...\n",
    "    # min_cluster_size: int = 5\n",
    "\n",
    "# 2: Final dataset: 123191 unique spectra\n",
    "# 5: Final dataset: 14696 unique spectra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1749e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin_width: 0.001\n",
      "chunk_size: 2000\n",
      "intensity_power: 0.5\n",
      "max_rt: 70.0\n",
      "min_cluster_size: 4\n",
      "min_matches: 3\n",
      "min_rt: 0.5\n",
      "min_score: 0.9\n",
      "mz_tol: 0.002\n",
      "network_max_mz_difference: 1000.0\n",
      "override_matches: 20\n",
      "remblink_cutoff: 0.05\n",
      "verbose: True\n"
     ]
    }
   ],
   "source": [
    "# for each attr in config if int or float, print the attr and its value\n",
    "for attr in dir(config):\n",
    "    if not attr.startswith(\"_\"):\n",
    "        value = getattr(config, attr)\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"{attr}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93840d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network...\n",
      "ENVnet Builder initialized\n",
      "Building network from 22128 spectra...\n",
      "Processing 12 chunks...\n",
      "Processing chunk 1/12\n",
      "Using model: /global/u2/b/bpb/repos/envnet/envnet/data/mdm_negative_random_forest.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 2/12\n",
      "Using model: /global/u2/b/bpb/repos/envnet/envnet/data/mdm_negative_random_forest.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 3/12\n",
      "Using model: /global/u2/b/bpb/repos/envnet/envnet/data/mdm_negative_random_forest.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 4/12\n",
      "Using model: /global/u2/b/bpb/repos/envnet/envnet/data/mdm_negative_random_forest.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 5/12\n",
      "Using model: /global/u2/b/bpb/repos/envnet/envnet/data/mdm_negative_random_forest.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 6/12\n",
      "Using model: /global/u2/b/bpb/repos/envnet/envnet/data/mdm_negative_random_forest.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 7/12\n",
      "Using model: /global/u2/b/bpb/repos/envnet/envnet/data/mdm_negative_random_forest.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 8/12\n",
      "Using model: /global/u2/b/bpb/repos/envnet/envnet/data/mdm_negative_random_forest.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 9/12\n",
      "Using model: /global/u2/b/bpb/repos/envnet/envnet/data/mdm_negative_random_forest.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 10/12\n",
      "Using model: /global/u2/b/bpb/repos/envnet/envnet/data/mdm_negative_random_forest.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 11/12\n",
      "Using model: /global/u2/b/bpb/repos/envnet/envnet/data/mdm_negative_random_forest.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 12/12\n",
      "Using model: /global/u2/b/bpb/repos/envnet/envnet/data/mdm_negative_random_forest.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/global/common/software/m2650/msbuddy/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total edges before filtering: 810775\n",
      "Edges after score cutoff (0.05): 810775\n",
      "Edges after m/z cutoff (1000.0): 810775\n",
      "Network nodes: 22128\n",
      "Network edges: 397235\n",
      "Connected components: 6302\n",
      "Nodes from original data in network: 22128/22128\n"
     ]
    }
   ],
   "source": [
    "# Build network\n",
    "print(\"Building network...\")\n",
    "# config.remblink_cutoff = 0.05\n",
    "# config.network_max_mz_difference = 5000.0\n",
    "\n",
    "builder = ENVnetBuilder(config=config, verbose=True)\n",
    "\n",
    "network = builder.workflows.network_builder.build_remblink_network(node_data)\n",
    "\n",
    "\n",
    "# drop singletons from networkx network\n",
    "components = nx.connected_components(network)\n",
    "\n",
    "# # 2. Create a list of actual subgraph objects for components that meet the size criteria\n",
    "large_subgraphs = [network.subgraph(c).copy() for c in components if len(c) > 10]\n",
    "\n",
    "# 3. Compose the large subgraphs into the final network\n",
    "if large_subgraphs:\n",
    "    network = nx.compose_all(large_subgraphs)\n",
    "\n",
    "output_network = os.path.join(output_dir, \"toy_network-no-MSV000081030-0p05-0p9-4-again.graphml\")\n",
    "nx.write_graphml(network, output_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envnet.build.mgf_tools import MGFGenerator\n",
    "print(\"\\nGenerating MGF files...\")\n",
    "mgf_generator = MGFGenerator(config)\n",
    "created_mgf_files = mgf_generator.create_mgf_files(\n",
    "    node_data=node_data,\n",
    "    network=network,\n",
    "    output_dir=str(output_dir)\n",
    ")\n",
    "\n",
    "print(\"MGF generation complete. Files created:\")\n",
    "for spec_type, file_path in created_mgf_files.items():\n",
    "    print(f\"  - {spec_type.capitalize()}: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c418fdbc",
   "metadata": {},
   "source": [
    "# here is the log from full build\n",
    "\n",
    "```bash\n",
    "Total edges before filtering: 195381\n",
    "Edges after score cutoff (0.05): 195381\n",
    "Edges after m/z cutoff (1000.0): 195381\n",
    "Network nodes: 24713\n",
    "Network edges: 86583\n",
    "Connected components: 13731\n",
    "Nodes from original data in network: 24713/24713\n",
    "Network: 24713 nodes, 86583 edges\n",
    "Connected components: 13731\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd27dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 20))\n",
    "# if network.number_of_nodes() > 0:\n",
    "#     # pos = nx.spring_layout(network, k=2, iterations=500)\n",
    "#     pos = nx.nx_agraph.graphviz_layout(network, prog='neato')\n",
    "\n",
    "#     # Draw nodes colored by precursor m/z\n",
    "#     node_colors = [network.nodes[node].get('precursor_mz', 0) for node in network.nodes()]\n",
    "    \n",
    "#     nodes = nx.draw_networkx_nodes(network, pos, \n",
    "#                             node_color=node_colors, \n",
    "#                             node_size=10,\n",
    "#                             cmap='viridis',\n",
    "#                             alpha=0.8,\n",
    "#                             vmin=150,\n",
    "#                             vmax=500)\n",
    "    \n",
    "#     nx.draw_networkx_edges(network, pos, alpha=0.25, width=1)\n",
    "    \n",
    "#     # Add labels with m/z values\n",
    "#     # labels = {node: f\"{network.nodes[node].get('precursor_mz', 0):.4f}\" \n",
    "#     #             for node in network.nodes()}\n",
    "#     # nx.draw_networkx_labels(network, pos, labels, font_size=8)\n",
    "    \n",
    "#     plt.colorbar(nodes, label='Precursor m/z')\n",
    "\n",
    "#     plt.title(f\"ENVnet Test Network\\n{network.number_of_nodes()} nodes, {network.number_of_edges()} edges\")\n",
    "    \n",
    "# else:\n",
    "#     plt.text(0.5, 0.5, \"No network generated\", \n",
    "#             horizontalalignment='center', verticalalignment='center')\n",
    "#     plt.title(\"ENVnet Test - No Network\")\n",
    "\n",
    "# plt.axis('off')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc2e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
