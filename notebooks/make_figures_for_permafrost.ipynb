{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e237023",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing requirements\n",
    "import pandas as pd\n",
    "# 1. Copy all .h5 and .mzML files from the job folders to a single output directory for easier access.\n",
    "# 2. Run the deconvolution to make <filename>_deconvoluted.parquet files.\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "# import ztest\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588438ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ENVnet reference data...\n",
      "  GraphML file: /global/homes/b/bpb/repos/envnet/results/full_build_20250908_181404/network_with_sirius.graphml\n",
      "  Deconvoluted MGF: /global/u2/b/bpb/repos/envnet/data//global/homes/b/bpb/repos/envnet/results/full_build_20250908_181404/envnet_deconvoluted_spectra.mgf\n",
      "  Original MGF: /global/u2/b/bpb/repos/envnet/data//global/homes/b/bpb/repos/envnet/results/full_build_20250908_181404/envnet_original_spectra.mgf\n",
      "Loaded 22128 ENVnet nodes\n"
     ]
    }
   ],
   "source": [
    "PYTHONPATH = \"/global/homes/b/bpb/repos/envnet\"\n",
    "if PYTHONPATH not in sys.path:\n",
    "    sys.path.insert(0, PYTHONPATH)\n",
    "\n",
    "from envnet.annotation.core import AnnotationEngine\n",
    "annotation_engine = AnnotationEngine()\n",
    "ref_dir = '/global/homes/b/bpb/repos/envnet/results/full_build_20250908_181404/'\n",
    "node_data = annotation_engine.load_envnet_reference(\n",
    "    graphml_file=os.path.join(ref_dir, \"network_with_sirius.graphml\"),\n",
    "    mgf_base_name=os.path.join(ref_dir, \"envnet\")\n",
    ")   \n",
    "cols = ['original_index','precursor_mz','inchi_key', 'compound_name', 'smiles','NPC#pathway', 'NPC#superclass', 'NPC#class','predicted_formula']\n",
    "node_data = node_data['nodes'][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b537a0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3602431, 10)\n",
      "(305275, 11)\n"
     ]
    }
   ],
   "source": [
    "ms1_filename = '/pscratch/sd/b/bpb/envnet_annotation_results/ms1_results_experiments_for_paper/ms1_annotations.parquet'\n",
    "ms2_filename = '/pscratch/sd/b/bpb/envnet_annotation_results/ms2_results_experiments_for_paper/ms2_deconvoluted_annotations.parquet'\n",
    "ms1_df = pd.read_parquet(ms1_filename)\n",
    "print(ms1_df.shape)\n",
    "ms2_cols = ['score_deconvoluted_match', 'matches_deconvoluted_match',\n",
    "       'original_index_deconvoluted_match',  'filename',\n",
    "        'mz_diff']\n",
    "ms2_df = pd.read_parquet(ms2_filename, columns=ms2_cols)\n",
    "# ms1_df = pd.merge(ms1_df, ms2_df, left_on=['lcmsrun_observed','original_index'], right_on=['filename','original_index_deconvoluted_match'], how='left')\n",
    "# ms1_df.drop(columns=['original_index_deconvoluted_match','filename','confidence_level','h5'], inplace=True)\n",
    "ms1_df['has_ms2_evidence'] = ms1_df['original_index'].isin(ms2_df['original_index_deconvoluted_match'].unique())\n",
    "ms1_df = ms1_df[ms1_df['lcmsrun_observed'].str.contains('T0-MeOH')]\n",
    "sample_metadata = pd.DataFrame(ms1_df['lcmsrun_observed'].unique(), columns=['lcmsrun_observed'])\n",
    "sample_metadata['basename'] = sample_metadata['lcmsrun_observed'].apply(lambda x: os.path.basename(x))\n",
    "sample_metadata['treatment'] = sample_metadata['basename'].apply(lambda x: x.split('_')[12])\n",
    "\n",
    "print(ms1_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dc30251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20M-T0-MeOH    3\n",
       "66M-T0-MeOH    3\n",
       "42M-T0-MeOH    3\n",
       "82M-T0-MeOH    3\n",
       "55M-T0-MeOH    3\n",
       "Name: treatment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_metadata['treatment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../results/full_build_20250908_181404/permafrost results gnps2'\n",
    "ms1_filename = os.path.join(data_path,'ms1_results','ms1_annotations.parquet')\n",
    "ms2_filename = os.path.join(data_path,'ms2_results','ms2_deconvoluted_annotations.parquet')\n",
    "analysis_filename = os.path.join(data_path,'analysis_results','statistical_results.csv')\n",
    "ms1_data = pd.read_parquet(ms1_filename)\n",
    "ms2_data = pd.read_parquet(ms2_filename)\n",
    "analysis_data = pd.read_csv(analysis_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd82f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1_data['basename'] = ms1_data['lcmsrun_observed'].apply(lambda x: os.path.basename(x).replace('.mzML',''))\n",
    "ms1_data['basename'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f279c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb07ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data[['original_index']] = analysis_data[['original_index']].astype(int)\n",
    "cols = ['original_index','log2_foldchange','p_value']\n",
    "temp = pd.merge(node_data, analysis_data[cols], on='original_index', how='inner')\n",
    "temp['classification'] = 'unclassified'\n",
    "idx1 = temp['p_value'] < 0.05\n",
    "idx2 = temp['log2_foldchange'] > 1\n",
    "temp.loc[idx1 & idx2, 'classification'] = 'increased'\n",
    "idx2 = temp['log2_foldchange'] < -1\n",
    "temp.loc[idx1 & idx2, 'classification'] = 'decreased'\n",
    "# temp = temp[temp['classification'] != 'unclassified']\n",
    "temp = temp.groupby('predicted_formula')['classification'].apply(list)\n",
    "temp = temp.reset_index()\n",
    "temp['classificiation_counts'] = temp['classification'].apply(lambda x: len(pd.unique(x)))\n",
    "# temp[temp['classificiation_counts'] > 1]\n",
    "temp.sort_values('classificiation_counts', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_indices = node_data.loc[node_data['predicted_formula']=='C16H18O9','original_index'].values\n",
    "print(node_data.loc[node_data['original_index'].isin(node_indices),'precursor_mz'].values)\n",
    "temp = ms1_data[ms1_data['original_index'].isin(node_indices)].copy()\n",
    "temp = temp.groupby(['original_index','lcmsrun_observed'])['peak_area'].sum().reset_index()\n",
    "temp['sample_type'] = temp['lcmsrun_observed'].apply(lambda x: os.path.basename(x).split('_')[12])\n",
    "temp['filename_short'] = temp['lcmsrun_observed'].apply(lambda x: '_'.join(os.path.basename(x).split('_')[12:16]) )\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots(figsize=(12,6))\n",
    "# sns.boxplot(data=temp, hue='sample_type', y='peak_area', ax=ax,x='original_index')\n",
    "sns.swarmplot(data=temp, hue='filename_short', y='peak_area', ax=ax,x='original_index')\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('C16H18O9 abundance in permafrost samples')\n",
    "ax.set_xlabel('Node original index')\n",
    "ax.set_ylabel('Peak Area (log scale)')\n",
    "# move legend outside\n",
    "ax.legend(bbox_to_anchor=(0.01, -0.3), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# PYTHONPATH = \"/global/homes/b/bpb/repos/envnet\"\n",
    "# if PYTHONPATH not in sys.path:\n",
    "#     sys.path.insert(0, PYTHONPATH)\n",
    "\n",
    "# from envnet.deconvolution.workflows import LCMSWorkflow\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Setup workflow once\n",
    "# workflow = LCMSWorkflow(do_buddy=True)\n",
    "\n",
    "# # Process multiple files\n",
    "# input_dir = Path('/global/cfs/cdirs/metatlas/projects/envnet_build_files/analysis_for_manuscript')\n",
    "# output_dir = Path('/global/cfs/cdirs/metatlas/projects/envnet_build_files/analysis_for_manuscript')\n",
    "# output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# for file_path in input_dir.glob('*.h5'):\n",
    "#     output_file = output_dir / f\"{file_path.stem}_deconvoluted.parquet\"\n",
    "#     if os.path.exists(output_file):\n",
    "#         continue\n",
    "#     df = workflow.run_full_workflow(str(file_path))\n",
    "#     if df is not None:\n",
    "#         df.drop(columns=['deconvoluted_spectrum','original_spectrum'], inplace=True, errors='ignore')\n",
    "#         df.to_parquet(output_file)\n",
    "#         print(f\"Processed: {file_path.name} -> {output_file.name}\")\n",
    "#     else:\n",
    "#         print(f\"No data found in: {file_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365cab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f83d8bd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ms1_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m nodes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2587640\u001b[39m,\u001b[38;5;241m2605164\u001b[39m]\n\u001b[1;32m      2\u001b[0m node_slice \u001b[38;5;241m=\u001b[39m node_data[node_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_index\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(nodes)]\n\u001b[0;32m----> 3\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mms1_data\u001b[49m[ms1_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_index\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(nodes)]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      4\u001b[0m temp \u001b[38;5;241m=\u001b[39m temp[temp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlcmsrun_observed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Run92\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      5\u001b[0m temp\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ms1_data' is not defined"
     ]
    }
   ],
   "source": [
    "nodes = [2587640,2605164]\n",
    "node_slice = node_data[node_data['original_index'].isin(nodes)]\n",
    "temp = ms1_data[ms1_data['original_index'].isin(nodes)].copy()\n",
    "temp = temp[temp['lcmsrun_observed'].str.contains('_Run92')]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4853250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cols = ['h5', 'parquet', 'environmental_subclass', 'lcmsrun_observed',\n",
    "       'original_file_type']\n",
    "import os\n",
    "my_dir = '/pscratch/sd/b/bpb/20231004_JGI_SB_503799_Pfrost_final_QEHF_C18_USDAY81384'\n",
    "my_files = os.listdir(my_dir)\n",
    "df = pd.DataFrame(columns=cols)\n",
    "df['h5'] = my_files\n",
    "df['lcmsrun_observed'] = df['h5'].apply(lambda x: os.path.basename(x).replace('.h5',''))\n",
    "df['parquet'] = df['h5'].apply(lambda x: x.replace('.h5','_deconvoluted.parquet'))\n",
    "df['environmental_subclass'] = 'not applicable'\n",
    "df['original_file_type'] = 'h5'\n",
    "output_filename = '/global/homes/b/bpb/repos/envnet/scripts/input_for_ms1-ms2_annotation-permafrost-for-paper.csv'\n",
    "df.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/global/homes/b/bpb/repos/envnet/scripts/build_files.csv')\n",
    "df['lcmsrun_observed'] = df['h5'].apply(lambda x: os.path.basename(x).replace('.h5',''))\n",
    "df['original_file_type'] = 'h5'\n",
    "\n",
    "df.to_csv('/global/homes/b/bpb/repos/envnet/scripts/build_files.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836924b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/global/homes/b/bpb/repos/envnet/scripts/input_for_ms1-ms2_annotation-experiments-for-paper.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785380f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_filename = '/global/cfs/cdirs/metatlas/raw_data/jgi/20231004_JGI_SB_503799_Pfrost_final_QEHF_C18_USDAY81384'\n",
    "basename = '20231004_JGI_SB_503799_Pfrost_final_QEHF_C18_USDAY81384_NEG_MS2_9A_55M-T0-MeOH_3_Rg80to1200-CE205060-perma-S1_Run148.h5'\n",
    "# basename = '20230822_JGI_SB_503799_Pfrost_final_QEHF_C18_USDAY81384_NEG_MS2_9A_55M-T0-MeOH_3_Rg80to1200-CE205060-perma-S1_Run148.h5'\n",
    "h5_filename = os.path.join(h5_filename, basename)\n",
    "raw_data = pd.read_hdf(h5_filename, key='ms1_neg')\n",
    "mz_tol= 10\n",
    "rt_min = 1\n",
    "rt_max = 9\n",
    "for i,row in node_slice.iterrows():\n",
    "    precursor_mz = row['precursor_mz']\n",
    "    diff = np.abs(raw_data['mz'] - precursor_mz) / precursor_mz * 1e6\n",
    "\n",
    "    idx = (diff < mz_tol) & (raw_data['rt'] >= rt_min) & (raw_data['rt'] <= rt_max)\n",
    "    extracted = raw_data[idx]\n",
    "    peak_area = extracted['i'].sum() / 1e8\n",
    "    print(f\"Node {row['original_index']}:{row['precursor_mz']:.4f} ({row['predicted_formula']}): Extracted Peak Area: {peak_area:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c314e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1_filename = '/pscratch/sd/b/bpb/envnet_annotation_results/ms1_results_experiments_for_paper/ms1_annotations.parquet'\n",
    "ms2_filename = '/pscratch/sd/b/bpb/envnet_annotation_results/ms2_results_experiments_for_paper/ms2_deconvoluted_annotations.parquet'\n",
    "ms1_df = pd.read_parquet(ms1_filename)\n",
    "print(ms1_df.shape)\n",
    "ms2_cols = ['score_deconvoluted_match', 'matches_deconvoluted_match',\n",
    "       'original_index_deconvoluted_match',  'filename',\n",
    "        'mz_diff']\n",
    "ms2_df = pd.read_parquet(ms2_filename, columns=ms2_cols)\n",
    "ms1_df = pd.merge(ms1_df, ms2_df, left_on=['lcmsrun_observed','original_index'], right_on=['filename','original_index_deconvoluted_match'], how='inner')\n",
    "ms1_df.drop(columns=['original_index_deconvoluted_match','filename','confidence_level','h5'], inplace=True)\n",
    "sample_metadata = pd.DataFrame(ms1_df['lcmsrun_observed'].unique(), columns=['lcmsrun_observed'])\n",
    "sample_metadata['basename'] = sample_metadata['lcmsrun_observed'].apply(lambda x: os.path.basename(x))\n",
    "sample_metadata['treatment'] = sample_metadata['basename'].apply(lambda x: x.split('_')[11])\n",
    "print(ms1_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_metadata = pd.DataFrame(ms1_df['lcmsrun_observed'].unique(), columns=['lcmsrun_observed'])\n",
    "sample_metadata['basename'] = sample_metadata['lcmsrun_observed'].apply(lambda x: os.path.basename(x))\n",
    "sample_metadata['project'] = sample_metadata['basename'].apply(lambda x: x.split('_')[4])\n",
    "sample_metadata['treatment'] = sample_metadata['basename'].apply(lambda x: x.split('_')[12])\n",
    "\n",
    "sample_metadata['timepoint'] = None\n",
    "idx = sample_metadata['treatment'].str.contains('-d7', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'timepoint'] = 7\n",
    "idx = sample_metadata['treatment'].str.contains('-day7', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'timepoint'] = 7\n",
    "idx = sample_metadata['treatment'].str.contains('-d0', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'timepoint'] = 0\n",
    "idx = sample_metadata['treatment'].str.contains('-day0', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'timepoint'] = 0\n",
    "\n",
    "sample_metadata['priming'] = None\n",
    "idx = sample_metadata['treatment'].str.endswith('-NA')\n",
    "sample_metadata.loc[idx,'priming'] = 'Unprimed'\n",
    "idx = sample_metadata['treatment'].str.contains('-na-', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'priming'] = 'Unprimed'\n",
    "idx = sample_metadata['treatment'].str.contains('-natcom-salts', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'priming'] = 'Unprimed'\n",
    "idx = sample_metadata['treatment'].str.endswith('-Lo')\n",
    "sample_metadata.loc[idx,'priming'] = 'Low'\n",
    "idx = sample_metadata['treatment'].str.contains('-0p05xnldm-', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'priming'] = 'Low'\n",
    "idx = sample_metadata['treatment'].str.endswith('-Hi')\n",
    "sample_metadata.loc[idx,'priming'] = 'High'\n",
    "idx = sample_metadata['treatment'].str.contains('-0p5xnldm-', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'priming'] = 'High'\n",
    "\n",
    "sample_metadata['soil_type'] = None\n",
    "idx = sample_metadata['treatment'].str.contains('supern-wave-natcom', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'soil_type'] = 'Potting Soil'\n",
    "idx = sample_metadata['treatment'].str.contains('omt1d2', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'soil_type'] = 'Agricultural Soil'\n",
    "idx = sample_metadata['treatment'].str.contains('h4171', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'soil_type'] = 'H4171 lignin'\n",
    "idx = sample_metadata['treatment'].str.contains('h4161', regex=False,case=False)\n",
    "sample_metadata.loc[idx,'soil_type'] = 'H4161 lignin'\n",
    "\n",
    "cols = ['timepoint','priming','soil_type']\n",
    "idx = pd.notna(sample_metadata[cols]).all(axis=1)\n",
    "sample_metadata = sample_metadata[idx].copy()\n",
    "sample_metadata.drop(columns=['basename','treatment'], inplace=True)\n",
    "ms1_df = pd.merge(ms1_df, sample_metadata, on='lcmsrun_observed', how='inner')\n",
    "print(ms1_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99977a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_filter(df,project,column_name):\n",
    "    df = df[~df[column_name].str.contains('exctrl|qc|txctrl',case=False)]\n",
    "    if project == 'Potting Soil':\n",
    "        return df[df[column_name].str.contains('6uL') & df[column_name].str.contains('NatCom') & df[column_name].str.contains('Day0|Day7')]\n",
    "    if project == 'Agricultural Soil NA':\n",
    "        return df[df[column_name].str.contains('NatCom') & df[column_name].str.contains('NA')]\n",
    "    if project == 'Agricultural Soil Lo':\n",
    "        return df[df[column_name].str.contains('NatCom') & df[column_name].str.contains('Lo')]\n",
    "    if project == 'Agricultural Soil':\n",
    "        return df[df[column_name].str.contains('NatCom') & df[column_name].str.contains('Hi')]\n",
    "    elif project == 'soil-ppl':\n",
    "        return df[df[column_name].str.contains('Run15')]\n",
    "    # elif project == 'syncom-exudates':\n",
    "        # return \n",
    "    elif project == 'century-exp':\n",
    "        return df[df[column_name].str.contains('omt|cmt',case=False)]\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "def tost(control, treatment, margin=0.25):\n",
    "    # Perform two one-sided tests.  Note that this\n",
    "    # is specific for testing if treatment is not\n",
    "    # the same as control.  If you control mean is\n",
    "    # near zero this will not work.\n",
    "    m = np.mean(control)\n",
    "    lower_margin =  -1*margin*m\n",
    "    upper_margin = margin*m\n",
    "    _, p_value_lower = ztest(control, treatment, value=lower_margin, alternative='larger')\n",
    "    _, p_value_upper = ztest(control, treatment, value=upper_margin, alternative='smaller')\n",
    "    return max(p_value_lower, p_value_upper)\n",
    "\n",
    "def do_ttest(df,control_group,treatment_group,do_split=True,min_intensity=1e6,margin=0.2):\n",
    "    p = pd.pivot_table(df,index=['lcmsrun_observed','timepoint'],values='peak_area',columns='original_index')\n",
    "    p.fillna(1e5,inplace=True)     \n",
    "    cols = p.columns\n",
    "    p.reset_index(inplace=True,drop=False)\n",
    "\n",
    "    ttest = []\n",
    "    for c in cols:\n",
    "        idx1 = p['timepoint']==treatment_group\n",
    "        idx2 = p['timepoint']==control_group\n",
    "        treatment = p[c][idx1].values\n",
    "        control = p[c][idx2].values\n",
    "        mean_treatment = 1+treatment.mean()\n",
    "        mean_control = 1+control.mean()\n",
    "        if (mean_treatment<min_intensity) & (mean_control<min_intensity):\n",
    "            continue\n",
    "        fold_change = np.log2(mean_treatment/mean_control)\n",
    "        t,p_value = ttest_ind(treatment, control)\n",
    "        # Two One-Sided Tests (TOST) procedure using 50% margin\n",
    "        p_tost = tost(control,treatment, margin=0.5)\n",
    "        ttest.append({'original_index':c,'t':t,'p':p_value,'fc':fold_change,'tost':p_tost,\n",
    "                      'mean_treatment':mean_treatment,'mean_control':mean_control,\n",
    "                      'treatment_vals':treatment,\n",
    "                      'control_vals':control})\n",
    "    ttest = pd.DataFrame(ttest)\n",
    "    return ttest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ms1_df = [g for _, g in ms1_df.groupby(['project','priming','soil_type'])]\n",
    "out = []\n",
    "for g_df in grouped_ms1_df:\n",
    "    project = g_df['project'].values[0]\n",
    "    priming = g_df['priming'].values[0]\n",
    "    soil_type = g_df['soil_type'].values[0]\n",
    "    print(f'Processing {project} {priming} {soil_type}')\n",
    "    ttest = do_ttest(g_df,0,7,min_intensity=1e7)\n",
    "    ttest['project'] = project\n",
    "    ttest['priming'] = priming\n",
    "    ttest['soil_type'] = soil_type\n",
    "    idx_prefered = (ttest['fc']<-0.5) & (ttest['p']<0.05)\n",
    "    idx_produced = (ttest['fc']>0.5) & (ttest['p']<0.05)\n",
    "    idx_ignored = ttest['tost']<0.05\n",
    "    ttest['classification'] = None\n",
    "    ttest.loc[idx_prefered,'classification'] = 'prefered'\n",
    "    ttest.loc[idx_produced,'classification'] = 'produced'\n",
    "    ttest.loc[idx_ignored,'classification'] = 'ignored'\n",
    "    out.append(ttest)\n",
    "final_df = pd.concat(out, ignore_index=True)\n",
    "print(final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc00182",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = final_df[final_df['classification'].notna()]\n",
    "idx1 = temp['project'] == 'Agricultural Soil'\n",
    "# idx2 = temp['classification'] == 'prefered'\n",
    "# idx3 = temp['classification'] == 'produced'\n",
    "temp = temp[idx1].copy()# & (idx2 | idx3)].copy()\n",
    "temp = pd.merge(temp, node_data, on='original_index', how='left')\n",
    "temp = temp.groupby('predicted_formula')['classification'].value_counts().unstack(fill_value=0)\n",
    "print(\"There are {} formulas that are prefered, produced or ignored\".format(len(temp)))\n",
    "idx1 = (temp>0).sum(axis=1)>1\n",
    "print('There are {} formulas that are prefered and produced or ignored'.format(temp[idx1].shape[0]))\n",
    "# temp = temp[(temp['prefered']>1) & (temp['produced']>1)].copy()\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd818ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "usecols = ['original_index', 't', 'p', 'fc', 'tost',\n",
    "       'mean_treatment', 'mean_control', 'treatment_vals', 'control_vals',\n",
    "       'project', 'priming', 'soil_type', 'classification',\n",
    "       'summary_classification', 'node_order_index', 'deconvoluted_spectrum',\n",
    "       'all_features_prediction', 'formula_based_prediction']\n",
    "prediction_df = pd.read_csv('training_data_with_predictions_full.csv', usecols=usecols)\n",
    "\n",
    "prediction_df = prediction_df[prediction_df['classification'].notna()]\n",
    "idx1 = prediction_df['project'] == 'Agricultural Soil'\n",
    "\n",
    "\n",
    "prediction_df = pd.merge(prediction_df, node_data, on='original_index', how='left')\n",
    "\n",
    "g = prediction_df.groupby('predicted_formula')['classification'].value_counts().unstack(fill_value=0)\n",
    "print(\"There are {} formulas that are prefered, produced or ignored\".format(len(g)))\n",
    "idx1 = (g>0).sum(axis=1)>1\n",
    "complex_ones = g[idx1].index.tolist()\n",
    "print('There are {} formulas that are prefered and produced or ignored'.format(g[idx1].shape[0]))\n",
    "# temp = temp[(temp['prefered']>1) & (temp['produced']>1)].copy()\n",
    "g = prediction_df.groupby('predicted_formula')[['classification','formula_based_prediction','all_features_prediction']].agg(lambda x: tuple(x)).reset_index()\n",
    "def replace_words_with_numbers(x):\n",
    "    out = []\n",
    "    for xx in x:\n",
    "        if xx=='prefered':\n",
    "            out.append(0)\n",
    "        elif xx=='produced':\n",
    "            out.append(0)\n",
    "        elif xx=='ignored':\n",
    "            out.append(1)\n",
    "    return tuple(out)\n",
    "    \n",
    "g['classification'] = g['classification'].apply(replace_words_with_numbers)\n",
    "g = g[g['predicted_formula'].isin(complex_ones)].copy()\n",
    "# g = g[['predicted_formula','classification','formula_based_prediction','all_features_prediction']]\n",
    "# idx = g['formula_based_prediction'] != g['all_features_prediction']\n",
    "g['disagreement'] = g['classification'].apply(lambda x: len(set(x))>1)\n",
    "idx = g['disagreement'] == True\n",
    "g = g[idx].copy()\n",
    "\n",
    "# melt and pivot classification, formula_based_prediction, all_features_prediction\n",
    "# melted = g.melt(id_vars=['predicted_formula'], value_vars=['classification','formula_based_prediction','all_features_prediction'], var_name='type', value_name='values')\n",
    "cols = ['classification','formula_based_prediction','all_features_prediction']\n",
    "melted = g.explode(cols)\n",
    "# melted =pd.pivot_table(melted,index=['predicted_formula'], columns=['type'], values='values')\n",
    "# melted.reset_index(inplace=True)\n",
    "correct_formula = melted['classification'] == melted['formula_based_prediction']\n",
    "correct_all = melted['classification'] == melted['all_features_prediction']\n",
    "incorrect_formula = melted['classification'] != melted['formula_based_prediction']\n",
    "incorrect_all = melted['classification'] != melted['all_features_prediction']\n",
    "print('Formula based prediction')\n",
    "print('TP: {}, FP: {}, TN: {}, FN: {}'.format(correct_formula.sum(), incorrect_formula.sum(), 0, 0))\n",
    "print('All features prediction')\n",
    "print('TP: {}, FP: {}, TN: {}, FN: {}'.format(correct_all.sum(), incorrect_all.sum(), 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8008ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "podman-hpc run --rm -v -it /pscratch/sd/b/bpb/20230127_JGI_ER_508059_POM_final_IDX_C18_USDAY63675:/data docker.io/proteowizard/pwiz-skyline-i-agree-to-the-vendor-licenses wine msconvert /data/*.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "row_terms = final_df['soil_type'].unique()\n",
    "num_rows = len(row_terms)\n",
    "col_terms = ['Unprimed','Low','High']\n",
    "num_cols = len(col_terms)\n",
    "\n",
    "classification_colors = {\n",
    "    'produced': '#1f77b4',  # Muted blue\n",
    "    'prefered': '#d62728',  # Muted red\n",
    "    'ignored': '#7f7f7f'    # Gray\n",
    "}\n",
    "\n",
    "fig,ax = plt.subplots(num_rows,num_cols, figsize=(4*num_cols,4*num_rows), sharex=True, sharey=True)\n",
    "for i,row in enumerate(row_terms):\n",
    "    for j,col in enumerate(col_terms):\n",
    "        ax_ij = ax[i,j]\n",
    "        idx = (final_df['soil_type']==row) & (final_df['priming']==col)\n",
    "        plot_df = final_df[idx].copy()\n",
    "        # idx = plot_df['p']<0.005\n",
    "        # plot_df.loc[idx,'p'] = 0.005 + np.random.rand(sum(idx))*0.001\n",
    "        # idx = plot_df['fc']>5\n",
    "        # plot_df.loc[idx,'fc'] = 5 + np.random.rand(sum(idx))\n",
    "        # idx = plot_df['fc']<-5\n",
    "        # plot_df.loc[idx,'fc'] = -5 - np.random.rand(sum(idx))\n",
    "        sns.scatterplot(data=plot_df, x='fc', y=-np.log10(plot_df['p']), \n",
    "                        hue='classification', \n",
    "                        palette=classification_colors, \n",
    "                        ax=ax_ij, alpha=0.7,legend=False)\n",
    "\n",
    "        ax_ij.axhline(-np.log10(0.05), color='red', linestyle='--')\n",
    "        ax_ij.axvline(0.5, color='blue', linestyle='--')\n",
    "        ax_ij.axvline(-0.5, color='blue', linestyle='--')\n",
    "        ax_ij.set_title(f'{row} {col}')\n",
    "        if i == num_rows-1:\n",
    "            ax_ij.set_xlabel('Log2 Fold Change (Day 7 / Day 0)')\n",
    "        else:\n",
    "            ax_ij.set_xlabel('')\n",
    "        if j == 0:\n",
    "            ax_ij.set_ylabel('-Log10(p-value)')\n",
    "        else:\n",
    "            ax_ij.set_ylabel('')\n",
    "\n",
    "\n",
    "handles = [mpatches.Patch(color=color, label=label) for label, color in classification_colors.items()]\n",
    "\n",
    "# Place the legend outside the plot area\n",
    "fig.legend(handles=handles, loc='center left', bbox_to_anchor=(1, 0.5), title='Classification')\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('training data for stability model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a99f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,all_axes = plt.subplots(figsize=(20,6),nrows=1,ncols=len(final_df['soil_type'].unique()), sharey=True)\n",
    "counter = 0\n",
    "class_term = 'NPC#class'\n",
    "counter = 0\n",
    "for soil_type in final_df['soil_type'].unique():\n",
    "    ax = all_axes[counter]\n",
    "    # make a stacked bar chart of the compound classes for each classification\n",
    "\n",
    "    idx1 = final_df['classification'].notna()\n",
    "    idx2 = final_df['soil_type'] == soil_type\n",
    "    idx = idx1 & idx2\n",
    "    classified_indices = final_df[idx].groupby('classification')['original_index'].unique().to_dict()\n",
    "    out = []\n",
    "    for key, value in classified_indices.items():\n",
    "        idx = node_data['original_index'].isin(value)\n",
    "        temp = node_data.loc[idx,class_term].value_counts()\n",
    "        temp = temp.to_frame().reset_index().rename(columns={'index':'class','NPC#class':'count'})\n",
    "        temp['classification'] = key\n",
    "        temp['total'] = temp['count'].sum()\n",
    "        temp['fraction'] = temp['count']/temp['total']\n",
    "        out.append(temp)\n",
    "    class_df = pd.concat(out, ignore_index=True)\n",
    "    class_df = class_df.pivot(index='classification', columns='class', values='fraction')\n",
    "    class_df.fillna(0,inplace=True)\n",
    "    cols = class_df.columns[class_df.sum(axis=0)>0.05]\n",
    "    class_df = class_df[cols]\n",
    "    # sum each row to one\n",
    "    class_df = class_df.div(class_df.sum(axis=1), axis=0)\n",
    "\n",
    "    class_df.plot(kind='bar', stacked=True, ax=ax, color=sns.color_palette(\"tab20\", n_colors=len(class_df.columns)))\n",
    "    ax.set_ylabel('Fraction of Annotations')\n",
    "    ax.set_xlabel('Compound Class')\n",
    "    ax.set_title(f'Distribution of Compound Classes by Classification\\n({soil_type})')\n",
    "    # plt.xticks(rotation=45, ha='right')\n",
    "    # move legend outside of plot\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    counter = counter + 1\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee34c5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
