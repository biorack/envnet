{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyteomics import mgf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pingouin as pg\n",
    "import seaborn as sns\n",
    "sys.path.insert(0,'/global/homes/b/bpb/repos/metatlas')\n",
    "from metatlas.io import feature_tools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/public_and_internal_files_with_massive_and_redu.tsv', sep='\\t')\n",
    "\n",
    "df = df[~df['buddy'].str.contains('qc',case=False)]\n",
    "df = df[~df['buddy'].str.contains('blank',case=False)]\n",
    "# df['keywords'] = df['keywords'].apply(lambda x: x.split('###') if type(x)==str else [])\n",
    "# df['keyword_DOM'] = df['keywords'].apply(lambda x: True if (('dom' in x) | ('organic matter' in x) | ('soil' in x)) else False)\n",
    "idx1 = (df['in_massive_dom_list'])# | df['keyword_DOM']\n",
    "# idx2 = (df['SampleType']=='plant')\n",
    "\n",
    "# idx2 = df['data_dir']=='/global/cfs/cdirs/metatlas/projects/massive_data_for_scn'\n",
    "idx3 = df['data_dir']=='/global/cfs/cdirs/metatlas/projects/rawdata_for_scn'\n",
    "# df = df[(idx3)]# | (idx1)] #  | (idx2)\n",
    "df = df[(idx3) | (idx1)] #  | (idx2)\n",
    "# df = df[df['SampleType']=='plant']\n",
    "file_metadata = df.copy()\n",
    "\n",
    "\n",
    "out_dir = '/global/cfs/cdirs/metatlas/projects/carbon_network/raw_data'\n",
    "temp_files = df['h5'].tolist()\n",
    "files = []\n",
    "for f in temp_files:\n",
    "    base_dir = os.path.dirname(f)\n",
    "    base_name = os.path.basename(f)\n",
    "    new_dir = os.path.join(out_dir,base_dir)\n",
    "    new_name = os.path.join(new_dir,base_name)\n",
    "    files.append(new_name)\n",
    "    if not os.path.isfile(new_name):\n",
    "        print('File Not Found!')\n",
    "        print(new_name)\n",
    "        print(f)\n",
    "files = pd.DataFrame(files,columns=['filename'])\n",
    "print(files.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = nx.read_graphml('../data/structural_clusters_network.graphml')\n",
    "# G = nx.read_graphml('../data/network.graphml')\n",
    "G = nx.read_graphml('/global/homes/b/bpb/repos/scndb/build/CarbonNetwork.graphml')\n",
    "df = dict(G.nodes(data=True))\n",
    "df = pd.DataFrame(df).T\n",
    "df.index.name = 'node_id'\n",
    "df.reset_index(inplace=True,drop=False)\n",
    "df.head()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.notna(df['pathway_results_propagated'])\n",
    "sum(idx) - temp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[['h_to_c','o_to_c','pathway_results','formula_identity','predicted_formula']]\n",
    "temp = temp[pd.notna(temp['predicted_formula'])]\n",
    "temp = temp[pd.notna(temp['h_to_c'])]\n",
    "temp = temp[pd.notna(temp['o_to_c'])]\n",
    "temp.fillna('Unknown',inplace=True)\n",
    "temp.shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = temp['predicted_formula']!=temp['formula_identity']\n",
    "temp[idx].shape[0]/temp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 10))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the colors for each category\n",
    "# color_map = {'environment': 'red', 'plant': 'blue'}\n",
    "\n",
    "# Map the colors to the 'origin' column\n",
    "u = temp['pathway_results'].unique()\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "# Create a scatter plot of the PCA results\n",
    "for uu in u:\n",
    "    idx = temp['pathway_results']==uu\n",
    "    ax.plot(temp.loc[idx,'o_to_c'], temp.loc[idx,'h_to_c'],'.',markersize=10,label=uu,alpha=0.7)\n",
    "    \n",
    "ax.set_xlim(0,1.2)\n",
    "ax.set_ylim(0,2.5)\n",
    "ax.legend(bbox_to_anchor=(1.01, 1.01),loc='upper left')\n",
    "ax.set_xlabel('O/C ratio',fontsize=18)\n",
    "ax.set_ylabel('H/C ratio',fontsize=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "429/(12*150)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pd.notna(df['inchi_key_identity'])),sum(pd.notna(df['pathway_results'])),df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ppm_tolerance = 5\n",
    "mz_tol = 0.002\n",
    "rt_min = 1\n",
    "rt_max = 100\n",
    "\n",
    "atlas = df[['node_id','precursor_mz']].copy()\n",
    "atlas.rename(columns={'precursor_mz':'mz','node_id':'label'},inplace=True)\n",
    "atlas['rt_min'] = rt_min\n",
    "atlas['rt_max'] = rt_max\n",
    "atlas['mz_tolerance'] = mz_tol\n",
    "atlas['rt_tolerance'] = 100\n",
    "atlas['ppm_tolerance'] = ppm_tolerance\n",
    "atlas['extra_time'] = 0\n",
    "atlas['rt_peak'] = (rt_min+rt_max)/2  \n",
    "atlas['group_index'] = ft.group_consecutive(atlas['mz'].values[:],\n",
    "                                    stepsize=ppm_tolerance,\n",
    "                                    do_ppm=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = []\n",
    "counter = 0\n",
    "# files = files.sample(150)\n",
    "for f in files['filename']:\n",
    "    try:\n",
    "        d = ft.get_atlas_data_from_file(f,atlas,desired_key='ms1_neg')\n",
    "    except:\n",
    "        print('Can not read',f)\n",
    "        continue\n",
    "    d = d[d['in_feature']==True].groupby('label').apply(ft.calculate_ms1_summary).reset_index()\n",
    "    d['filename'] = f\n",
    "    out.append(d)\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "out = pd.concat(out)\n",
    "out.rename(columns={'label':'node_id'},inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.to_pickle('/global/cfs/cdirs/metatlas/projects/carbon_network/temp_data/all_berkeley_lab_data.pkl')\n",
    "# out.to_pickle('/global/cfs/cdirs/metatlas/projects/carbon_network/temp_data/century_experiment_data.pkl')\n",
    "# out.to_pickle('/global/cfs/cdirs/metatlas/projects/carbon_network/temp_data/wavestab3_data.pkl')\n",
    "# out.to_pickle('/global/cfs/cdirs/metatlas/projects/carbon_network/temp_data/plantmasst_data.pkl')\n",
    "out.to_pickle('/global/cfs/cdirs/metatlas/projects/carbon_network/temp_data/allCarbonData.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = pd.read_pickle('/global/cfs/cdirs/metatlas/projects/carbon_network/temp_data/all_berkeley_lab_data.pkl')\n",
    "# out = pd.read_pickle('/global/cfs/cdirs/metatlas/projects/carbon_network/temp_data/wavestab3_data.pkl')\n",
    "# out = pd.read_pickle('/global/cfs/cdirs/metatlas/projects/carbon_network/temp_data/century_experiment_data.pkl')\n",
    "out = pd.read_pickle('/global/cfs/cdirs/metatlas/projects/carbon_network/temp_data/allCarbonData.pkl')\n",
    "out_plant = pd.read_pickle('/global/cfs/cdirs/metatlas/projects/carbon_network/temp_data/plantmasst_data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spectrum(out):\n",
    "    # y_col = 'peak_height'\n",
    "    y_col = 'peak_area'\n",
    "    cols = ['mz_centroid',y_col]\n",
    "    # & (out['num_datapoints']>1) & (out['peak_height']<1e80) & (out['peak_height']>1)\n",
    "    y = out[(out['mz_centroid']>150)].groupby('node_id')[cols].median()\n",
    "    # y = y[y['peak_area']<1e9]\n",
    "    y.reset_index(inplace=True,drop=True)\n",
    "    fig, ax = plt.subplots(figsize=(15,4))\n",
    "    ax.vlines(y['mz_centroid'],0*y[y_col],y[y_col])\n",
    "    # ax.set_xlim(300.0,320)\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "make_spectrum(out)\n",
    "make_spectrum(out_plant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['origin'] = 'environment'\n",
    "out_plant['origin'] = 'plant'\n",
    "out = pd.concat([out,out_plant])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = out[['filename','origin']].drop_duplicates()\n",
    "origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def make_dataframe(out,fill_value = 0):\n",
    "    min_value = 0\n",
    "    # out['sampletype'] = out['filename'].apply(lambda x: os.path.basename(x))\n",
    "    node_data = pd.pivot_table(out,index=['node_id'],columns=['filename'],values='peak_area',aggfunc=np.mean,fill_value=fill_value)\n",
    "    # cols = node_data.columns\n",
    "    # node_data['lbnl-carbon'] = node_data.mean(axis=1)\n",
    "    # Select the columns containing the intensity values\n",
    "\n",
    "    # node_data['average_intensity'] = node_data.apply(lambda row: np.mean(row.nlargest(10)), axis=1)\n",
    "    # node_data.drop(columns=cols,inplace=True)\n",
    "    # node_data = node_data.apply(lambda x: np.log10(x))\n",
    "    # Calculate the average of the 10 most intense columns for each row\n",
    "    node_data = node_data.div(node_data.max())\n",
    "\n",
    "    return node_data\n",
    "\n",
    "carbon_df = make_dataframe(out,fill_value=0)\n",
    "# plant_df = make_dataframe(out_plant,fill_value=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(ncols=2,nrows=1,figsize=(10, 10))\n",
    "idx = carbon_df[carbon_df['origin']=='environment']\n",
    "\n",
    "y = carbon_df[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = carbon_df.T\n",
    "temp = pd.merge(temp,origin,left_index=True,right_on='filename')\n",
    "temp.reset_index(inplace=True,drop=True)\n",
    "temp.drop(columns=['filename'],inplace=True)\n",
    "cols = temp.columns\n",
    "cols = list(set(cols) - set(['origin']))\n",
    "v = temp[cols].values\n",
    "v[v<0.0001] = 0\n",
    "v = v + 0.0001\n",
    "v = np.log10(v)\n",
    "Z = sch.linkage(v, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 10))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the colors for each category\n",
    "color_map = {'environment': 'red', 'plant': 'blue'}\n",
    "\n",
    "# Map the colors to the 'origin' column\n",
    "colors = temp['origin'].map(color_map)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# Perform PCA on v dataframe\n",
    "\n",
    "\n",
    "pca_result = pca.fit_transform(v)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "# Create a scatter plot of the PCA results\n",
    "s = ax.scatter(pca_result[:, 0], pca_result[:, 1],c=colors,alpha=0.7)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_metadata = pd.read_csv('../data/public_and_internal_files_with_massive_and_redu.tsv', sep='\\t')\n",
    "file_metadata = file_metadata[['title','massive_id','h5','description','keywords']]\n",
    "file_metadata.drop_duplicates(inplace=True)\n",
    "file_metadata.reset_index(inplace=True,drop=True)\n",
    "file_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = carbon_df.index.tolist()\n",
    "g = pd.merge(carbon_df.T,file_metadata,left_index=True,right_on='h5',how='inner')[cols + ['massive_id']].groupby('massive_id').apply(lambda x: x.mean())\n",
    "g = g > 0.01\n",
    "g = g.sum(axis=1)\n",
    "g = g.to_frame()\n",
    "g.columns = ['num_nodes']\n",
    "cols = ['title','massive_id','description','keywords']\n",
    "g = pd.merge(g,file_metadata[cols],left_index=True,right_on='massive_id',how='inner').drop_duplicates().sort_values('num_nodes',ascending=False)\n",
    "g.reset_index(inplace=True,drop=True)\n",
    "# g.to_csv('node_count_per_experiment.csv',index=False)\n",
    "g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = carbon_df.index.tolist()\n",
    "file_metadata.fillna('',inplace=True)\n",
    "g = pd.merge(carbon_df.T,file_metadata,left_index=True,right_on='h5',how='inner')[cols + ['massive_id','title']].groupby(['massive_id','title']).apply(lambda x: x.mean())\n",
    "g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = carbon_df.index.tolist()\n",
    "file_metadata.fillna('',inplace=True)\n",
    "g = pd.merge(carbon_df.T,file_metadata,left_index=True,right_on='h5',how='inner')[cols + ['massive_id','title']].groupby(['massive_id','title']).apply(lambda x: x.mean())\n",
    "g[g<0.0001] = 0\n",
    "g = g + 0.001\n",
    "g = np.log10(g)\n",
    "print(g.shape)\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the linkage matrix\n",
    "Z = sch.linkage(g.values, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 14))\n",
    "sch.dendrogram(Z, orientation='left', labels=g.index)\n",
    "\n",
    "# Set plot title and labels\n",
    "# plt.title('Vertical Dendrogram')\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Index')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from adjustText import adjust_text\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Perform PCA on the carbon_df dataframe\n",
    "\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "cols = carbon_df.index.tolist()\n",
    "file_metadata.fillna('',inplace=True)\n",
    "g = pd.merge(carbon_df.T,file_metadata,left_index=True,right_on='h5',how='inner')[cols + ['massive_id','title']].groupby(['massive_id','title']).apply(lambda x: x.mean())\n",
    "\n",
    "\n",
    "# pca_result = pca.fit_transform(g)\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "pca_result = tsne.fit_transform(g)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "# Create a scatter plot of the PCA results\n",
    "ax.scatter(pca_result[:, 0], pca_result[:, 1])\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "\n",
    "# Create a list to hold the text objects\n",
    "texts = []\n",
    "for i, txt in enumerate(g.index.get_level_values(0)):\n",
    "    texts.append(ax.text(pca_result[i, 0], pca_result[i, 1], txt, fontsize=9))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,ax = plt.subplots(figsize=(20,20))\n",
    "# Create a scatter plot of the PCA results\n",
    "ax.scatter(pca_result[:, 0], pca_result[:, 1])\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "\n",
    "# Create a list to hold the text objects\n",
    "texts = []\n",
    "a1 = g.index.get_level_values(0)\n",
    "a2 = g.index.get_level_values(1)\n",
    "for i, txt in enumerate(zip(a1,a2)):\n",
    "    texts.append(ax.text(pca_result[i, 0], pca_result[i, 1], txt, fontsize=9))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the clustermap\n",
    "carbon_df = carbon_df + 0.0001\n",
    "clustermap = sns.clustermap(carbon_df.apply(np.log10))\n",
    "\n",
    "# Display the clustermap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_df.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [3061,2093,7579]\n",
    "nodes = ['%.1f'%n for n in nodes]\n",
    "carbon_df['sum'] = carbon_df.max(axis=1)\n",
    "carbon_df.loc[carbon_df.index.isin(nodes),'sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_count = (carbon_df>0.01).sum(axis=1)\n",
    "node_count = node_count.to_frame()\n",
    "node_count.columns = ['node_count']\n",
    "\n",
    "backup_G = G.copy()\n",
    "nx.set_node_attributes(backup_G, node_count.to_dict('index'))\n",
    "nx.write_graphml(backup_G,'../data/carbon_networkwith-CarbonSample-node-count.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_value = 3000\n",
    "# out['sampletype'] = 'plantmasst'\n",
    "# node_data = pd.pivot_table(out,index=['node_id'],columns=['sampletype'],values='peak_area',aggfunc=np.mean,fill_value=min_value)\n",
    "# node_data = node_data.apply(lambda x: np.log10(x))\n",
    "\n",
    "# # node_data.columns = ['Quant: %s'%c for c in node_data.columns]\n",
    "# # node_data = node_data[cols]\n",
    "# # # Select the columns to normalize\n",
    "# # columns_to_normalize = node_data.columns\n",
    "\n",
    "# # # Normalize the selected columns\n",
    "# # node_data[columns_to_normalize] = normalize(node_data[columns_to_normalize], norm='l2', axis=1)\n",
    "\n",
    "backup_G = G.copy()\n",
    "nx.set_node_attributes(backup_G, node_data.to_dict('index'))\n",
    "nx.write_graphml(backup_G,'../data/carbon_networkwith-plantmasst.graphml')\n",
    "backup_G_mst = nx.maximum_spanning_tree(backup_G)\n",
    "nx.write_graphml(backup_G_mst,'../data/carbon_networkwith-plantmasst-mst.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_index = nx.get_node_attributes(G, 'original_index')\n",
    "np_classifier_pathway = nx.get_node_attributes(G, 'pathway_results')\n",
    "np_classifier_class = nx.get_node_attributes(G, 'class_results')\n",
    "smiles_identity = nx.get_node_attributes(G, 'smiles_identity')\n",
    "# structural_cluster_subclassname  = nx.get_node_attributes(G, 'structural_cluster_subclassname')\n",
    "# structural_cluster_subclassname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_data = pd.pivot_table(out,index=['node_id'],columns=['experiment'],values='peak_area',aggfunc=np.mean,fill_value=300)\n",
    "# node_data = pd.pivot_table(out,index=['node_id'],columns=['sampletype'],values='peak_area',aggfunc=np.mean,fill_value=300)\n",
    "# node_data = pd.pivot_table(out[out['sampletype'].str.contains('Hi')],index=['node_id'],columns=['experiment','sampletype'],values='peak_area',aggfunc=np.mean,fill_value=300)\n",
    "# node_data.columns = ['%s-%s'%(c[1],c[0]) for c in node_data.columns]\n",
    "\n",
    "# node_data = node_data.apply(lambda  x: np.log10(x+1),axis=1)\n",
    "# s = node_data.sum(axis=0).values\n",
    "# m = s.mean()\n",
    "# node_data.values[:,:] = m * (node_data.values[:,:]/s)\n",
    "# node_data.columns = [c.split('/')[-1].split('_')[12] for c in node_data.columns]\n",
    "\n",
    "# node_data.to_csv('../data/log10_averages_treatments.csv')\n",
    "node_data.columns = ['Quant: %s'%c for c in node_data.columns]\n",
    "\n",
    "n = node_data.copy()\n",
    "m = n.min(axis=1)\n",
    "m = n.values - m.values[:,None]\n",
    "n = pd.DataFrame(m,index=n.index,columns=n.columns)\n",
    "\n",
    "m = n.max(axis=1)\n",
    "m = n.values / m.values[:,None]\n",
    "n = pd.DataFrame(m,index=n.index,columns=n.columns)\n",
    "\n",
    "\n",
    "original_index = nx.get_node_attributes(G, 'original_index')\n",
    "np_classifier_pathway = nx.get_node_attributes(G, 'pathway_results')\n",
    "np_classifier_class = nx.get_node_attributes(G, 'class_results')\n",
    "smiles_identity = nx.get_node_attributes(G, 'smiles_identity')\n",
    "# temp = {}\n",
    "# for c in n.columns:\n",
    "    # temp[c] = nx.get_node_attributes(backup_G, c)\n",
    "\n",
    "temp = pd.merge(n,pd.DataFrame({'np_classifier_pathway':np_classifier_pathway}),left_index=True,right_index=True)\n",
    "temp = pd.merge(temp,pd.DataFrame({'np_classifier_class':np_classifier_class}),left_index=True,right_index=True)\n",
    "temp = pd.merge(temp,pd.DataFrame({'smiles_identity':smiles_identity}),left_index=True,right_index=True)\n",
    "cols = [c for c in temp.columns if 'Quant' in c]\n",
    "\n",
    "# cluster_df = temp.groupby(['np_classifier_pathway'])[cols].mean()\n",
    "cluster_df = temp.groupby(['np_classifier_class','np_classifier_pathway'])[cols].mean()\n",
    "identity_df = temp.groupby(['np_classifier_pathway','np_classifier_class','smiles_identity'])[cols].mean()\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.sort_values('np_classifier_pathway').head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "# Create the polar plot\n",
    "\n",
    "def order_similar_rows(cluster_df):\n",
    "    # Calculate the pairwise distances between rows\n",
    "    # cols = [c for c in cluster_df.columns]# if ('deciduousforests' in c) | ('coniferousforests' in c)]\n",
    "    distances = cluster_df.values\n",
    "\n",
    "    # Perform hierarchical clustering\n",
    "    Z = linkage(distances, method='average', metric='euclidean')\n",
    "\n",
    "    # Get the order of the rows based on the clustering\n",
    "    order = dendrogram(Z, no_plot=True)['leaves']\n",
    "\n",
    "    # Reorder the rows of cluster_df\n",
    "    cluster_df = cluster_df.iloc[order]\n",
    "\n",
    "    # Show the reordered cluster_df\n",
    "    return cluster_df#[cols]\n",
    "\n",
    "def make_polar_structural_cluster_plot(cluster_df):\n",
    "    cluster_df = order_similar_rows(cluster_df)\n",
    "    s = cluster_df.shape[0]/20*7 + 5\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(s, s), subplot_kw={'projection': 'polar'})\n",
    "\n",
    "    # Define the angles for each side of the polygons\n",
    "    angles = np.linspace(0, 2 * np.pi, cluster_df.shape[0] + 1)[:-1]\n",
    "    shift_amount = np.diff(angles)[0] / cluster_df.shape[1]\n",
    "    for iii in range(cluster_df.shape[1]):\n",
    "        # Define the lengths of the bars\n",
    "        bar_lengths = cluster_df.values[:, iii]\n",
    "        # Plot the bars\n",
    "        ax.bar(angles + iii * shift_amount, bar_lengths*1, width=shift_amount, align='edge', alpha=0.74, label=cluster_df.columns[iii].replace('Quant: ',''))\n",
    "\n",
    "    # Set the labels for each side of the polygons\n",
    "    ax.set_xticks(angles)\n",
    "    ax.set_xticklabels(['' for i in range(cluster_df.shape[0])])\n",
    "\n",
    "    # Set the title of the plot\n",
    "    tick_labels = ax.xaxis.get_ticklabels()\n",
    "\n",
    "    m = ax.get_ylim()[1]*1.1\n",
    "    ax.set_ylim(0, m)\n",
    "\n",
    "\n",
    "\n",
    "    counter = 0\n",
    "    for i, row in cluster_df.iterrows():\n",
    "        my_angle = angles[counter] + shift_amount* cluster_df.shape[1]/2\n",
    "        # my_angle = angles[counter] + shift_amount*2\n",
    "        # my_angle = my_angle * 180/np.pi - 90\n",
    "        # my_angle = my_angle * 180/np.pi\n",
    "        my_angle = 0\n",
    "        # ax.text(angles[counter] + shift_amount*cluster_df.shape[1]/2, m * 1.11, '%s'%i, ha='center', va='center', fontsize=12, rotation=my_angle)#angles[counter]*180/4/np.pi)\n",
    "        my_text = '%s'%i\n",
    "        my_text = my_text.replace(' ','\\n')\n",
    "        ax.text(angles[counter] + shift_amount*cluster_df.shape[1]/2, m * 0.95, my_text, ha='center', va='center', fontsize=12, rotation=my_angle)#angles[counter]*180/4/np.pi)\n",
    "\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    ax.spines['polar'].set_visible(False)\n",
    "    ax.grid(color='black')\n",
    "    # Hide the y tick labels\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    # Position the legend outside of the plot area\n",
    "    ax.legend(bbox_to_anchor=(0.85, 1.1), loc='upper left',fontsize=16,frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "unique_values = cluster_df.index.get_level_values('np_classifier_pathway').unique()\n",
    "# unique_values = [u for u in unique_values is u!=None]\n",
    "for uv in unique_values:\n",
    "    selected_rows = cluster_df.loc[cluster_df.index.get_level_values('np_classifier_pathway') == uv]\n",
    "    selected_rows.reset_index(drop=True, level=\"np_classifier_pathway\", inplace=True)\n",
    "    make_polar_structural_cluster_plot(selected_rows)\n",
    "    # plt.savefig('../data/structural_clusters_%s.png'%uv)\n",
    "    # plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of sides of the polygons\n",
    "num_sides = bar_df.shape[0]\n",
    "\n",
    "# Define the angles for each side of the polygons\n",
    "angles = np.linspace(0, 2 * np.pi, num_sides + 1)[:-1]\n",
    "\n",
    "\n",
    "\n",
    "# Create the polar plot\n",
    "fig, ax = plt.subplots(nrows=2,ncols=2,figsize=(12,12),subplot_kw={'projection': 'polar'})\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(len(ax)):\n",
    "    # Define the lengths of the bars\n",
    "    bar_lengths = bar_df.values[:,i]\n",
    "    # Plot the bars\n",
    "    ax[i].bar(angles, bar_lengths, width=0.1, align='edge', color='blue', alpha=0.5)\n",
    "\n",
    "\n",
    "    # Set the labels for each side of the polygons\n",
    "    ax[i].set_xticks(angles)\n",
    "    ax[i].set_xticklabels(['Side {}'.format(i+1) for i in range(num_sides)])\n",
    "\n",
    "    # Set the title of the plot\n",
    "    ax[i].set_title(bar_df.columns[i])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "categories = ['Category 1', 'Category 2', 'Category 3', 'Category 4', 'Category 5']\n",
    "values = [10, 15, 7, 12, 9]\n",
    "\n",
    "# Convert values to radians\n",
    "theta = np.linspace(0.0, 2 * np.pi, len(categories), endpoint=False)\n",
    "\n",
    "# Create polar plot\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "bars = ax.bar(theta, values)\n",
    "\n",
    "# Set the color of each bar\n",
    "for bar in bars:\n",
    "    bar.set_facecolor(np.random.rand(3))\n",
    "\n",
    "# Set the labels for each category\n",
    "ax.set_xticks(theta)\n",
    "ax.set_xticklabels(categories)\n",
    "\n",
    "# Set the title of the plot\n",
    "ax.set_title('Circular Barchart')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
