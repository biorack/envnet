{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import os\n",
    "\n",
    "import multiprocessing\n",
    "import re\n",
    "from rdkit.Chem import rdchem\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "# sys.path.insert(0,'/global/homes/b/bpb/repos/carbon_network')\n",
    "import build_tools as wt\n",
    "\n",
    "\n",
    "# sys.path.insert(0,'/global/homes/b/bpb/repos/metatlas')\n",
    "from metatlas.io import feature_tools as ft\n",
    "\n",
    "# sys.path.insert(0,'/global/homes/b/bpb/repos/carbon_network/blink')\n",
    "\n",
    "import blink\n",
    "\n",
    "\n",
    "# Set the display width for pandas columns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# show more pandas columns\n",
    "pd.set_option('display.max_columns', 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_from_disk(directory,extension):\n",
    "    \"\"\"\n",
    "    Get on disk with date\n",
    "    \"\"\"\n",
    "    get_with_date = ''.join(['find %s -iname \"*%s\"' % (directory,extension),' -printf \"%Ts SplitThat%p\\n\"'])\n",
    "    files = check_output(get_with_date, shell=True)\n",
    "    files = files.decode('utf-8').splitlines()\n",
    "    files = [f.split('SplitThat') for f in files]\n",
    "    dates = [int(f[0].strip()) for f in files]\n",
    "    files = [f[1].strip() for f in files]\n",
    "    return dates,files\n",
    "\n",
    "half_precursor_tolerance = 0.001 # since we typically use 0.002 as the precursor tolerance, we can use 0.001 as the half tolerance\n",
    "\n",
    "data_dir = '/global/cfs/cdirs/metatlas/projects/carbon_network/raw_data'\n",
    "metadata_folder = '/global/cfs/cdirs/metatlas/projects/carbon_network'\n",
    "environmental_metatlas_folder = os.path.join(data_dir,'metatlas')\n",
    "metatlas_no_buddy_folder = os.path.join(data_dir,'metatlas_no_buddy')\n",
    "\n",
    "\n",
    "# Get metatlas-no-buddy  files\n",
    "dates, parquet_files = get_files_from_disk(metatlas_no_buddy_folder, 'parquet')\n",
    "dates, h5_files = get_files_from_disk(metatlas_no_buddy_folder, 'h5')\n",
    "\n",
    "df = {}\n",
    "for f in parquet_files:\n",
    "    df[f.replace('.parquet','')] = {}\n",
    "for f in h5_files:\n",
    "    df[f.replace('.h5','')] = {}\n",
    "\n",
    "for f in parquet_files:\n",
    "    df[f.replace('.parquet','')]['parquet'] = f\n",
    "for f in h5_files:\n",
    "    df[f.replace('.h5','')]['h5'] = f\n",
    "\n",
    "# Get metatlas environmental files\n",
    "dates, parquet_files = get_files_from_disk(environmental_metatlas_folder, 'parquet')\n",
    "dates, h5_files = get_files_from_disk(environmental_metatlas_folder, 'h5')\n",
    "\n",
    "for f in parquet_files:\n",
    "    df[f.replace('.parquet','')] = {}\n",
    "for f in h5_files:\n",
    "    df[f.replace('.h5','')] = {}\n",
    "\n",
    "for f in parquet_files:\n",
    "    df[f.replace('.parquet','')]['parquet'] = f\n",
    "for f in h5_files:\n",
    "    df[f.replace('.h5','')]['h5'] = f\n",
    "\n",
    "# get massive files\n",
    "dates, parquet_files = get_files_from_disk(os.path.join(data_dir,'massive'), 'parquet')\n",
    "dates, h5_files = get_files_from_disk(os.path.join(data_dir,'massive'), 'h5')\n",
    "for f in parquet_files:\n",
    "    df[f.replace('.parquet','')] = {}\n",
    "for f in h5_files:\n",
    "    df[f.replace('.h5','')] = {}\n",
    "\n",
    "for f in parquet_files:\n",
    "    df[f.replace('.parquet','')]['parquet'] = f\n",
    "for f in h5_files:\n",
    "    df[f.replace('.h5','')]['h5'] = f\n",
    "\n",
    "\n",
    "for k,v in df.items():\n",
    "    if environmental_metatlas_folder in k:\n",
    "        df[k]['experiment_id'] = '_'.join(os.path.basename(k).split('_')[4:6])\n",
    "\n",
    "for k,v in df.items():\n",
    "    if metatlas_no_buddy_folder in k:\n",
    "        df[k]['experiment_id'] = '_'.join(os.path.basename(k).split('_')[4:6])    \n",
    "\n",
    "df = pd.DataFrame(df).T\n",
    "\n",
    "# extract the massive ID from the path\n",
    "pattern = r'(?<=/)(MSV.*?)(?=/)'\n",
    "a = df.copy().index.str.extract(pattern)\n",
    "df['massive_id'] = a[0].tolist()\n",
    "\n",
    "# extract everything from the massive ID to the filename\n",
    "pattern = r'(\\/MSV.*)' \n",
    "df['redu_filename'] = df.copy().index.str.extract(pattern,expand=False)\n",
    "df['redu_filename'] = df['redu_filename'].str.replace(r'^/', 'f.', regex=True)\n",
    "\n",
    "# ADD IN EXPERIMENT_ID TO DEAL WITH BOTH MASSIVE AND LBL DATASETS\n",
    "idx = pd.notna(df['massive_id'])\n",
    "df.loc[idx,'experiment_id'] = df.loc[idx,'massive_id']\n",
    "\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "df_grouped = df.groupby('experiment_id').size().reset_index(name='files_per_experiment')\n",
    "df = pd.merge(df, df_grouped, on='experiment_id', how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metatlas.untargeted.tools import get_google_sheet\n",
    "file_df = get_google_sheet(notebook_name='Supplementary Tables',sheet_name='environmental')\n",
    "print(df.shape)\n",
    "df = df[df['parquet'].isin(file_df['parquet'])]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge massive metadata with all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = ['title','dataset','description','keywords','instrument']\n",
    "massive_metadata = pd.read_csv(os.path.join(metadata_folder,'massive_metadata_2024.tsv'), sep='\\t',usecols=usecols)\n",
    "massive_metadata.columns = [c.lower() for c in massive_metadata.columns]\n",
    "df = pd.merge(df,massive_metadata.add_suffix('_massive'),left_on='massive_id',right_on='dataset_massive',how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge REDU Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redu = pd.read_csv(os.path.join(metadata_folder,'all_sampleinformation.tsv'), sep='\\t')\n",
    "redu['filename'] = redu['filename'].str.replace(r'\\.mz(ML|XML)$', '', regex=True)\n",
    "redu.columns = [c.lower() for c in redu.columns]\n",
    "df = pd.merge(df,redu.add_suffix('_redu'),left_on='redu_filename',right_on='filename_redu',how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGE in Environmental Experiment List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environmental_metatlas_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_samples = pd.read_csv(os.path.join(metadata_folder,'dom_public_datasets.csv'),usecols=['dataset'])\n",
    "dom_samples = pd.Series(dom_samples['dataset'].unique())\n",
    "df['environmental'] = (df['massive_id'].isin(dom_samples)) | (df['parquet'].str.contains(environmental_metatlas_folder+'/'))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Plant files for manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = df['sampletype_redu'].str.contains('plant',case=False,na=False)\n",
    "idx2 = df['parquet'].str.contains('20240409_EB_NB_107915-001_PRISM-RtExu_combined-rep1-5_EXP120A_C18-EP_USDAY72349')\n",
    "df['plant'] = (idx1) | (idx2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[pd.notna(df['parquet'])]\n",
    "df = df[~df['parquet'].str.contains('qc',case=False)]\n",
    "df = df[~df['parquet'].str.contains('blank',case=False)]\n",
    "df = df[~df['parquet'].str.contains('exctrl',case=False)]\n",
    "df[df['plant']].to_csv(os.path.join(metadata_folder,'plant_samples.csv'),index=False)\n",
    "df[df['plant']].to_csv('plant_samples.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter to either plant or environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['environmental']]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove rows that do not have a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[pd.notna(df['parquet'])]\n",
    "df = df[~df['parquet'].str.contains('qc',case=False)]\n",
    "df = df[~df['parquet'].str.contains('blank',case=False)]\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove duplicate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def hash_dataframe_row(filename):\n",
    "    cols = ['precursor_mz', 'rt', 'coisolated_precursor_count', 'predicted_formula', 'estimated_fdr']\n",
    "    try:\n",
    "        t = pd.read_parquet(filename)\n",
    "        hash_value = int(hashlib.sha256(pd.util.hash_pandas_object(t[cols], index=True).values).hexdigest(), 16)\n",
    "        return hash_value\n",
    "    except:\n",
    "        print('Error',filename)\n",
    "        return None\n",
    "\n",
    "def parallel_hash_dataframe(files):\n",
    "    with multiprocessing.Pool(20) as pool:\n",
    "        results = pool.map(hash_dataframe_row, files)\n",
    "    return results\n",
    "\n",
    "hash_values = parallel_hash_dataframe(df['parquet'].tolist())\n",
    "df['hash_value'] = hash_values\n",
    "df.sort_values(['environmental','files_per_experiment','sampletype_redu'],ascending=False,inplace=True) # True is greater than False\n",
    "df.drop_duplicates(subset=['hash_value'],keep='first',inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use natural language processing to parse the column description_massive and categorize each row in df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all MDM spectra and formula from the parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_row(parquet_file):\n",
    "    temp = pd.read_parquet(parquet_file)\n",
    "    if temp.shape[0]>0:\n",
    "        temp['parquet'] = parquet_file\n",
    "        return temp\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "files = df[pd.notna(df['parquet'])]['parquet'].tolist()\n",
    "with multiprocessing.Pool(20) as pool:\n",
    "    out = pool.map(process_row, files)\n",
    "\n",
    "all_mdm_df = [o for o in out if o is not None]\n",
    "all_mdm_df = pd.concat(all_mdm_df)\n",
    "all_mdm_df.reset_index(inplace=True,drop=True)\n",
    "all_mdm_df = all_mdm_df[pd.notna(all_mdm_df['mdm_mz_vals'])] # 1132622, 833718\n",
    "all_mdm_df.reset_index(inplace=True,drop=True)\n",
    "all_mdm_df.index.name = 'original_index' # you need this later for looking at ms1 evidence\n",
    "all_mdm_df.reset_index(inplace=True,drop=False)\n",
    "all_mdm_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the fraction of formulae observed in FTICR experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fraction_in_fticr(df,group_term='parquet'):\n",
    "    df = df[pd.notna(df['predicted_formula'])]\n",
    "    result_true = df[df['isin_fticr_formula'] == True].groupby(group_term)['predicted_formula'].nunique()\n",
    "    result_false = df[df['isin_fticr_formula'] == False].groupby(group_term)['predicted_formula'].nunique()\n",
    "    combined_result = pd.concat([result_true, result_false], axis=1)\n",
    "    combined_result.columns = ['formula_count_in_fticr', 'formula_count_not_in_fticr']\n",
    "\n",
    "    combined_result.fillna(0, inplace=True)\n",
    "    combined_result['fraction_formula_in_fticr'] = combined_result['formula_count_in_fticr'] / (combined_result['formula_count_in_fticr'] + combined_result['formula_count_not_in_fticr'])\n",
    "    combined_result.reset_index(inplace=True,drop=False)\n",
    "    \n",
    "    return combined_result\n",
    "\n",
    "formula_df = pd.read_csv('/global/homes/b/bpb/repos/scndb/data/merged_fticr_formula (2).csv')\n",
    "formula_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "# Add new columns for each unique value in \"environment type\"\n",
    "# cols = ['formula','environment_type']\n",
    "# formula_df = pd.pivot_table(formula_df[cols], index='formula', columns='environment_type', aggfunc=lambda x: True, fill_value=False)\n",
    "cols =  ['environment_type', 'extraction_solvent',\n",
    "       'ppl_extracted', 'instrument_type',\n",
    "       'mass_range', 'doi']\n",
    "for c in cols:\n",
    "    env = formula_df.groupby(['formula',c])['polarity'].count().unstack().fillna(0)\n",
    "    env[env>0] = 1\n",
    "    env.columns = ['%s:%s'%(c,x) for x in env.columns]\n",
    "    env = env.astype(int)\n",
    "    formula_df = formula_df.merge(env, left_on='formula', right_index=True)\n",
    "    formula_df.drop(columns=c, inplace=True)\n",
    "    print(c)\n",
    "formula_df.drop_duplicates(subset='formula', inplace=True)\n",
    "formula_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "formula_df.drop(columns=['mz','ionization_method','polarity'],inplace=True)\n",
    "all_mdm_df['isin_fticr_formula'] = all_mdm_df['predicted_formula'].isin(formula_df['formula'])\n",
    "\n",
    "fraction_in_fticr = calculate_fraction_in_fticr(all_mdm_df)\n",
    "\n",
    "df = pd.merge(df,fraction_in_fticr,on='parquet',how='left')\n",
    "v = all_mdm_df.groupby('parquet')['isin_fticr_formula'].mean().sort_values(ascending=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(v,bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_mdm_df.shape,df.shape)\n",
    "all_mdm_df = all_mdm_df[all_mdm_df['parquet'].isin(v[v>0.5].index.tolist())]\n",
    "df = df[df['parquet'].isin(all_mdm_df['parquet'].unique())]\n",
    "print(all_mdm_df.shape,df.shape)\n",
    "cols = ['formula_count_in_fticr','formula_count_not_in_fticr']\n",
    "files_no_formula = df.loc[df[cols].sum(axis=1)<10,'parquet'].tolist()\n",
    "df = df[~df['parquet'].isin(files_no_formula)]\n",
    "all_mdm_df = all_mdm_df[all_mdm_df['parquet'].isin(df['parquet'].unique())]\n",
    "print(all_mdm_df.shape,df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get hits to MDM and original spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mz_tol = 0.002\n",
    "deltas = pd.read_csv(os.path.join(metadata_folder,'mdm_neutral_losses.csv'))\n",
    "ref,ref2 = wt.get_p2d2(deltas,mz_tol=mz_tol)\n",
    "print(ref.shape)\n",
    "ref.reset_index(inplace=True,drop=True)\n",
    "ref2.reset_index(inplace=True,drop=True)\n",
    "print(ref.shape)\n",
    "ref_spec = ref['spectrum'].tolist()\n",
    "ref_pmz = ref['precursor_mz'].tolist()\n",
    "\n",
    "ref_spec_nl = ref2['nl_spectrum'].tolist()\n",
    "ref_pmz_nl = ref2['precursor_mz'].tolist()\n",
    "\n",
    "def score_df(df):\n",
    "    min_matches=3,\n",
    "    min_score=0.7,\n",
    "    override_matches=20\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    q_cols = ['predicted_formula','precursor_mz']\n",
    "    r_cols = ['original_p2d2_index', 'formula','precursor_mz', 'inchi_key','name']\n",
    "\n",
    "    if df.shape[0]==0:\n",
    "        return None\n",
    "    if 'mdm_mz_vals' not in df.columns:\n",
    "        return None\n",
    "    df['num_mdm_frags'] = df['mdm_mz_vals'].apply(lambda x: len(x) if type(x)!=float else 0)\n",
    "\n",
    "    df = df[df['num_mdm_frags']>0]\n",
    "\n",
    "\n",
    "    df['mdm_spectrum'] = df.apply(lambda x: np.asarray([x['mdm_mz_vals'],x['mdm_i_vals']]),axis=1)\n",
    "    df['original_spectrum'] = df.apply(lambda x: np.asarray([x['original_mz_vals'],x['original_i_vals']]),axis=1)\n",
    "\n",
    "    query_spec = df['original_spectrum'].tolist()\n",
    "    query_pmz = df['precursor_mz'].tolist()\n",
    "    query_spec_nl = df['mdm_spectrum'].tolist()\n",
    "    query_pmz_nl = df['precursor_mz'].tolist()\n",
    "\n",
    "    d_specs = blink.discretize_spectra(query_spec,  ref_spec, query_pmz, ref_pmz, intensity_power=0.5, bin_width=0.001, tolerance=0.01,network_score=False)#,mass_diffs=mass_diffs)\n",
    "    d_specs_nl = blink.discretize_spectra(query_spec_nl,  ref_spec_nl, query_pmz_nl, ref_pmz_nl, intensity_power=0.5, bin_width=0.001, tolerance=0.01,network_score=False)#,mass_diffs=mass_diffs)\n",
    "    \n",
    "    def score_and_filter(specs,r,q,mz_tol=0.002,min_score=0.7,min_matches=3,override_matches=20,\n",
    "                         q_cols=['predicted_formula','precursor_mz'],\n",
    "                         r_cols=['original_p2d2_index', 'formula','precursor_mz', 'inchi_key','name']):\n",
    "        scores = blink.score_sparse_spectra(specs)\n",
    "        filtered_scores = blink.filter_hits(scores,min_score=min_score,min_matches=min_matches,override_matches=override_matches,)\n",
    "        mz_df = blink.reformat_score_matrix(filtered_scores)\n",
    "        mz_df = blink.make_output_df(mz_df)\n",
    "        for c in mz_df.columns:\n",
    "            mz_df[c] = mz_df[c].sparse.to_dense()\n",
    "\n",
    "        mz_df = pd.merge(mz_df,q[q_cols],left_on='query',right_index=True)\n",
    "        mz_df = pd.merge(mz_df,r[r_cols].add_suffix('_ref'),left_on='ref',right_index=True)\n",
    "        # mz_df = mz_df[mz_df['predicted_formula']==mz_df['formula']]\n",
    "        mz_df = mz_df[abs(mz_df['precursor_mz']-mz_df['precursor_mz_ref'])<mz_tol]\n",
    "        # mz_df.sort_values('score',ascending=False,inplace=True)\n",
    "        # mz_df.drop_duplicates('inchi_key_ref',keep='first',inplace=True)\n",
    "        return mz_df\n",
    "    orig_hits = score_and_filter(d_specs,ref,df,mz_tol=mz_tol,min_score=0.7,min_matches=3,override_matches=20,\n",
    "                             q_cols=q_cols,\n",
    "                             r_cols=['original_p2d2_index', 'formula','precursor_mz', 'inchi_key','name'])\n",
    "    nl_hits = score_and_filter(d_specs_nl,ref2,df,mz_tol=mz_tol,min_score=0.7,min_matches=3,override_matches=20,\n",
    "                             q_cols=q_cols,\n",
    "                             r_cols=['original_p2d2_index', 'formula','precursor_mz', 'inchi_key','name'])\n",
    "    temp = pd.merge(nl_hits,orig_hits,on=['query','original_p2d2_index_ref'],how='outer',suffixes=('_original','_mdm'))\n",
    "    if temp.shape[0]==0:\n",
    "        return None\n",
    "    temp['max_score'] = temp[['score_original','score_mdm']].max(axis=1)\n",
    "    temp['best_match_method'] = temp[['score_original','score_mdm']].idxmax(axis=1)\n",
    "    temp.sort_values('max_score',ascending=False,inplace=True)\n",
    "    temp = temp[temp['max_score']>min_score]  #filter on score\n",
    "    temp['max_matches'] = 0\n",
    "    idx = temp['best_match_method']=='score_original'\n",
    "    temp.loc[idx,'max_matches'] = temp.loc[idx,'matches_original']\n",
    "    idx = temp['best_match_method']=='score_mdm'\n",
    "    temp.loc[idx,'max_matches'] = temp.loc[idx,'matches_mdm']\n",
    "    temp = temp[temp['max_matches']>=min_matches]  #filter on matches\n",
    "\n",
    "\n",
    "    cols = ['score_original','matches_original', 'score_mdm', 'matches_mdm']\n",
    "    temp.drop(columns=cols,inplace=True)\n",
    "    temp = pd.merge(temp,df[['original_index','precursor_mz','coisolated_precursor_count']].add_suffix('_query'),left_on='query',right_index=True)\n",
    "\n",
    "    idx_isolated = (temp['coisolated_precursor_count_query']>1) & (temp['best_match_method']=='score_original')\n",
    "    temp = temp[~idx_isolated] #filter on isolated precursor\n",
    "\n",
    "    cols = ['name', 'inchi_key', 'smiles','formula','precursor_mz','original_p2d2_index']\n",
    "    temp = pd.merge(temp,ref[cols],left_on='original_p2d2_index_ref',right_on='original_p2d2_index',how='left')\n",
    "\n",
    "    idx_precursor = (abs(temp['precursor_mz_query']-temp['precursor_mz'])<mz_tol)\n",
    "    temp = temp[idx_precursor] #filter on precursor m/z\n",
    "    \n",
    "    # temp.sort_values('max_score',ascending=False,inplace=True)\n",
    "    # temp = temp.groupby('query').head(1)\n",
    "    # temp.reset_index(inplace=True,drop=True)\n",
    "    return temp\n",
    "\n",
    "\n",
    "# split all_mdm_df into chunks that are roughly 1000 large\n",
    "chunks = np.array_split(all_mdm_df, np.ceil(len(all_mdm_df) / 1000))\n",
    "\n",
    "def process_row(chunk):\n",
    "    temp = score_df(chunk)\n",
    "    return temp\n",
    "    \n",
    "\n",
    "with multiprocessing.Pool(10) as pool:\n",
    "    out = pool.map(process_row, chunks)\n",
    "\n",
    "out = pd.concat(out)\n",
    "out.to_parquet(os.path.join(metadata_folder,'all_environmental_identity_matches.parquet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_mass(formula):\n",
    "    # Regular expression to match elements and their counts\n",
    "    pattern = r'([A-Z][a-z]*)(\\d*)'    \n",
    "    mass = 0\n",
    "    pt = rdchem.GetPeriodicTable()\n",
    "\n",
    "    for el, count in re.findall(pattern, formula):\n",
    "        # If count is an empty string, it means there's only one atom of this element\n",
    "        count = int(count) if count else 1\n",
    "        mass += pt.GetMostCommonIsotopeMass(el) * count\n",
    "    return mass\n",
    "\n",
    "unique_formulas = all_mdm_df['predicted_formula'].unique()\n",
    "masses = {f:calculate_mass(f) for f in unique_formulas}\n",
    "all_mdm_df['predicted_mass'] = all_mdm_df['predicted_formula'].map(masses)\n",
    "all_mdm_df['predicted_mass'] = all_mdm_df['predicted_mass'] - 1.007276\n",
    "all_mdm_df['mass_error'] = abs(all_mdm_df['precursor_mz'] - all_mdm_df['predicted_mass'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# there better be nothing to remove here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mdm_df.groupby('parquet')['mass_error'].median().sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge hdf5 filename with the parquet filenames in all_mdm_df\n",
    "\n",
    "# copy any missing hdf5 files to where they are supposed to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['filename','parquet']\n",
    "files_df = all_mdm_df[cols].copy()\n",
    "files_df.drop_duplicates(inplace=True)\n",
    "files_df.reset_index(drop=True,inplace=True)\n",
    "files_df['h5_missing'] = [not os.path.exists(f.replace('.parquet','.h5')) for f in files_df['parquet']]\n",
    "files_to_copy = files_df[files_df['h5_missing']].copy()\n",
    "files_to_copy['original_h5_missing'] = [not os.path.exists(f) for f in files_to_copy['filename']]\n",
    "print(sum(files_to_copy['original_h5_missing']))\n",
    "for i,row in files_to_copy.iterrows():\n",
    "    parquet_file = row['parquet']\n",
    "    h5_file = parquet_file.replace('.parquet','.h5')\n",
    "    source_file = row['filename']\n",
    "    dest_file = h5_file\n",
    "    print('Copying %s to %s'%(source_file,dest_file))\n",
    "    os.system('cp \"%s\" \"%s\"'%(source_file,dest_file))\n",
    "\n",
    "all_mdm_df['h5'] = [f.replace('.parquet','.h5') for f in all_mdm_df['parquet']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the MS1 evidence for the predicted precursor in the MDM spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER OUT ANY SPECTRA THAT DO NOT HAVE MS1 OR NOT CALIBRATED WELL\n",
    "\n",
    "\n",
    "def calculate_ms1_summary(row):\n",
    "    \"\"\"\n",
    "    Calculate summary properties for features from data\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    #Before doing this make sure \"in_feature\"==True has already occured\n",
    "    d['num_datapoints'] = row['i'].count()\n",
    "    if d['num_datapoints'] == 0:\n",
    "        return pd.Series(d)\n",
    "    d['peak_area'] = row['i'].sum()\n",
    "    idx = row['i'].idxmax()\n",
    "    d['peak_height'] = row.loc[idx,'i']\n",
    "    d['mz_centroid'] = sum(row['i']*row['mz'])/d['peak_area']\n",
    "    d['rt_peak'] = row.loc[idx,'rt']\n",
    "    return pd.Series(d)\n",
    "\n",
    "def make_atlas(df,ppm_tolerance = 5,mz_tol = 0.002,extra_rt=1):\n",
    "    atlas = df.copy()\n",
    "    atlas.rename(columns={'original_index':'label','rt':'rt_peak','precursor_mz':'mz'},inplace=True)\n",
    "    atlas['rt_min'] = atlas['rt_peak'] - extra_rt\n",
    "    atlas['rt_max'] = atlas['rt_peak'] + extra_rt\n",
    "    atlas['mz_tolerance'] = mz_tol\n",
    "    atlas['ppm_tolerance'] = ppm_tolerance\n",
    "    atlas['extra_time'] = 0\n",
    "    atlas['group_index'] = ft.group_consecutive(atlas['mz'].values[:],\n",
    "                                        stepsize=ppm_tolerance,\n",
    "                                        do_ppm=True)\n",
    "    return atlas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_row(gg):\n",
    "    filename = gg[0]\n",
    "    atlas = make_atlas(gg[1])\n",
    "    # try:\n",
    "    d = ft.get_atlas_data_from_file(filename,atlas,desired_key='ms1_neg')\n",
    "    # d = d[d['in_feature']==True]\n",
    "    # except:\n",
    "    #     print('Can not read',filename)\n",
    "    d = d.groupby('label',group_keys=True).apply(calculate_ms1_summary)\n",
    "    return d\n",
    "\n",
    "\n",
    "cols = ['original_index','precursor_mz',  'rt']\n",
    "\n",
    "g = [(_,gg[cols]) for _,gg in all_mdm_df[pd.notna(all_mdm_df['h5'])].groupby('h5')]    \n",
    "with multiprocessing.Pool(20) as pool:\n",
    "    out = pool.map(process_row,g)    \n",
    "out = pd.concat(out)\n",
    "\n",
    "cols = ['num_datapoints','peak_area','peak_height','mz_centroid','rt_peak']\n",
    "out = out[cols]\n",
    "\n",
    "\n",
    "# temp = pd.merge(out,all_df[cols],left_on='label',right_on='original_index',how='inner')\n",
    "# temp['ppm_error'] = abs(temp['mz_centroid']-temp['precursor_mz']) / temp['precursor_mz'] * 1e6\n",
    "# temp = temp[temp['ppm_error']<5]\n",
    "# temp = temp[temp['num_datapoints']>5]\n",
    "# # fig,ax = plt.subplots()\n",
    "# # temp['num_datapoints'].apply(np.log10).hist(bins=100,ax=ax)\n",
    "\n",
    "\n",
    "# all_df = all_df[all_df.index.isin(temp['label'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.reset_index(inplace=True,drop=False)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.merge(out.add_suffix('_ms1'),all_mdm_df,left_on='label_ms1',right_on='original_index',how='right')\n",
    "print(temp.shape[0])\n",
    "temp = temp[temp['num_datapoints_ms1']>2]\n",
    "temp = pd.merge(temp,df,left_on='parquet',right_on='parquet',how='left')\n",
    "if 'hash_value' in temp.columns:\n",
    "    temp.drop(columns=['hash_value'],inplace=True)\n",
    "print(temp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['formula_count_in_fticr','h5_x', 'h5_y','filename','redu_filename',\n",
    "       'files_per_experiment', 'ms2_mz_vals',\n",
    "       'ms2_i_vals','label_ms1','formula_count_in_fticr',\n",
    "       'formula_count_not_in_fticr', 'fraction_formula_in_fticr']\n",
    "cols = [c for c in cols if c in temp.columns]\n",
    "temp.drop(columns=cols,inplace=True)\n",
    "temp.to_parquet(os.path.join(metadata_folder,'all_environmental_spectra.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mz_tol = 0.002\n",
    "min_intensity_ratio = 2\n",
    "similarity_cutoff = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export table of files used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spectra = pd.read_parquet(os.path.join(metadata_folder,'all_environmental_spectra.parquet'))\n",
    "cols = ['parquet', \n",
    "       'experiment_id', 'massive_id',\n",
    "       'title_massive', 'dataset_massive', 'description_massive',\n",
    "       'instrument_massive', 'keywords_massive', 'filename_redu',\n",
    "       'attribute_datasetaccession_redu', 'sampletype_redu',\n",
    "       'sampletypesub1_redu', 'ncbitaxonomy_redu', 'ncbidivision_redu',\n",
    "       'ncbirank_redu', 'yearofanalysis_redu', 'uberonbodypartname_redu',\n",
    "       'biologicalsex_redu', 'ageinyears_redu', 'lifestage_redu',\n",
    "       'country_redu', 'healthstatus_redu', 'chromatographyandphase_redu',\n",
    "       'ionizationsourceandpolarity_redu', 'massspectrometer_redu',\n",
    "       'sampleextractionmethod_redu', 'samplecollectionmethod_redu',\n",
    "       'comorbiditylistdoidindex_redu', 'doidcommonname_redu',\n",
    "       'doidontologyindex_redu', 'envoenvironmentbiome_redu',\n",
    "       'depthoraltitudemeters_redu', 'humanpopulationdensity_redu',\n",
    "       'internalstandardsused_redu', 'latitudeandlongitude_redu',\n",
    "       'samplecollectiondateandtime_redu', 'envoenvironmentmaterial_redu',\n",
    "       'envoenvironmentbiomeindex_redu', 'envoenvironmentmaterialindex_redu',\n",
    "       'subjectidentifierasrecorded_redu', 'termsofposition_redu',\n",
    "       'uberonontologyindex_redu', 'uniquesubjectid_redu', 'usi_redu',\n",
    "       'datasource_redu']\n",
    "all_spectra = all_spectra[cols]\n",
    "all_spectra.drop_duplicates(subset='parquet',inplace=True)\n",
    "all_spectra.reset_index(drop=True,inplace=True)\n",
    "all_spectra['h5'] = [f.replace('.parquet','.h5') for f in all_spectra['parquet']]\n",
    "\n",
    "all_spectra.to_csv(os.path.join(metadata_folder,'all_files_for_environetwork.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spectra = pd.read_parquet(os.path.join(metadata_folder,'all_environmental_spectra.parquet'))\n",
    "\n",
    "all_hits = pd.read_parquet(os.path.join(metadata_folder,'all_environmental_identity_matches.parquet'))\n",
    "\n",
    "all_hits.sort_values('max_score',ascending=False,inplace=True)\n",
    "all_hits.drop_duplicates('original_index_query',keep='first',inplace=True)\n",
    "all_spectra = pd.merge(all_spectra,all_hits[['inchi_key','max_score','original_index_query']].add_suffix('_identity'),left_on='original_index',right_on='original_index_query_identity',how='left')\n",
    "all_spectra['max_score_identity'] = all_spectra['max_score_identity'].fillna(0)\n",
    "all_spectra['inchi_key_identity'] = all_spectra['inchi_key_identity'].fillna('')\n",
    "# chunk df into smaller dataframes using natural breaks in the precursor m/z greater than 0.01\n",
    "all_spectra.sort_values('precursor_mz',inplace=True)\n",
    "all_spectra['precursor_mz_group'] = ft.group_consecutive(all_spectra['precursor_mz'].values[:],stepsize=mz_tol,do_ppm=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spectra['num_mdm_frags'] = all_spectra['mdm_mz_vals'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spectra['num_mdm_frags'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A  better way might be to start with the unique compound nodes and then add to them until you don't find any more unique!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def eliminate_duplicate_spectra(gg):\n",
    "    ms2_df = gg[1]\n",
    "    ms2_df['sum_frag_intensity'] = ms2_df['mdm_i_vals'].apply(lambda x: x.sum())\n",
    "    ms2_df['max_score_identity'] = ms2_df['max_score_identity'].fillna(0)\n",
    "    ms2_df['number_mdm_frags'] = ms2_df['mdm_mz_vals'].apply(lambda x: len(x))\n",
    "    ms2_df.sort_values(['max_score_identity','sum_frag_intensity'],ascending=False,inplace=True) # \n",
    "    ms2_df.reset_index(inplace=True,drop=True)\n",
    "    spec = ms2_df.apply(lambda row: np.array([row['mdm_mz_vals'],row['mdm_i_vals']]),axis=1).tolist()\n",
    "    precursors = ms2_df['precursor_mz'].tolist()\n",
    "    d_specs = blink.discretize_spectra(spec,  spec, precursors, precursors, intensity_power=0.5, bin_width=0.001, tolerance=0.01,network_score=False)#,mass_diffs=mass_diffs)\n",
    "    scores = blink.score_sparse_spectra(d_specs)\n",
    "    similarity_matrix = scores['mzi'].todense()\n",
    "    # idx = np.triu_indices(similarity_matrix.shape[0],k=0)\n",
    "    # similarity_matrix[idx] = 1000\n",
    "    idx_ms2similarity = similarity_matrix>similarity_cutoff\n",
    "    \n",
    "\n",
    "    pmz_diff = abs(np.subtract.outer(ms2_df['precursor_mz'].values,ms2_df['precursor_mz'].values))\n",
    "    # idx = np.triu_indices(pmz_diff.shape[0],k=0)\n",
    "    # pmz_diff[idx] = 1000\n",
    "    idx_pmz_same = pmz_diff<mz_tol\n",
    "    \n",
    "    conditions = (idx_pmz_same) & (idx_ms2similarity)\n",
    "    r,c = np.argwhere(conditions).T\n",
    "    idx = r<c\n",
    "    r = r[idx]\n",
    "    c = c[idx]\n",
    "    if sum(idx)==0:\n",
    "        return ms2_df\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from([(r[i],c[i]) for i in range(len(r))])\n",
    "    # draw the graph colored by subgraphs layout using spring layout\n",
    "    # df = pd.DataFrame({'source':source,'target':target})\n",
    "    # add the subgraph number to the dataframe\n",
    "    sub_graph_indices=list(nx.connected_components(G))\n",
    "    sub_graph_indices = [(i, v) for i,d in enumerate(sub_graph_indices) for k, v in enumerate(d)]\n",
    "    sub_graph_indices = pd.DataFrame(sub_graph_indices,columns=['spectrum_collection','id'])\n",
    "    ms2_df = pd.merge(ms2_df,sub_graph_indices,left_index=True,right_on='id',how='inner')\n",
    "    ms2_df.sort_values(['max_score_identity','number_mdm_frags','sum_frag_intensity'],ascending=False,inplace=True)\n",
    "\n",
    "    # ms2_df.drop_duplicates(['spectrum_collection','inchi_key_identity'],keep='first',inplace=True)\n",
    "    ms2_df.drop_duplicates(['spectrum_collection'],keep='first',inplace=True)\n",
    "\n",
    "    return ms2_df\n",
    "    \n",
    "# def eliminate_duplicate_spectra(ms2_df,preserve_unique_iks=True):\n",
    "#     ms2_df['sum_frag_intensity'] = ms2_df['mdm_i_vals'].apply(lambda x: x.sum())\n",
    "#     ms2_df['max_score_identity'] = ms2_df['max_score_identity'].fillna(0)\n",
    "#     ms2_df.sort_values(['max_score_identity','sum_frag_intensity'],ascending=False,inplace=True) # \n",
    "#     ms2_df.reset_index(inplace=True,drop=True)\n",
    "#     spec = ms2_df.apply(lambda row: np.array([row['mdm_mz_vals'],row['mdm_i_vals']]),axis=1).tolist()\n",
    "#     precursors = ms2_df['precursor_mz'].tolist()\n",
    "#     d_specs = blink.discretize_spectra(spec,  spec, precursors, precursors, intensity_power=0.5, bin_width=0.001, tolerance=0.01,network_score=False)#,mass_diffs=mass_diffs)\n",
    "#     scores = blink.score_sparse_spectra(d_specs)\n",
    "#     similarity_matrix = scores['mzi'].todense()\n",
    "    \n",
    "#     iks = ms2_df['inchi_key_identity'].values\n",
    "#     idx_ik_same = iks[:,None] == iks\n",
    "#     # idx_ik_same = idx_ik_same.astype(int)\n",
    "\n",
    "#     has_ik = iks!=''\n",
    "#     idx_has_ik = has_ik[:,None] != has_ik\n",
    "\n",
    "#     # idx_ik_same = idx_ik_same | idx_no_ik\n",
    "#     idx_ik_same_or_dominant = idx_ik_same | idx_has_ik\n",
    "\n",
    "#     pmz_diff = abs(np.subtract.outer(ms2_df['precursor_mz'].values,ms2_df['precursor_mz'].values))\n",
    "#     idx = np.triu_indices(pmz_diff.shape[0],k=0)\n",
    "#     pmz_diff[idx] = 1000\n",
    "    \n",
    "#     frag_intensity = ms2_df['sum_frag_intensity'].values\n",
    "#     intensity_diff = -1*np.subtract.outer(frag_intensity,frag_intensity)\n",
    "#     intensity_diff = intensity_diff / frag_intensity[:,None]\n",
    "#     idx_pmz_same = pmz_diff<mz_tol\n",
    "#     idx_intensity_greater = intensity_diff>min_intensity_ratio\n",
    "#     idx_ms2similarity = similarity_matrix>similarity_cutoff\n",
    "    \n",
    "#     # if preserve_unique_iks==True:\n",
    "#     #     conditions = (idx_pmz_same) & ((idx_intensity_greater) | (idx_ms2similarity)) & (idx_ik_same_or_dominant)\n",
    "#     # else:\n",
    "#     # conditions = (idx_pmz_same) & ((idx_intensity_greater) | (idx_ms2similarity))\n",
    "#     conditions = (idx_pmz_same) & (idx_ms2similarity)\n",
    "#     r,c = np.argwhere(conditions).T\n",
    "#     ms2_df = ms2_df[~ms2_df.index.isin(r)]\n",
    "#     return ms2_df\n",
    "\n",
    "# def process_row(gg):\n",
    "#     return eliminate_duplicate_spectra(gg[1])\n",
    "\n",
    "cols = ['precursor_mz','predicted_formula','original_index','mdm_mz_vals','mdm_i_vals','inchi_key_identity','max_score_identity','precursor_mz_group']\n",
    "g = [(_,gg[cols]) for _,gg in all_spectra.groupby('precursor_mz_group')]\n",
    "with multiprocessing.Pool(20) as pool:\n",
    "    out = pool.map(eliminate_duplicate_spectra,g)\n",
    "out = pd.concat(out)\n",
    "idx1 = all_spectra['original_index'].isin(out['original_index'])\n",
    "\n",
    "# # make sure to get all the spectra that have identity matches to every unique inchi key\n",
    "# all_hits = pd.read_parquet(os.path.join(metadata_folder,'all_environmental_identity_matches.parquer'))\n",
    "# all_hits.sort_values('max_score',ascending=False,inplace=True)\n",
    "# all_hits.drop_duplicates('inchi_key',keep='first',inplace=True)\n",
    "# idx2 = all_spectra['original_index'].isin(all_hits['original_index_query'])\n",
    "\n",
    "node_data = all_spectra[idx1]\n",
    "# node_data.drop_duplicates('original_index',inplace=True)\n",
    "node_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge in the top hit for each spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hits = pd.read_parquet(os.path.join(metadata_folder,'all_environmental_identity_matches.parquet'))\n",
    "all_hits.sort_values('max_score',ascending=False,inplace=True)\n",
    "all_hits.drop_duplicates('original_index_query',keep='first',inplace=True)\n",
    "node_data.drop(columns=['inchi_key_identity','max_score_identity','original_index_query_identity'],inplace=True)\n",
    "node_data = pd.merge(node_data,all_hits.add_suffix('_identity'),left_on='original_index',right_on='original_index_query_identity',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data['original_index_query_identity'].nunique(),node_data['inchi_key_identity'].nunique(),node_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "node_data.reset_index(inplace=True,drop=True)\n",
    "node_data['mdm_spectrum'] = node_data.apply(lambda row: np.array([row['mdm_mz_vals'],row['mdm_i_vals']]),axis=1)\n",
    "node_data['original_spectrum'] = node_data.apply(lambda row: np.array([row['original_mz_vals'],row['original_i_vals']]),axis=1)\n",
    "# this is where you merge in any spectra that have a identity hit\n",
    "# out.reset_index(inplace=True,drop=True)\n",
    "# temp = pd.concat([df,out],axis=0)\n",
    "# temp.reset_index(inplace=True,drop=True)\n",
    "mass_diffs = [0] + deltas['mass'].tolist()\n",
    "node_chunks = np.array_split(node_data, np.ceil(len(node_data) / 1000))\n",
    "rem_df = []\n",
    "for i,chunk in enumerate(node_chunks):\n",
    "    temp_edges = wt.do_remblink_networking(chunk,node_data,mass_diffs=mass_diffs,spectra_attr='mdm_spectrum')\n",
    "    temp_edges.rename(columns={'rem_predicted_score':'rem_blink_score'},inplace=True)\n",
    "    cols = ['ref','query','rem_blink_score']\n",
    "    temp_edges = temp_edges[cols]\n",
    "    temp_edges['query'] = temp_edges['query']+(i*chunk.shape[0])\n",
    "    rem_df.append(temp_edges)\n",
    "    \n",
    "\n",
    "rem_df = pd.concat(rem_df)\n",
    "print(rem_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.hist(rem_df['rem_blink_score'],bins=200)\n",
    "# ax.set_yscale('log')\n",
    "ax.set_xlim(0,0.5)\n",
    "ax.axvline(0.12,color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rem_df = rem_df[rem_df['rem_blink_score']>0.12]\n",
    "print(rem_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(os.path.join(metadata_folder,'calibrated_parquet_files_with_metadata.tsv'), sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = wt.get_formula_props(node_data,formula_key='predicted_formula')\n",
    "node_data = pd.merge(node_data,p,left_on='predicted_formula',right_on='formula',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'original_index' in node_data.columns:\n",
    "    node_data.drop(columns=['original_index'],inplace=True)\n",
    "node_data.index.name = 'original_index'\n",
    "node_data.reset_index(inplace=True,drop=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# # Create the graph from the similarity matrix\n",
    "G_draft = nx.from_pandas_edgelist(rem_df,source='ref',target='query',edge_attr='rem_blink_score')\n",
    "G_draft.remove_edges_from([(u, v) for u, v in G_draft.edges() if u == v])         \n",
    "G_draft.remove_nodes_from(list(nx.isolates(G_draft)))\n",
    "\n",
    "G = G_draft.copy()\n",
    "\n",
    "drop_cols = \"\"\"mdm_mz_vals\n",
    "mdm_i_vals\n",
    "original_mz_vals\n",
    "original_i_vals\n",
    "mdm_spectrum\n",
    "original_spectrum\"\"\"\n",
    "drop_cols = drop_cols.split('\\n')\n",
    "cols = list(set(node_data.columns) - set(drop_cols))\n",
    "node_data_dict = node_data[cols].fillna('').to_dict(orient='index')\n",
    "\n",
    "nx.set_node_attributes(G, node_data_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyteomics import mgf\n",
    "\n",
    "def make_mgf(output_filename,df,G,spectra_type='nl_spectrum'):\n",
    "    cols = ['FEATURE_ID','SCANS','ORIGINAL_ID','PEPMASS','PRECURSOR_MZ','RTINSECONDS','CHARGE','MSLEVEL']\n",
    "    temp = df[df.index.isin(G.nodes)].copy()\n",
    "    temp.reset_index(inplace=True,drop=True)\n",
    "    temp['FEATURE_ID'] = temp['original_index']\n",
    "    temp['SCANS'] = temp.index.tolist() # probably needs +1\n",
    "    temp['ORIGINAL_ID'] = temp['original_index']\n",
    "    temp['CHARGE'] = '1-'\n",
    "    temp['MSLEVEL'] = 2\n",
    "    temp['RTINSECONDS'] = temp['rt']*60\n",
    "    temp['PRECURSOR_MZ'] = temp['precursor_mz']\n",
    "    temp['PEPMASS'] = temp['precursor_mz']\n",
    "\n",
    "    spectra = []\n",
    "    for i,row in temp.iterrows():\n",
    "        spectra.append({'params':row[cols].to_dict(),\n",
    "                        'm/z array':row[spectra_type][0],'intensity array':row[spectra_type][1]})\n",
    "    mgf.write(spectra,output_filename)\n",
    "    \n",
    "\n",
    "\n",
    "make_mgf('../../data/nl_spectra.mgf',node_data,G,spectra_type='mdm_spectrum')\n",
    "make_mgf('../../data/original_spectra.mgf',node_data,G,spectra_type='original_spectrum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import MolFromSmiles, MolToInchiKey\n",
    "smiles_col = 'smiles_identity'\n",
    "# Get identity hits smiles for all nodes\n",
    "identity_smiles = nx.get_node_attributes(G, smiles_col)\n",
    "node_id = [k for k, v in identity_smiles.items() if isinstance(v,str)]\n",
    "identity_smiles = [v for k, v in identity_smiles.items() if isinstance(v,str)]\n",
    "df = pd.DataFrame(data=identity_smiles,columns=['smiles'])\n",
    "df['node_id'] = node_id\n",
    "df['mol'] = df['smiles'].apply(lambda x: MolFromSmiles(x) if x is not '' else None)\n",
    "df['inchi_key'] = df['mol'].apply(lambda x: MolToInchiKey(x) if x is not None else None)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://npclassifier.gnps2.org/classify\"\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "out = []\n",
    "for i,row in df[pd.notna(df['inchi_key'])].iterrows(): # for some reason smiles is \"\" and not None\n",
    "    # smiles = df.loc[0,'smiles_identity']\n",
    "    r = requests.get(url, params={\"smiles\": row['smiles']})\n",
    "    if r.status_code == 200:\n",
    "        d = r.json()\n",
    "    else:\n",
    "        if 'C' in row['smiles']:\n",
    "            print(row['smiles'])\n",
    "        d = {'class_results': None,\n",
    "                     'superclass_results': None,\n",
    "                     'pathway_results': None,\n",
    "                     'isglycoside': False}\n",
    "    d['inchi_key'] = row['inchi_key']\n",
    "    out.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = pd.DataFrame(out)\n",
    "cols = ['class_results', 'superclass_results', 'pathway_results']\n",
    "for c in cols:\n",
    "    out2[c] = out2[c].apply(lambda x: ','.join(sorted(x)) if isinstance(x,list) else '')\n",
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_list_identifiers(df,identifier='class_results'):\n",
    "    temp = df.loc[pd.notna(df[identifier]),identifier].tolist()\n",
    "    temp = [x.split(',') for x in temp]\n",
    "    temp = [item for sublist in temp for item in sublist]\n",
    "    temp = pd.Series(temp)\n",
    "    pathway_counts = temp.value_counts()\n",
    "    for i,row in df[pd.notna(df[identifier])].iterrows():\n",
    "        split_results = row[identifier].split(',')\n",
    "        if len(split_results) > 1:\n",
    "            d = {}\n",
    "            for r in split_results:\n",
    "                d[r] = pathway_counts[r]\n",
    "            df.at[i, identifier] = max(d, key=d.get)\n",
    "\n",
    "    return df\n",
    "\n",
    "out2 = replace_list_identifiers(out2,identifier='class_results')\n",
    "out2 = replace_list_identifiers(out2,identifier='pathway_results')\n",
    "out2 = replace_list_identifiers(out2,identifier='superclass_results')\n",
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df,out2,on='inchi_key',how='left')\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['class_results', 'superclass_results', 'pathway_results']:\n",
    "    nx.set_node_attributes(G, df.set_index('node_id')[c].fillna('').to_dict(), c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(G,'../../data/CarbonNetwork_noprop.graphml')\n",
    "# G_filt = nx.maximum_spanning_tree(G)\n",
    "# nx.write_graphml(G_filt, '/global/cfs/cdirs/metatlas/projects/carbon_network/CarbonNetwork_mst_noprop.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_graphml('../../data/CarbonNetwork_noprop.graphml')\n",
    "# main_column = 'pathway_results'\n",
    "main_column = 'class_results'\n",
    "all_columns = ['class_results', 'superclass_results', 'pathway_results']\n",
    "df = G.nodes(data=True) \n",
    "df = pd.DataFrame(df, columns=['node_id', 'data'])\n",
    "df.set_index('node_id', inplace=True)\n",
    "df = df['data'].apply(pd.Series)\n",
    "df.reset_index(inplace=True,drop=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_most_common(classes):\n",
    "#     class_counts = Counter(classes)\n",
    "#     # Get the most common string\n",
    "#     most_common_class = class_counts.most_common(1)[0][0]\n",
    "#     return most_common_class\n",
    "\n",
    "# classyfire_dir = '/global/cfs/cdirs/metatlas/projects/classyfire_annotations/'\n",
    "# superclass_name = []\n",
    "# class_name = []\n",
    "# subclass_name = []\n",
    "# iks = df.loc[pd.notna(df['inchi_key']),'inchi_key'].unique()\n",
    "# for ik in iks:\n",
    "#     f = '%s.json'%ik\n",
    "#     f = os.path.join(classyfire_dir,f)\n",
    "#     with open(f,'r') as fid:\n",
    "#         cf = fid.read()\n",
    "#     cf = json.loads(cf.strip())\n",
    "#     if isinstance(cf,str):\n",
    "#         cf = json.loads(cf)\n",
    "#     # cf_json = json.dumps(cf)\n",
    "#     if not main_column in cf:\n",
    "#         cf[main_column] = {'name':None}\n",
    "#     if not 'superclass' in cf:\n",
    "#         cf['superclass'] = {'name':None}\n",
    "#         cf['class'] = {'name':None}\n",
    "#     if cf[main_column] is None:\n",
    "#         cf[main_column] = {'name':None}\n",
    "#     superclass_name.append(cf['superclass']['name'])\n",
    "#     class_name.append(cf['class']['name'])\n",
    "#     subclass_name.append(cf[main_column]['name'])\n",
    "\n",
    "# temp = pd.DataFrame(index=iks)\n",
    "# temp['superclass'] = superclass_name\n",
    "# temp['class'] = class_name\n",
    "# temp[main_column] = subclass_name\n",
    "\n",
    "# df = pd.merge(df,temp,left_on='inchi_key',right_index=True,how='left')\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['node_id']=='41.0',all_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_additional_subgraph(graph, original_nodes,radius=1,min_count=0):\n",
    "    nodes = []\n",
    "    for node in original_nodes:\n",
    "        # Create an ego graph centered at the current node\n",
    "        \n",
    "        ego_subgraph = nx.ego_graph(graph, node,radius=radius)\n",
    "        nodes.extend(list(ego_subgraph.nodes))\n",
    "    df = pd.DataFrame(data=nodes,columns=['nodes'])\n",
    "    df = df.groupby('nodes').filter(lambda x: len(x) >= min_count)\n",
    "    nodes = df['nodes'].tolist() + original_nodes\n",
    "    \n",
    "    nodes = list(set(nodes))\n",
    "    nodes = sorted(nodes)\n",
    "    return nodes\n",
    "\n",
    "sc_df = pd.DataFrame()\n",
    "sc_df['node_id'] = df['node_id'].values\n",
    "sc_df[main_column] = df[main_column].values\n",
    "for class_name in sc_df.loc[pd.notna(sc_df[main_column]),main_column].unique():\n",
    "    idx = sc_df[main_column]==class_name\n",
    "    original_nodes = sc_df.loc[idx,'node_id'].tolist()\n",
    "    new_nodes = min_additional_subgraph(G, original_nodes,radius=2,min_count=3)\n",
    "    sc_df[class_name] = False\n",
    "    # idx = sc_df['node_id'].isin(original_nodes)\n",
    "    # sc_df.loc[idx,class_name] = True\n",
    "    idx = sc_df['node_id'].isin(new_nodes)\n",
    "    sc_df.loc[idx,class_name] = True\n",
    "sc_df.set_index('node_id',inplace=True)\n",
    "sc_df.drop(columns=[main_column],inplace=True)\n",
    "sc_df\n",
    "# For each row in the DataFrame\n",
    "# If the sum of the row is greater than 1\n",
    "# Find the class with the highest frequency\n",
    "# Set all other classes to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for columns with more than one True value, get the regional value_counts from an ego graph of radius 5 and replace with the most common value\n",
    "for i,row in sc_df[sc_df.sum(axis=1)>1].iterrows():\n",
    "    ego_subgraph = nx.ego_graph(G, i,radius=1) #was 5\n",
    "    nodes = list(ego_subgraph.nodes)\n",
    "    temp = sc_df.loc[nodes]\n",
    "    temp = temp.sum()\n",
    "    temp = temp[temp>0]\n",
    "    if len(temp)>0:\n",
    "        most_common_class = temp.idxmax()\n",
    "        sc_df.loc[i] = False\n",
    "        sc_df.loc[i,most_common_class] = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a list of the classes that are most common\n",
    "# ranked_frequency = sc_df.sum(axis=0).sort_values(ascending=False)\n",
    "# # for nodes that are in more than one class, only keep the most abundant class\n",
    "# for i,row in sc_df[sc_df.sum(axis=1)>1].iterrows():\n",
    "#     # get the most commonly occuring class\n",
    "#     keep_class = ranked_frequency[row[row==True].index].sort_values(ascending=False).index[0]\n",
    "#     # get the indices of all the assigned classes\n",
    "#     idx = row[row==True].index\n",
    "#     # drop all but the most common\n",
    "#     idx = idx[idx!=keep_class]\n",
    "#     # zero them out\n",
    "#     sc_df.loc[i,idx] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for any class that has an ID in the original, reset it back to its original state\n",
    "for i,row in df.iterrows():\n",
    "    if pd.notna(row[main_column]):\n",
    "        sc_df.loc[row['node_id'],:] = False\n",
    "        sc_df.loc[row['node_id'],row[main_column]] = True\n",
    "\n",
    "\n",
    "# recomute the ranked frequency now that ambiguous classes have been resolved\n",
    "ranked_frequency = sc_df.sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = sc_df.columns\n",
    "sc_df = sc_df.reset_index().melt(id_vars='node_id',value_vars=cols)\n",
    "idx = sc_df['value']==True\n",
    "sc_df = sc_df[idx]\n",
    "sc_df = sc_df[pd.notna(sc_df['variable'])]\n",
    "sc_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_df.value_counts('variable').head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to graph\n",
    "cluster_dict = sc_df.copy()\n",
    "cluster_dict.rename(columns={'variable':'structural_cluster_subclassname'},inplace=True)\n",
    "print(cluster_dict.shape)\n",
    "if main_column != 'pathway_results':\n",
    "    cluster_dict = pd.merge(df[all_columns].drop_duplicates(main_column),cluster_dict,left_on=main_column,right_on='structural_cluster_subclassname',how='right')\n",
    "else:\n",
    "    cluster_dict[main_column] = cluster_dict['structural_cluster_subclassname']\n",
    "\n",
    "cluster_dict.drop(columns=['structural_cluster_subclassname','value'],inplace=True)\n",
    "cluster_dict.drop_duplicates('node_id',inplace=True)\n",
    "cluster_dict.set_index('node_id',inplace=True)\n",
    "cluster_dict.fillna('',inplace=True)\n",
    "cluster_dict = cluster_dict.add_suffix('_propagated')\n",
    "\n",
    "cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_dict.shape)\n",
    "# need to understand why there are duplicates\n",
    "\n",
    "cluster_dict = cluster_dict.to_dict(orient='index')\n",
    "\n",
    "nx.set_node_attributes(G, cluster_dict)\n",
    "# cluster_dict\n",
    "# nx.set_node_attributes(G, cluster_dict, 'mcs_structural_cluster_number')\n",
    "# len(clusters),len(set(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = G.nodes(data=True)\n",
    "df = pd.DataFrame(df)\n",
    "df.columns = ['node','data']\n",
    "df['data'] = df['data'].apply(lambda x: dict(x))\n",
    "df = pd.concat([df.drop(['data'], axis=1), df['data'].apply(pd.Series)], axis=1)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "df.value_counts('pathway_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts('pathway_results_propagated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = G.nodes(data=True)\n",
    "# df = pd.DataFrame(df)\n",
    "# df.columns = ['node','data']\n",
    "# df['data'] = df['data'].apply(lambda x: dict(x))\n",
    "# df = pd.concat([df.drop(['data'], axis=1), df['data'].apply(pd.Series)], axis=1)\n",
    "# df.reset_index(inplace=True,drop=True)\n",
    "# df['consensus_class'] = None\n",
    "\n",
    "# # Count the number of occurrences of each subclass\n",
    "# subclass_counts = df[main_column].value_counts()\n",
    "\n",
    "# # Get the subclasses that have at least 100 values\n",
    "# large_subclasses = subclass_counts[subclass_counts >= 100].index\n",
    "\n",
    "# # Set the consensus_class for large subclasses\n",
    "# df.loc[df[main_column].isin(large_subclasses), 'consensus_class'] = df.loc[df[main_column].isin(large_subclasses), main_column]\n",
    "\n",
    "# # Get the subclasses that have less than 100 values\n",
    "# small_subclasses = subclass_counts[subclass_counts < 100].index\n",
    "\n",
    "# # Set the consensus_class for small subclasses\n",
    "# df.loc[df[main_column].isin(small_subclasses), 'consensus_class'] = df.loc[df[main_column].isin(small_subclasses), 'class']\n",
    "\n",
    "# # Get the subclasses that have less than 100 values\n",
    "# class_counts = df['consensus_class'].value_counts()\n",
    "# small_classes = class_counts[class_counts < 100].index\n",
    "\n",
    "# # Set the consensus_class for small subclasses\n",
    "# df.loc[df['consensus_class'].isin(small_classes), 'consensus_class'] = df.loc[df['consensus_class'].isin(small_classes), 'superclass']\n",
    "\n",
    "# df = df[['node','consensus_class']]\n",
    "\n",
    "# df.loc[df['consensus_class'].map(df['consensus_class'].value_counts()) < 100, 'consensus_class'] = None\n",
    "\n",
    "# df.fillna('',inplace=True)\n",
    "# df.value_counts('consensus_class')\n",
    "# nx.set_node_attributes(G, df.set_index('node')['consensus_class'].to_dict(), 'consensus_class')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import networkx as nx\n",
    "# from pathlib import Path\n",
    "# try:\n",
    "#     module_path = os.path.join(Path(__file__).parents[2])\n",
    "# except: # Jupyter notebook probably\n",
    "#     module_path = os.path.join(os.getcwd(), '../../')\n",
    "# graphml_file = os.path.join(module_path, 'data/CarbonNetwork.graphml')\n",
    "# G = nx.read_graphml(graphml_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.write_graphml(G,'/global/cfs/cdirs/metatlas/projects/carbon_network/CarbonNetwork.graphml')\n",
    "# G_filt = nx.maximum_spanning_tree(G)\n",
    "# nx.write_graphml(G_filt, '/global/cfs/cdirs/metatlas/projects/carbon_network/CarbonNetwork_mst.graphml')\n",
    "print('nodes',len(G.nodes))\n",
    "print('edges',len(G.edges))\n",
    "\n",
    "nx.write_graphml(G,'../../data/CarbonNetwork-all-edges.graphml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a maximum spanning tree from G\n",
    "T = nx.maximum_spanning_tree(G, weight='rem_blink_score')\n",
    "\n",
    "# Get the list of edges in G sorted by weight in descending order\n",
    "edges_sorted_by_weight = sorted(G.edges(data=True), key=lambda x: x[2]['rem_blink_score'], reverse=True)\n",
    "max_degree = 3\n",
    "# Add edges back to the tree from the original network until there are 30000 edges total in the network\n",
    "for u, v, data in edges_sorted_by_weight:\n",
    "    if T.number_of_edges() >= 350000:\n",
    "        break\n",
    "    # check the degree of the nodes\n",
    "    if T.degree(u) > max_degree:\n",
    "        continue\n",
    "    if T.degree(v) > max_degree:\n",
    "        continue\n",
    "    if not T.has_edge(u, v):\n",
    "        T.add_edge(u, v, **data)\n",
    "    \n",
    "\n",
    "print('nodes', len(T.nodes))\n",
    "print('edges', len(T.edges))\n",
    "\n",
    "# G.remove_nodes_from(list(nx.isolates(G)))\n",
    "# k = 5  # replace with your desired k\n",
    "\n",
    "# from blink.utils import filter_top_k\n",
    "# # filter_top_k(G, top_k=k, edge_score='rem_blink_score')\n",
    "\n",
    "\n",
    "\n",
    "# drop_cols = \"\"\"mdm_mz_vals\n",
    "# mdm_i_vals\n",
    "# original_mz_vals\n",
    "# original_i_vals\n",
    "# mdm_spectrum\n",
    "# original_spectrum\"\"\"\n",
    "# drop_cols = drop_cols.split('\\n')\n",
    "# cols = list(set(node_data.columns) - set(drop_cols))\n",
    "\n",
    "# node_data_dict = node_data[cols].fillna('').to_dict(orient='index')\n",
    "\n",
    "# nx.set_node_attributes(T, node_data_dict)\n",
    "# # Remove self-loops\n",
    "# Remove isolates\n",
    "T.remove_nodes_from(list(nx.isolates(T)))\n",
    "print('nodes',len(T.nodes))\n",
    "print('edges',len(T.edges))\n",
    "nx.write_graphml(T,'../../data/CarbonNetwork.graphml')\n",
    "# nx.write_graphml(G,'CarbonNetwork.graphml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_sorted_by_weight[40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the edge data from T and store it as a data from with \"source\" and \"target\" and \"weight\" attributes\n",
    "edge_data = nx.to_pandas_edgelist(T)\n",
    "edge_data['source'] = edge_data['source'].astype(float).astype(int)\n",
    "edge_data['target'] = edge_data['target'].astype(float).astype(int)\n",
    "edge_data = edge_data[['source','target']]\n",
    "edge_data.to_csv('../../data/edge_data.csv',index=None)\n",
    "\n",
    "# get the node data from T and store it as a data from with \"node\" and \"data\" attributes\n",
    "node_data = T.nodes(data=True)\n",
    "node_data = pd.DataFrame(node_data)\n",
    "node_data.columns = ['node','data']\n",
    "node_data['data'] = node_data['data'].apply(lambda x: dict(x))\n",
    "node_data = pd.concat([node_data.drop(['data'], axis=1), node_data['data'].apply(pd.Series)], axis=1)\n",
    "node_data.to_csv('../../data/node_data.csv',index=None)\n",
    "\n",
    "# https://cosmograph.app/run/?data=https://raw.githubusercontent.com/benbowen/thoughts/master/edge_data.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this will stop the run\n",
    "# # import networkx as nx\n",
    "\n",
    "# # # Get the connected components in the graph\n",
    "# components = list(nx.connected_components(G))\n",
    "\n",
    "# # # Filter out subgraphs with less than 1000 nodes\n",
    "# filtered_components = [component for component in components if len(component) >= 1000]\n",
    "\n",
    "# # # Create a new graph\n",
    "# G_filtered = nx.Graph()\n",
    "\n",
    "# # # Add each subgraph to the new graph\n",
    "# for component in filtered_components:\n",
    "#     G_filtered = nx.compose(G_filtered, G.subgraph(component))\n",
    "    \n",
    "# # alg = 'sfdp'\n",
    "\n",
    "# # do it like this\n",
    "# # pos = graphviz_layout(G, prog='dot', root=0, args='-Grankdir=\"LR\"')\n",
    "\n",
    "# alg = 'neato'\n",
    "# # pos = nx.nx_agraph.graphviz_layout(G_filtered, prog=alg,args='-Goverlap=\"scale\"')\n",
    "# # pos = nx.nx_agraph.graphviz_layout(G_filtered, prog=alg,args='-Goverlap=\"False\"') #blob\n",
    "# # pos = nx.nx_agraph.graphviz_layout(G_filtered, prog=alg,args='-Goverlap=\"vpsc\"') #crashes\n",
    "# pos = nx.nx_agraph.graphviz_layout(G_filtered, prog=alg,args='-Goverlap=\"orthoxy\"') #looks like a lightning bolt\n",
    "# pos = nx.nx_agraph.graphviz_layout(G_filtered, prog=alg,args='-Goverlap=\"ipsep\"') #looks like \n",
    "# ipsep\n",
    "\n",
    "# for node, (x, y) in pos.items():\n",
    "#     G_filtered.nodes[node]['x'] = x\n",
    "#     G_filtered.nodes[node]['y'] = y\n",
    "\n",
    "# nx.write_graphml(G_filtered, graphml_file.replace('.graphml', '_with-positions.graphml'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# nx.draw(G_filtered, pos, with_labels=False, font_weight='bold', node_color='lightblue', node_size=3)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove node overlaps\n",
    "# pos = nx.kamada_kawai_layout(G_filtered)\n",
    "# for node, (x, y) in pos.items():\n",
    "#     G_filtered.nodes[node]['x'] = x\n",
    "#     G_filtered.nodes[node]['y'] = y\n",
    "# nx.write_graphml(G_filtered, graphml_file.replace('.graphml', '_with-positions_kamada-kawai.graphml'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
