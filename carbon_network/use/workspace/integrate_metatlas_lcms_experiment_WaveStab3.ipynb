{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rdkit:Enabling RDKit 2023.09.1 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from pyteomics import mgf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pingouin as pg\n",
    "import seaborn as sns\n",
    "sys.path.insert(0,'/global/homes/b/bpb/repos/metatlas')\n",
    "from metatlas.io import feature_tools as ft\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# replace with submodules\n",
    "\n",
    "\n",
    "\n",
    "sys.path.insert(0,'../')\n",
    "import analysis_tools as at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT_wavestab1_supern-WAVE-NatCom-NLDM-Day7-vs-supern-WAVE-NatCom-NLDM-Day0.csv\n"
     ]
    }
   ],
   "source": [
    "# experiment directory\n",
    "## note: must follow JGI file naming conventions and be converted to hdf5 format\n",
    "\n",
    "\n",
    "# Syndac5 experiment\n",
    "# /global/cfs/cdirs/metatlas/raw_data/egsb/20230706_EB_CL_107002-011_SynDAC5_20230630_EXP120A_C18-EP_USDAY72349\n",
    "\n",
    "\n",
    "# wavestab1 experiment\n",
    "exp_dir = '/global/cfs/cdirs/metatlas/raw_data/egsb/20231018_EB_MdR_109570-002_WAVEstab_20231017_EXP120A_C18-EP_USDAY72349_vols'\n",
    "group_control = 'supern-WAVE-NatCom-NLDM-Day0'\n",
    "group_treatment = 'supern-WAVE-NatCom-NLDM-Day7'\n",
    "my_groups = {'control':group_control,'treatment':group_treatment}\n",
    "experiment_name = 'wavestab1'\n",
    "output_filename = f'OUTPUT_{experiment_name}_{group_treatment}-vs-{group_control}.csv'\n",
    "print(output_filename)\n",
    "####  All groups for wavestab1\n",
    "# ['supern-WAVE-NatCom-NLDM-Day7', 'TxCtrl-NA-Sterile-Salts-Day0',\n",
    "#        'TxCtrl-NA-Sterile-NLDM-Day0', 'supern-WAVE-NatCom-Salts-Day0',\n",
    "#        'supern-WAVE-NatCom-Salts-Day7', 'supern-WAVE-NatCom-NLDM-Day0']\n",
    "\n",
    "\n",
    "# russell ranch experiment\n",
    "# exp_dir = '/global/cfs/cdirs/metatlas/raw_data/egsb/20231113_EB_SMK_107002-011_CenturyExp_20230414_EXP120A_C18-EP_USDAY72349'\n",
    "# my_groups = None\n",
    "\n",
    "# wavestab3 experiment\n",
    "# exp_dir  = '/global/cfs/cdirs/metatlas/raw_data/egsb/20240125_EB_MdR_101544-059_WAVESTAB3_20231222_EXP120A_C18-EP_USDAY72349'\n",
    "# group_control = 'supern-CentExp-OMT1d2-Sterile-d0-NA'\n",
    "# group_treatment = 'supern-CentExp-OMT1d2-NatCom-d7-NA'\n",
    "# my_groups = {'control':group_control,'treatment':group_treatment}\n",
    "# output_filename = 'nldm_wavestab3_output.csv'\n",
    "\n",
    "# tolerance in ppm between experimental signal and node mz\n",
    "mz_ppm_tolerance = 5\n",
    "peak_height_min = 1e4\n",
    "num_datapoints_min = 10\n",
    "# minimum MSMS score \n",
    "msms_score_min = 0.5\n",
    "\n",
    "# minimum MSMS matching ion count\n",
    "msms_matches_min = 3\n",
    "\n",
    "# retention time range in minutes for feature finding\n",
    "rt_range = [1, 700]\n",
    "\n",
    "# tolerance in daltons used for calculating MS/MS similarity scores\n",
    "frag_mz_tolerance = 0.05\n",
    "\n",
    "# combine all parameters into a single dictionary and export to a file\n",
    "params = {'mz_ppm_tolerance':mz_ppm_tolerance,\n",
    "          'peak_height_min':peak_height_min,\n",
    "          'num_datapoints_min':num_datapoints_min,\n",
    "          'msms_score_min':msms_score_min,\n",
    "          'msms_matches_min':msms_matches_min,\n",
    "          'rt_range':rt_range,\n",
    "          'frag_mz_tolerance':frag_mz_tolerance}\n",
    "params['exp_dir'] = exp_dir\n",
    "params['my_groups'] = my_groups\n",
    "params['output_filename'] = output_filename\n",
    "\n",
    "with open(output_filename.replace('.csv','.params'),'w') as f:\n",
    "    for key in params.keys():\n",
    "        f.write(\"%s: %s\\n\"%(key,params[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing original_spectra.mgf\n",
      "INFO:root:Processing nl_spectra.mgf\n"
     ]
    }
   ],
   "source": [
    "# collect and merge required data and metadata\n",
    "\n",
    "node_data = at.graph_to_df()\n",
    "node_atlas = at.make_node_atlas(node_data, rt_range)\n",
    "merged_node_data = at.merge_spectral_data(node_data)\n",
    "files_data = at.get_files_df(exp_dir,parse_filename=True,groups=my_groups)\n",
    "files = files_data['filename'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sample_category</th>\n",
       "      <th>experiment</th>\n",
       "      <th>sampletype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>control</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>treatment</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>treatment</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>control</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>treatment</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>control</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>control</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>control</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>treatment</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>treatment</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>treatment</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>control</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>treatment</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>control</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>control</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>/global/cfs/cdirs/metatlas/raw_data/egsb/20231...</td>\n",
       "      <td>treatment</td>\n",
       "      <td>WAVEstab_20231017</td>\n",
       "      <td>supern-WAVE-NatCom-NLDM-Day7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename sample_category  \\\n",
       "0   /global/cfs/cdirs/metatlas/raw_data/egsb/20231...         control   \n",
       "1   /global/cfs/cdirs/metatlas/raw_data/egsb/20231...       treatment   \n",
       "4   /global/cfs/cdirs/metatlas/raw_data/egsb/20231...       treatment   \n",
       "5   /global/cfs/cdirs/metatlas/raw_data/egsb/20231...         control   \n",
       "9   /global/cfs/cdirs/metatlas/raw_data/egsb/20231...       treatment   \n",
       "14  /global/cfs/cdirs/metatlas/raw_data/egsb/20231...         control   \n",
       "22  /global/cfs/cdirs/metatlas/raw_data/egsb/20231...         control   \n",
       "26  /global/cfs/cdirs/metatlas/raw_data/egsb/20231...         control   \n",
       "29  /global/cfs/cdirs/metatlas/raw_data/egsb/20231...       treatment   \n",
       "31  /global/cfs/cdirs/metatlas/raw_data/egsb/20231...       treatment   \n",
       "32  /global/cfs/cdirs/metatlas/raw_data/egsb/20231...       treatment   \n",
       "33  /global/cfs/cdirs/metatlas/raw_data/egsb/20231...         control   \n",
       "34  /global/cfs/cdirs/metatlas/raw_data/egsb/20231...       treatment   \n",
       "36  /global/cfs/cdirs/metatlas/raw_data/egsb/20231...         control   \n",
       "38  /global/cfs/cdirs/metatlas/raw_data/egsb/20231...         control   \n",
       "39  /global/cfs/cdirs/metatlas/raw_data/egsb/20231...       treatment   \n",
       "\n",
       "           experiment                    sampletype  \n",
       "0   WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day0  \n",
       "1   WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day7  \n",
       "4   WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day7  \n",
       "5   WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day0  \n",
       "9   WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day7  \n",
       "14  WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day0  \n",
       "22  WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day0  \n",
       "26  WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day0  \n",
       "29  WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day7  \n",
       "31  WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day7  \n",
       "32  WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day7  \n",
       "33  WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day0  \n",
       "34  WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day7  \n",
       "36  WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day0  \n",
       "38  WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day0  \n",
       "39  WAVEstab_20231017  supern-WAVE-NatCom-NLDM-Day7  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee726fbf21ea4ad1b8dc4c399ef17fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73dc98b4fc484b4cb4065b61000b36d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get ms1 and ms2 data\n",
    "ms1_data = at.get_sample_ms1_data(node_atlas, files, mz_ppm_tolerance,peak_height_min,num_datapoints_min)\n",
    "max_ms1_data = at.get_best_ms1_rawdata(ms1_data,node_data)\n",
    "ms2_data = at.get_sample_ms2_data(files,merged_node_data,msms_score_min,msms_matches_min,mz_ppm_tolerance,frag_mz_tolerance)\n",
    "max_ms2_data = at.get_best_ms2_rawdata(ms2_data)\n",
    "best_hits = at.get_best_ms1_ms2_combined(max_ms1_data,max_ms2_data)\n",
    "\n",
    "stats_df = at.do_basic_stats(ms1_data,files_data)\n",
    "output_df = at.make_output_df(node_data,best_hits,stats_df,filename=output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1_data['experiment'] = ms1_data['lcmsrun_observed'].apply(lambda x: '_'.join(x.split('/')[-1].split('_')[4:6]))\n",
    "ms1_data['sampletype'] = ms1_data['lcmsrun_observed'].apply(lambda x: x.split('/')[-1].split('_')[12])\n",
    "ms1_data['time'] = ms1_data['lcmsrun_observed'].apply(lambda x: x.split('/')[-1].split('_')[12].split('-')[-2])\n",
    "ms1_data['community'] = ms1_data['lcmsrun_observed'].apply(lambda x: x.split('/')[-1].split('_')[12].split('-')[-3])\n",
    "ms1_data['supplement'] = ms1_data['lcmsrun_observed'].apply(lambda x: x.split('/')[-1].split('_')[12].split('-')[-1])\n",
    "ms1_data\n",
    "# out['sampletype1'] = out['filename'].apply(lambda x: '-'.join(x.split('/')[-1].split('_')[12].split('-')[1:]))\n",
    "# # out['sampletype2'] = out['filename'].apply(lambda x: x.split('/')[-1].split('_')[14].split('-')[3])\n",
    "# g = out.groupby('filename')['peak_area'].median()\n",
    "# g.sort_values(ascending=False,inplace=True)\n",
    "# g.head(20).to_csv('ones_that_are_big.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1_data.rename(columns={'label':'node_id'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sample = ms1_data[ms1_data['time']!='NA'].pivot_table(columns='node_id',index=['time','community','supplement'],values='peak_area',aggfunc='mean',fill_value=300)\n",
    "d_control = ms1_data[ms1_data['time']=='NA'].pivot_table(columns='node_id',index=['time','community','supplement'],values='peak_area',aggfunc='mean',fill_value=300)\n",
    "fig,ax = plt.subplots()\n",
    "d_sample.max().apply(np.log10).hist(ax=ax,bins=100,label='sample')\n",
    "d_control.max().apply(np.log10).hist(ax=ax,bins=100,label='control')\n",
    "max_sample = d_sample.max()\n",
    "min_sample = d_sample.min()\n",
    "idx = (max_sample>5e6) #& ((max_sample/min_sample)>5)\n",
    "\n",
    "d_sample = d_sample[max_sample[idx].index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Normalize the columns of d_sample\n",
    "scaler = MinMaxScaler()\n",
    "d_sample_scaled = scaler.fit_transform(d_sample)\n",
    "\n",
    "# Compute the distance matrix\n",
    "d_sample_dist = sch.distance.pdist(d_sample_scaled)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "d_sample_linkage = sch.linkage(d_sample_dist, method='average')\n",
    "labels = list(map(str, d_sample.index.tolist()))\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(6, 10))\n",
    "sch.dendrogram(d_sample_linkage,labels=labels,orientation='right')\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Conditions')\n",
    "# plt.title('Dendrogram of d_sample')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the values of d_sample_scaled in d_sample\n",
    "d_sample = out.pivot_table(columns='node_id',index=['depth','environment'],values='peak_area',aggfunc='mean',fill_value=300)\n",
    "# fig,ax = plt.subplots()\n",
    "# d_sample.max().apply(np.log10).hist(ax=ax,bins=100,label='sample')\n",
    "\n",
    "# idx = (max_sample>10000) & ((max_sample/min_sample)>5)\n",
    "\n",
    "# d_sample = d_sample[max_sample[idx].index]\n",
    "level1_values = d_sample.index.get_level_values(0).unique()\n",
    "temp = d_sample[d_sample.index.get_level_values(0)==level1_values[0]]\n",
    "min_sample = temp.min()\n",
    "temp = temp - min_sample\n",
    "max_sample = temp.max()\n",
    "temp = temp / max_sample\n",
    "d_sample[d_sample.index.get_level_values(0)==level1_values[0]] = temp\n",
    "\n",
    "temp = d_sample[d_sample.index.get_level_values(0)==level1_values[1]]\n",
    "min_sample = temp.min()\n",
    "temp = temp - min_sample\n",
    "max_sample = temp.max()\n",
    "temp = temp / max_sample\n",
    "d_sample[d_sample.index.get_level_values(0)==level1_values[1]] = temp\n",
    "\n",
    "\n",
    "d_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sample_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "# Normalize the columns of d_sample\n",
    "# scaler = StandardScaler()\n",
    "# d_sample_scaled = scaler.fit_transform(d_sample)\n",
    "\n",
    "d_sample_scaled = d_sample.apply(lambda x: np.log2(x / d_sample.loc[('d0', 'Sterile', 'NA')]),axis=1)\n",
    "d_sample_scaled = d_sample_scaled.drop(('d0', 'Sterile', 'NA'))\n",
    "\n",
    "# Compute the distance matrix\n",
    "d_sample_dist = sch.distance.pdist(d_sample_scaled.T)\n",
    "d_sample_linkage = sch.linkage(d_sample_dist, method='average')\n",
    "\n",
    "# Form clusters from the hierarchical clustering\n",
    "max_d = 5\n",
    "  # max_d is the maximum distance to form cluster. You can adjust this value\n",
    "clusters = fcluster(d_sample_linkage, max_d, criterion='distance')\n",
    "n = len(pd.unique(clusters))\n",
    "print(n)\n",
    "# Create a DataFrame from the scaled data\n",
    "d_sample_scaled_df = pd.DataFrame(d_sample_scaled.T, index=d_sample.columns, columns=d_sample.index)\n",
    "\n",
    "\n",
    "\n",
    "# Add the cluster labels to the DataFrame\n",
    "# d_sample_scaled_df['cluster'] = clusters\n",
    "min_nodes=5\n",
    "cols = d_sample_scaled_df.T.columns\n",
    "num_rows = 0\n",
    "for my_cluster in pd.unique(clusters):\n",
    "    my_cols = cols[clusters==my_cluster]\n",
    "    if len(my_cols)<min_nodes:\n",
    "        continue\n",
    "    num_rows += 1\n",
    "print(num_rows)\n",
    "fig,ax = plt.subplots(figsize=(12, 4*num_rows),ncols=2,nrows=num_rows,sharey=True,sharex=True)\n",
    "ax = ax.flatten()\n",
    "counter = 0\n",
    "save_clusters = []\n",
    "for my_cluster in pd.unique(clusters):\n",
    "    my_cols = cols[clusters==my_cluster]\n",
    "    if len(my_cols)<min_nodes:\n",
    "        continue\n",
    "    save_clusters.append(my_cluster)\n",
    "    d = d_sample_scaled_df.T[my_cols].copy()\n",
    "    d.reset_index(inplace=True,drop=False)\n",
    "    d['community-supplement'] = d.apply(lambda row: row['community'] + '-' + row['supplement'],axis=1)\n",
    "    d = d.melt(id_vars=['time','community','supplement','community-supplement'], var_name='node_id', value_name='value')\n",
    "    # d['community-supplement'] = d['community-supplement'].astype(str)\n",
    "    for g in ['NatCom','Sterile']:\n",
    "        sns.boxplot(x='time', y='value', hue='supplement',data=d[d['community']==g],ax=ax[counter])\n",
    "        ax[counter].set_title(g,fontsize=14)\n",
    "        if g=='NatCom':\n",
    "            ax[counter].set_ylabel(\"Log2 Fold Change relative to ('d0', 'Sterile', 'NA')\\nCluster %d: %d nodes\"%(my_cluster,len(my_cols)))\n",
    "        counter += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Assuming `t` is your DataFrame with a MultiIndex\n",
    "linestyles = ['-', '--']  # Define the linestyles you want to use\n",
    "# stacked bar chart for each environment/depth.  Each stack will be a compound class.\n",
    "# class_level = 'pathway_results_propagated'\n",
    "# class_level='superclass_results_propagated'\n",
    "class_level='class_results_propagated'\n",
    "\n",
    "df[class_level] = df[class_level].fillna('Unknown')\n",
    "u_class = df[class_level].unique()\n",
    "plotted_count = 0\n",
    "for u in u_class:\n",
    "    my_nodes = df[df[class_level]==u]['node_id'].tolist()\n",
    "    my_nodes = list(set(my_nodes) & set(d_sample_scaled.columns))\n",
    "    if len(my_nodes)>min_nodes:\n",
    "        plotted_count += 1\n",
    "nrows=int(np.ceil(plotted_count/4))\n",
    "fig,ax = plt.subplots(nrows=nrows,ncols=4,figsize=(24,nrows*3),sharex=True,sharey=False)\n",
    "ax = ax.flatten()\n",
    "counter = 0\n",
    "for u in u_class:\n",
    "    my_nodes = df[df[class_level]==u]['node_id'].tolist()\n",
    "    my_nodes = list(set(my_nodes) & set(d_sample_scaled.columns))\n",
    "    if len(my_nodes)<min_nodes:\n",
    "        continue\n",
    "    t = d_sample_scaled[my_nodes]\n",
    "    t_melted = t.reset_index().melt(id_vars=t.index.names)\n",
    "\n",
    "    t_melted['community-supplement'] = d.apply(lambda row: row['community'] + '-' + row['supplement'],axis=1)\n",
    "\n",
    "    # sns.violinplot(t_melted,x='environment',y='value',hue='depth',ax=ax[counter],split=True)\n",
    "    sns.boxplot(t_melted,x='community-supplement',y='value',hue='time',ax=ax[counter])\n",
    "    # ax[counter].set_ylim(-0.1,1.1)\n",
    "    ax[counter].set_title('%s with %d nodes'%(u,len(my_nodes)))\n",
    "    ax[counter].set_ylim(-2,2)\n",
    "    ax[counter].grid()\n",
    "    # ax[counter].get_legend().remove()\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "# t_melted = t.reset_index().melt(id_vars=t.index.names)\n",
    "# t_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.cm as cm\n",
    "min_val = 0.03\n",
    "counter = 0\n",
    "for cluster_col in ['pathway_results_propagated','superclass_results_propagated','class_results_propagated']:\n",
    "    t = []\n",
    "    fig,ax = plt.subplots(figsize=(12,2))#,nrows=1,ncols=1,sharex=True)\n",
    "\n",
    "    for my_cluster in save_clusters:\n",
    "        my_cols = cols[clusters==my_cluster]\n",
    "        temp = df.loc[df['node_id'].isin(my_cols),cluster_col]\n",
    "        temp.fillna('Unknown',inplace=True)\n",
    "        temp = temp.value_counts()\n",
    "        temp = temp / temp.sum()\n",
    "        temp = temp[temp>min_val]\n",
    "        temp = temp / temp.sum()\n",
    "        temp = temp.to_frame()\n",
    "        temp['cluster'] = my_cluster\n",
    "        t.append(temp)\n",
    "    t = pd.concat(t)\n",
    "    t.reset_index(inplace=True,drop=False)\n",
    "    t = pd.pivot_table(t,index=cluster_col,columns='cluster',values='count',fill_value=0)\n",
    "    t = t[save_clusters]\n",
    "# New code for using a colormap\n",
    "    if t.shape[0]>10:\n",
    "        colormap = cm.get_cmap('tab20')\n",
    "    else:\n",
    "        colormap = cm.get_cmap('tab10')\n",
    "    colors = [colormap(i) for i in np.linspace(0, 1, t.shape[0])]\n",
    "\n",
    "    t.T.plot(kind='bar', stacked=True, figsize=(10, 7),ax=ax, color=colors)\n",
    "    ax.set_title('%s greater than %d percent'%(cluster_col,100*min_val))\n",
    "    legend = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Move the legend outside the plot area\n",
    "    legend.set_bbox_to_anchor((1.05, 0.5))\n",
    "    \n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# node_data.columns = ['Quant: %s'%c for c in node_data.columns]\n",
    "backup_G = G.copy()\n",
    "t = d_sample_scaled_df.copy()\n",
    "t.columns = ['-'.join(c) for c in t.columns]\n",
    "t['cluster'] = clusters\n",
    "\n",
    "nx.set_node_attributes(backup_G, t.to_dict('index'))\n",
    "nx.write_graphml(backup_G,'wavestab3-with-cluster-log2.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = d_sample_scaled_df.copy()\n",
    "t['cluster'] = clusters\n",
    "t.columns = ['-'.join(x) for x in t.columns]\n",
    "t = pd.merge(t,df,left_index=True,right_on='node_id',how='left')\n",
    "t.to_csv('clustered_nodes_wavestab3_lognorm.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = d_sample_scaled_df.copy()\n",
    "t.columns = ['-'.join(c) for c in t.columns]\n",
    "t['cluster'] = clusters\n",
    "t = t[t['cluster']==50]\n",
    "t = pd.merge(t,df,left_index=True,right_on='node_id',how='left')\n",
    "t = t['superclass_results'].value_counts() / t.shape[0]\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Melt the DataFrame to long format for seaborn\n",
    "# include the index in the melt\n",
    "\n",
    "idx = d['cluster']==1\n",
    "\n",
    "# # Create a boxplot for each cluster\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='time', y='value', hue='community-supplement',data=d[idx])\n",
    "# plt.xlabel('Cluster')\n",
    "# plt.ylabel('Value')\n",
    "# plt.title('Boxplot of Each Cluster')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['time'].unique(),out['community'].unique(),out['supplement'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataframe(out):\n",
    "    d = out.pivot_table(columns=['filename'],index=['node_id'],values='peak_area',aggfunc='mean',fill_value=300)\n",
    "    # temp = d.T\n",
    "    # temp.columns = [f\"{index[1]}-{index[0]}\" for index in temp.columns]\n",
    "    d = d.apply(shannon_diversity_index,axis=0)\n",
    "    d = d.to_frame()\n",
    "    d.reset_index(inplace=True,drop=False)\n",
    "    d.rename(columns={0:'diversity'},inplace=True)\n",
    "    d['time'] = d['filename'].apply(lambda x: x.split('/')[-1].split('_')[12].split('-')[-2])\n",
    "    d['community'] = d['filename'].apply(lambda x: x.split('/')[-1].split('_')[12].split('-')[-3])\n",
    "    d['supplement'] = d['filename'].apply(lambda x: x.split('/')[-1].split('_')[12].split('-')[-1])\n",
    "   \n",
    "    # Make treatment a categorical variable and put it in order\n",
    "    # c = ['unbdp', 'omt', 'grass', 'cmt', 'rwc']\n",
    "    # c = [cc.upper() for cc in c]\n",
    "    # d['treatment'] = pd.Categorical(d['treatment'], categories=c, ordered=True)\n",
    "    return d\n",
    "\n",
    "\n",
    "def shannon_diversity_index(m,threshold=1e6,q=2):\n",
    "    # m = m>threshold\n",
    "    # m = m**0.5\n",
    "    # probabilities = m / m.sum()\n",
    "    # shannon_diversity_index = -np.sum(probabilities * np.log2(probabilities))\n",
    "    # return shannon_diversity_index\n",
    "    probabilities = m / m.sum()\n",
    "    hill_number = np.sum(probabilities**q)**(1/(1-q))\n",
    "    return hill_number\n",
    "\n",
    "d = setup_dataframe(out)\n",
    "\n",
    "# # Plot the boxplot\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots()\n",
    "sns.boxplot(data=d, x='supplement', y='diversity', hue='time', ax=ax)\n",
    "\n",
    "# sns.boxplot(data=d, x='treatment', y='shannon_diversity_index', hue='depth', ax=ax, order=['unbdp', 'omt', 'grass', 'cmt', 'rwc'])\n",
    "\n",
    "# fig,ax = plt.subplots()\n",
    "# sns.boxplot(data=d,x='treatment',y='shannon_diversity_index',hue='depth',ax=ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pd.pivot_table(out[out['sampletype'].str.contains('UNBDP')],index='node_id',columns='filename',values='peak_area')\n",
    "# temp = out[out['supplement']=='Hi'].copy()\n",
    "# # temp['community'].unique()\n",
    "# len(pd.unique(temp['node_id']))\n",
    "out['supplement'].unique()\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "d = out.pivot_table(columns='node_id',index=['filename','time','community','supplement'],values='peak_area',aggfunc='mean',fill_value=300)\n",
    "t_cols = ['d0','d7']\n",
    "# (array(['d7', 'd0', 'NA'], dtype=object),\n",
    "#  array(['NatCom', 'Sterile', 'NA'], dtype=object),\n",
    "#  array(['Lo', 'NA', 'Hi'], dtype=object))\n",
    "\n",
    "\n",
    "comparisons = [{'supplement':'Hi','community':'NatCom'},\n",
    "                {'supplement':'Lo','community':'NatCom'},\n",
    "                {'supplement':'Hi','community':'Sterile'},\n",
    "                {'supplement':'NA','community':'Sterile'},\n",
    "                {'supplement':'NA','community':'NatCom'}]\n",
    "\n",
    "for comp in comparisons:\n",
    "    print(comp)\n",
    "    idx1 = d.index.get_level_values('time').isin(t_cols)\n",
    "    idx2 = d.index.get_level_values('community')==comp['community']\n",
    "    idx3 = d.index.get_level_values('supplement')==comp['supplement']\n",
    "    temp = d[(idx1) & (idx2) & (idx3)].copy()\n",
    "\n",
    "    stats_df = []\n",
    "\n",
    "    for c in temp.columns:\n",
    "        v = temp[c].values\n",
    "        g = temp.index.get_level_values('time').values\n",
    "        t_score, p_value = stats.ttest_ind(v[g=='d0'], v[g=='d7'])\n",
    "        d7_mean = np.mean(v[g=='d7'])\n",
    "        d0_mean = np.mean(v[g=='d0'])\n",
    "        fold_change = np.log2((1+d7_mean)/(1+d0_mean))\n",
    "        stats_df.append({'node_id':c,\n",
    "                        'p_value':p_value,\n",
    "                        't_score':t_score,\n",
    "                        'fold_change':fold_change,\n",
    "                        'd0_mean':d0_mean,\n",
    "                        'd7_mean':d7_mean})\n",
    "        \n",
    "    stats_df = pd.DataFrame(stats_df)\n",
    "    stats_df.to_csv('supplement-%s_community-%s.csv'%(comp['supplement'],comp['community']),index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "stats_df['max_intensity'] = stats_df[['d0_mean','d7_mean']].max(axis=1)\n",
    "stats_df['max_intensity'] = stats_df['max_intensity'].apply(np.log10)\n",
    "# Set the significance threshold for p-values\n",
    "# significance_threshold = 0.05\n",
    "intensity_threshold = 7\n",
    "# Create the volcano plot\n",
    "# idx = stats_df['p_value'] >= significance_threshold\n",
    "# plt.scatter(stats_df['fold_change'], -1 * np.log10(stats_df['p_value']), c=stats_df['max_intensity'], alpha=1)\n",
    "idx = stats_df['max_intensity'] > intensity_threshold\n",
    "plt.scatter(stats_df[idx]['fold_change'],\n",
    "            -1 * np.log10(stats_df[idx]['p_value']),\n",
    "            c=stats_df[idx]['max_intensity'], alpha=1)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('log2 Fold Change')\n",
    "plt.ylabel('-log10 p-value')\n",
    "plt.title('Volcano Plot')\n",
    "\n",
    "# Add a horizontal line for the significance threshold\n",
    "plt.axhline(-1 * np.log10(significance_threshold), color='black', linestyle='--')\n",
    "plt.axvline(0, color='black', linestyle='--')\n",
    "plt.colorbar(label='log10(max_intensity)')\n",
    "plt.clim(intensity_threshold, 10)\n",
    "plt.xlim(-20,20)\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "from scipy import stats\n",
    "\n",
    "d = out[out['time']=='d7'].pivot_table(columns='node_id',index=['filename','time'],values='peak_area',aggfunc='mean',fill_value=300)\n",
    "# d.reset_index(inplace=True,drop=False)\n",
    "# d.shape,len(cluster_strings),Z.shape\n",
    "# Perform t-test for each column in d\n",
    "# ttest_results = d.apply(lambda x: pg.pairwise_ttests(x, group=d.index.get_level_values('time')))\n",
    "cols = d.columns\n",
    "\n",
    "# pg.pairwise_ttests(d[cols[0]])#, group=d.index.get_level_values('time'))\n",
    "# Print the t-test results\n",
    "# print(ttest_results)\\\n",
    "\n",
    "p_values = []\n",
    "t_scores = []\n",
    "fold_changes = []\n",
    "\n",
    "for c in cols:\n",
    "    v = d[c].values\n",
    "    g = d.index.get_level_values('time').values\n",
    "    t_score, p_value = stats.ttest_ind(v[g=='d0'], v[g=='d7'])\n",
    "    fold_change = np.log2((1+np.mean(v[g=='d7']))/(1+np.mean(v[g=='d0'])))\n",
    "    p_values.append(p_value)\n",
    "    t_scores.append(t_score)\n",
    "    fold_changes.append(fold_change)\n",
    "\n",
    "stats_df = pd.DataFrame(index=d.T.index,data={'p_value':p_values,'t_score':t_scores,'fold_change':fold_changes})\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "from scipy import stats\n",
    "\n",
    "# d = out[out['supplement']=='Hi'].pivot_table(columns='node_id',index=['filename','time'],values='peak_area',aggfunc='mean',fill_value=300)\n",
    "d = out[out['time']=='d7'].pivot_table(columns='node_id',index=['filename','supplement'],values='peak_area',aggfunc='mean',fill_value=300)\n",
    "\n",
    "# d.reset_index(inplace=True,drop=False)\n",
    "# d.shape,len(cluster_strings),Z.shape\n",
    "# Perform t-test for each column in d\n",
    "# ttest_results = d.apply(lambda x: pg.pairwise_ttests(x, group=d.index.get_level_values('time')))\n",
    "cols = d.columns\n",
    "\n",
    "# pg.pairwise_ttests(d[cols[0]])#, group=d.index.get_level_values('time'))\n",
    "# Print the t-test results\n",
    "# print(ttest_results)\\\n",
    "\n",
    "p_values = []\n",
    "t_scores = []\n",
    "fold_changes = []\n",
    "\n",
    "for c in cols:\n",
    "    v = d[c].values\n",
    "    g = d.index.get_level_values('supplement').values\n",
    "    t_score, p_value = stats.ttest_ind(v[g=='NA'], v[g=='Hi'])\n",
    "    fold_change = np.log2((1+np.mean(v[g=='Hi']))/(1+np.mean(v[g=='NA'])))\n",
    "    p_values.append(p_value)\n",
    "    t_scores.append(t_score)\n",
    "    fold_changes.append(fold_change)\n",
    "\n",
    "stats_df = pd.DataFrame(index=d.T.index,data={'p_value':p_values,'t_score':t_scores,'fold_change':fold_changes})\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(stats_df['fold_change'],-1*np.log10(stats_df['p_value']),'.')\n",
    "ax.set_xlabel('fold change')\n",
    "ax.set_ylabel('-log10(p-value)')\n",
    "ax.axhline(-1*np.log10(0.05),color='r',linestyle='--')\n",
    "ax.axvline(-1,color='r',linestyle='--')\n",
    "ax.axvline(+1,color='r',linestyle='--')\n",
    "# ax.set_xlim(-16,16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "from scipy import stats\n",
    "\n",
    "d = out[out['supplement']=='Hi'].pivot_table(columns='node_id',index=['filename','time'],values='peak_area',aggfunc='mean',fill_value=300)\n",
    "# d.reset_index(inplace=True,drop=False)\n",
    "# d.shape,len(cluster_strings),Z.shape\n",
    "# Perform t-test for each column in d\n",
    "# ttest_results = d.apply(lambda x: pg.pairwise_ttests(x, group=d.index.get_level_values('time')))\n",
    "cols = d.columns\n",
    "\n",
    "# pg.pairwise_ttests(d[cols[0]])#, group=d.index.get_level_values('time'))\n",
    "# Print the t-test results\n",
    "# print(ttest_results)\\\n",
    "\n",
    "p_values = []\n",
    "t_scores = []\n",
    "fold_changes = []\n",
    "\n",
    "for c in cols:\n",
    "    v = d[c].values\n",
    "    g = d.index.get_level_values('time').values\n",
    "    t_score, p_value = stats.ttest_ind(v[g=='d0'], v[g=='d7'])\n",
    "    fold_change = np.log2((1+np.mean(v[g=='d7']))/(1+np.mean(v[g=='d0'])))\n",
    "    p_values.append(p_value)\n",
    "    t_scores.append(t_score)\n",
    "    fold_changes.append(fold_change)\n",
    "\n",
    "stats_df_nldm = pd.DataFrame(index=d.T.index,data={'p_value':p_values,'t_score':t_scores,'fold_change':fold_changes})\n",
    "stats_df_nldm.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = out.pivot_table(columns='node_id',index=['filename','sampletype','supplement','time'],values='peak_area',aggfunc='mean',fill_value=300)\n",
    "cols = d.columns\n",
    "d.reset_index(inplace=True,drop=False)\n",
    "d = d.groupby(['sampletype','supplement','time'])[cols].mean()\n",
    "d = d.T\n",
    "d.columns = [f\"{index[0]}:{index[1]}:{index[2]}\" for index in d.columns]\n",
    "d = pd.merge(d,stats_df.add_suffix('_sterile-vs-Hi'),left_index=True,right_index=True)\n",
    "d = pd.merge(d,stats_df_nldm.add_suffix('_Hi-d0-vs-Hi-d7'),left_index=True,right_index=True)\n",
    "d.to_csv('Pairwise-ttests-sterile-vs-Hi_with_NLDM-0-vs-7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "from scipy import stats\n",
    "\n",
    "d = out[out['supplement']!='Hi'].pivot_table(columns='node_id',index=['filename','supplement'],values='peak_area',aggfunc='mean',fill_value=300)\n",
    "# d.reset_index(inplace=True,drop=False)\n",
    "# d.shape,len(cluster_strings),Z.shape\n",
    "# Perform t-test for each column in d\n",
    "# ttest_results = d.apply(lambda x: pg.pairwise_ttests(x, group=d.index.get_level_values('time')))\n",
    "cols = d.columns\n",
    "\n",
    "# pg.pairwise_ttests(d[cols[0]])#, group=d.index.get_level_values('time'))\n",
    "# Print the t-test results\n",
    "# print(ttest_results)\\\n",
    "\n",
    "p_values = []\n",
    "t_scores = []\n",
    "fold_changes = []\n",
    "\n",
    "for c in cols:\n",
    "    v = d[c].values\n",
    "    g = d.index.get_level_values('supplement').values\n",
    "    t_score, p_value = stats.ttest_ind(v[g=='Day0'], v[g=='Day7'])\n",
    "    fold_change = np.log2((1+np.mean(v[g=='Day7']))/(1+np.mean(v[g=='Day0'])))\n",
    "    p_values.append(p_value)\n",
    "    t_scores.append(t_score)\n",
    "    fold_changes.append(fold_change)\n",
    "\n",
    "stats_df_potting = pd.DataFrame(index=d.T.index,data={'p_value':p_values,'t_score':t_scores,'fold_change':fold_changes})\n",
    "stats_df_potting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df[(stats_df['p_value']<0.05) & (stats_df['fold_change']<-0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(stats_df['fold_change'],-1*np.log10(stats_df['p_value']),'.')\n",
    "ax.set_xlabel('fold change')\n",
    "ax.set_ylabel('-log10(p-value)')\n",
    "ax.axhline(-1*np.log10(0.05),color='r',linestyle='--')\n",
    "ax.axvline(-1,color='r',linestyle='--')\n",
    "ax.axvline(+1,color='r',linestyle='--')\n",
    "# ax.set_xlim(-16,16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(stats_df_potting['fold_change'],-1*np.log10(stats_df_potting['p_value']),'.')\n",
    "ax.set_xlabel('fold change')\n",
    "ax.set_ylabel('-log10(p-value)')\n",
    "ax.axhline(-1*np.log10(0.05),color='r',linestyle='--')\n",
    "ax.axvline(-1,color='r',linestyle='--')\n",
    "ax.axvline(+1,color='r',linestyle='--')\n",
    "# ax.set_xlim(-16,16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = stats_df_potting.add_suffix('_potting').join(stats_df.add_suffix('_omt'),how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_graphml('../data/network.graphml')\n",
    "df = dict(G.nodes(data=True))\n",
    "df = pd.DataFrame(df).T\n",
    "df.index.name = 'node_id'\n",
    "df = pd.merge(df,stats_df,left_index=True,right_index=True,how='inner')\n",
    "df.to_csv('network_data_with_stats_RR-OMT-SunshineMix.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "numeric_df = df.select_dtypes(include='number')\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "correlation_matrix[['fold_change_omt','fold_change_potting']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.to_csv('RR_and_Sunshine_priming_wavestab3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# node_data.columns = ['Quant: %s'%c for c in node_data.columns]\n",
    "backup_G = G.copy()\n",
    "nx.set_node_attributes(backup_G, stats_df.to_dict('index'))\n",
    "nx.write_graphml(backup_G,'../data/network_RR-Sunshine-priming-WaveStab3-Stats.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "# d = setup_dataframe(out)\n",
    "d = out.pivot_table(index='node_id',columns=['time','community','supplement'],values='peak_area',aggfunc='mean',fill_value=300)\n",
    "d.columns = ['-'.join(index) for index in d.columns]\n",
    "\n",
    "cluster_strings = list(d.columns)#[os.path.basename(c).split('_')[12] for c in d.index]\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "Z = linkage(d.T.apply(lambda x: x**0.5), method='ward', metric='euclidean')\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(20, 3))\n",
    "\n",
    "clusters = fcluster(Z, 4e4, criterion='distance')\n",
    "# d['cluster'] = clusters\n",
    "# cluster_strings = [f\"{index[1]}-{index[0]} Cluster {cluster}\" for index, cluster in zip(d.index, d['cluster'])]\n",
    "# cluster_strings = [f\"{index} Cluster {cluster}\" for index, cluster in zip(d.index, d['cluster'])]\n",
    "\n",
    "dendrogram(Z, labels=cluster_strings, leaf_font_size=14,color_threshold=4e4,)\n",
    "# Set plot title and labels\n",
    "# plt.title(\"Dendrogram with Depth and Environment Labels\")\n",
    "# plt.xlabel(\"Samples\", fontsize=14)\n",
    "plt.ylabel(\"Distance\", fontsize=14)\n",
    "\n",
    "# Set y tick label fontsize\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Rotate x-axis labels for better visibility\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import Draw\n",
    "# mz = nx.get_node_attributes(G, 'precursor_mz')\n",
    "# smiles_identity = nx.get_node_attributes(G, 'smiles_identity')\n",
    "# temp = d.T.copy()\n",
    "# c1 = 'soil-CentExp-OMT1d2-1130buck-50g'\n",
    "# c2 = 'soil-Sunshine-Mix4-NA-water'\n",
    "# temp['fold_change'] = np.log2(temp[c1]/temp[c2])\n",
    "# temp = pd.merge(temp,pd.DataFrame({'precursor_mz':mz}),left_index=True,right_index=True)\n",
    "\n",
    "# temp = pd.merge(temp,pd.DataFrame({'smiles_identity':smiles_identity}),left_index=True,right_index=True)\n",
    "# p = temp[pd.notna(temp['smiles_identity'])]\n",
    "\n",
    "# p.sort_values('fold_change',ascending=False,inplace=True)\n",
    "# top_10 = p.head(20)\n",
    "# bottom_10 = p.tail(20)\n",
    "\n",
    "# top_mols = [Chem.MolFromSmiles(smiles) for smiles in top_10['smiles_identity']]\n",
    "# bottom_mols = [Chem.MolFromSmiles(smiles) for smiles in bottom_10['smiles_identity']]\n",
    "# mols = top_mols + bottom_mols\n",
    "# image = Draw.MolsToGridImage(mols, subImgSize=(200,200),molsPerRow=10,maxMols=1000,useSVG=True)\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mz = nx.get_node_attributes(G, 'precursor_mz')\n",
    "# smiles_identity = nx.get_node_attributes(G, 'smiles_identity')\n",
    "\n",
    "# temp = d.T.copy()\n",
    "# c1 = 'soil-CentExp-OMT1d2-1130buck-50g'\n",
    "# c2 = 'soil-Sunshine-Mix4-NA-water'\n",
    "# temp['fold_change'] = np.log2(temp[c1]/temp[c2])\n",
    "\n",
    "# temp = pd.merge(temp,pd.DataFrame({'precursor_mz':mz}),left_index=True,right_index=True)\n",
    "# temp = pd.merge(temp,pd.DataFrame({'smiles_identity':smiles_identity}),left_index=True,right_index=True)\n",
    "# # p = d[pd.notna(d['smiles_identity'])]\n",
    "# p = temp.copy()\n",
    "# fig,ax = plt.subplots(ncols=1,nrows=1,figsize=(15,5),sharey=True,sharex=True)\n",
    "# ax.vlines(p['precursor_mz'], 0, p[c1]**0.5)\n",
    "# ax.vlines(p['precursor_mz'], 0, -1*(p[c2]**0.5))\n",
    "# # ax[1].vlines(p['precursor_mz'], 0, p['fold_change'])\n",
    "# # ax[2].vlines(p['precursor_mz'], 0, ymax=-1*(p[2]**0.5))\n",
    "\n",
    "# # ax.set_ylim(-50000,50000)\n",
    "# ax.grid(True)\n",
    "# ax.set_xlim(50,650)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mz = nx.get_node_attributes(G, 'precursor_mz')\n",
    "# smiles_identity = nx.get_node_attributes(G, 'smiles_identity')\n",
    "\n",
    "# temp = d.T.copy()\n",
    "# c1 = 'soil-CentExp-OMT1d2-1130buck-50g'\n",
    "# c2 = 'soil-Sunshine-Mix4-NA-water'\n",
    "# temp['fold_change'] = np.log2(temp[c1]/temp[c2])\n",
    "\n",
    "# temp = pd.merge(temp,pd.DataFrame({'precursor_mz':mz}),left_index=True,right_index=True)\n",
    "# temp = pd.merge(temp,pd.DataFrame({'smiles_identity':smiles_identity}),left_index=True,right_index=True)\n",
    "# # p = d[pd.notna(d['smiles_identity'])]\n",
    "# p = temp.copy()\n",
    "# fig,ax = plt.subplots(ncols=1,nrows=2,figsize=(15,11),sharey=True,sharex=True)\n",
    "# ax[0].vlines(p['precursor_mz'], 0, p['fold_change'])\n",
    "\n",
    "\n",
    "# p = temp[pd.notna(temp['smiles_identity'])]\n",
    "# ax[1].vlines(p['precursor_mz'], 0, p['fold_change'])\n",
    "# # ax[2].vlines(p['precursor_mz'], 0, ymax=-1*(p[2]**0.5))\n",
    "\n",
    "\n",
    "# # for a in ax:\n",
    "#     # a.set_xlim(270,330)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcs_structural_cluster_number = nx.get_node_attributes(G, 'mcs_structural_cluster_number')\n",
    "smiles_identity = nx.get_node_attributes(G, 'smiles_identity')\n",
    "\n",
    "temp = pd.DataFrame({'cluster':mcs_structural_cluster_number})\n",
    "temp = pd.merge(temp,pd.DataFrame({'smiles':smiles_identity}),left_index=True,right_index=True)\n",
    "temp.fillna('',inplace=True)\n",
    "sig_out = []\n",
    "val_out = []\n",
    "for i in temp['cluster'].unique():\n",
    "    df = pd.merge(out,temp,left_on='node_id',right_index=True)\n",
    "    # df = df[df['depth']!='litter']\n",
    "    # df = df[df['depth']!='b']\n",
    "    # df = df[(df['environment']=='coniferousforests') | (df['environment']=='grasses') ]\n",
    "    df = df[df['cluster']==i]\n",
    "    cols = ['community','time','supplement','peak_area','filename','node_id']\n",
    "    df = df[cols]\n",
    "    df = df[df['community']=='NatCom']\n",
    "    # df = df[df['time']=='1d2']\n",
    "    df = df[df['supplement']=='Hi']\n",
    "    # filename_mapping = {f: i+1 for i, f in enumerate(df['filename'].unique())}\n",
    "    # df['filename'] = df['filename'].map(filename_mapping)\n",
    "\n",
    "    # node_id_mapping = {f: i+1 for i, f in enumerate(df['node_id'].unique())}\n",
    "    # df['node_id'] = df['node_id'].map(node_id_mapping)\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.pivot_table(df,values='peak_area',index=['filename','time'],columns=['node_id'],fill_value=300)\n",
    "    df = df.mean(axis=1).to_frame()\n",
    "    df.reset_index(inplace=True,drop=False)\n",
    "    df.rename(columns={0:'peak_area'},inplace=True)\n",
    "    anova_result = pg.anova(data=df, dv='peak_area', between=['time'])#, subject='filename')\n",
    "    anova_result['structural_cluster'] = i\n",
    "    sig_out.append(anova_result)\n",
    "    # THERE IS A PROBLEM HERE.  OUT SHOULD BE FILTERED TO ONLY STRUCTURAL CLUSTERS\n",
    "    v = out.groupby(['community','supplement','time'])['peak_area'].mean().to_frame().reset_index()\n",
    "    v['structural_cluster'] = i\n",
    "    val_out.append(v)\n",
    "sig_out = pd.concat(sig_out)\n",
    "val_out = pd.concat(val_out)\n",
    "sig_out = pd.pivot_table(sig_out,values='p-unc',index=['structural_cluster'],columns=['Source'])\n",
    "sig_out = sig_out[sig_out.min(axis=1)<0.05]\n",
    "val_out = val_out.pivot_table(index='structural_cluster',columns=['community','supplement','time'],values='peak_area')\n",
    "val_out.columns = ['_'.join(c) for c in val_out.columns]\n",
    "val_out.reset_index(inplace=True,drop=False)\n",
    "sig_out = pd.merge(sig_out,val_out,left_index=True,right_on='structural_cluster',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_data = pd.pivot_table(out,index=['node_id'],columns=['sampletype','depth'],values='peak_area',aggfunc=np.mean,fill_value=300)\n",
    "# node_data.columns = ['_'.join(c) for c in node_data.columns]\n",
    "\n",
    "# node_data.columns = ['Quant: %s'%c for c in node_data.columns]\n",
    "# backup_G = G.copy()\n",
    "# nx.set_node_attributes(backup_G, node_data.to_dict('index'))\n",
    "# # nx.write_graphml(backup_G,'../data/network_omt-potting.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data = pd.pivot_table(out[out['supplement']=='Hi'],index=['node_id'],columns=['filename','community','time','supplement'],values='peak_area',aggfunc=np.mean,fill_value=300)\n",
    "node_data.columns = ['-'.join(c) for c in node_data.columns]\n",
    "\n",
    "# node_data = node_data.apply(lambda  x: np.log10(x+1),axis=1)\n",
    "# s = node_data.sum(axis=0).values\n",
    "# m = s.mean()\n",
    "# node_data.values[:,:] = m * (node_data.values[:,:]/s)\n",
    "# node_data.columns = [c.split('/')[-1].split('_')[12] for c in node_data.columns]\n",
    "\n",
    "# node_data.to_csv('../data/log10_averages_treatments.csv')\n",
    "node_data.columns = ['Quant: %s'%c for c in node_data.columns]\n",
    "\n",
    "n = node_data.copy()\n",
    "m = n.min(axis=1)\n",
    "m = n.values - m.values[:,None]\n",
    "n = pd.DataFrame(m,index=n.index,columns=n.columns)\n",
    "\n",
    "m = n.max(axis=1)\n",
    "m = n.values / m.values[:,None]\n",
    "n = pd.DataFrame(m,index=n.index,columns=n.columns)\n",
    "\n",
    "n = n.T\n",
    "n.index.name = 'group'\n",
    "n.reset_index(inplace=True,drop=False)\n",
    "n['group'] = n['group'].apply(lambda x: '-'.join(x.split('-')[-3:]))\n",
    "n = n.groupby('group').mean()\n",
    "n = n.T\n",
    "n.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data = pd.pivot_table(out,index=['node_id'],columns=['filename','community','time','supplement'],values='peak_area',aggfunc=np.mean,fill_value=300)\n",
    "# node_data = pd.pivot_table(out[out['supplement']=='Hi'],index=['node_id'],columns=['filename','community','time','supplement'],values='peak_area',aggfunc=np.mean,fill_value=300)\n",
    "node_data.columns = ['-'.join(c) for c in node_data.columns]\n",
    "\n",
    "# node_data = node_data.apply(lambda  x: np.log10(x+1),axis=1)\n",
    "# s = node_data.sum(axis=0).values\n",
    "# m = s.mean()\n",
    "# node_data.values[:,:] = m * (node_data.values[:,:]/s)\n",
    "# node_data.columns = [c.split('/')[-1].split('_')[12] for c in node_data.columns]\n",
    "\n",
    "# node_data.to_csv('../data/log10_averages_treatments.csv')\n",
    "node_data.columns = ['Quant: %s'%c for c in node_data.columns]\n",
    "\n",
    "n = node_data.copy()\n",
    "m = n.min(axis=1)\n",
    "m = n.values - m.values[:,None]\n",
    "n = pd.DataFrame(m,index=n.index,columns=n.columns)\n",
    "\n",
    "m = n.max(axis=1)\n",
    "m = n.values / m.values[:,None]\n",
    "n = pd.DataFrame(m,index=n.index,columns=n.columns)\n",
    "\n",
    "n = n.T\n",
    "n.index.name = 'group'\n",
    "n.reset_index(inplace=True,drop=False)\n",
    "n['group'] = n['group'].apply(lambda x: 'Quant: %s'%('-'.join(x.split('-')[-3:])))\n",
    "n = n.groupby('group').mean()\n",
    "n = n.T\n",
    "\n",
    "original_index = nx.get_node_attributes(G, 'original_index')\n",
    "mcs_structural_cluster_number = nx.get_node_attributes(G, 'mcs_structural_cluster_number')\n",
    "mcs_structural_pattern = nx.get_node_attributes(G, 'mcs_structural_pattern')\n",
    "smiles_identity = nx.get_node_attributes(G, 'smiles_identity')\n",
    "# temp = {}\n",
    "# for c in n.columns:\n",
    "    # temp[c] = nx.get_node_attributes(backup_G, c)\n",
    "\n",
    "temp = pd.merge(n,pd.DataFrame({'cluster_id':mcs_structural_cluster_number}),left_index=True,right_index=True)\n",
    "temp = pd.merge(temp,pd.DataFrame({'cluster_pattern':mcs_structural_pattern}),left_index=True,right_index=True)\n",
    "temp = pd.merge(temp,pd.DataFrame({'smiles_identity':smiles_identity}),left_index=True,right_index=True)\n",
    "cols = [c for c in temp.columns if 'Quant' in c]\n",
    "cluster_df = temp.groupby(['cluster_id','cluster_pattern'])[cols].mean()\n",
    "identity_df = temp.groupby(['cluster_id','cluster_pattern','smiles_identity'])[cols].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cols = identity_df.columns\n",
    "c = 0\n",
    "x_labels = [col.replace('Quant: ','') for col in cols]\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "def show_mol(d2d,mol,legend='',highlightAtoms=[]):\n",
    "    d2d.DrawMolecule(mol)#,legend=legend, highlightAtoms=highlightAtoms)\n",
    "    d2d.FinishDrawing()\n",
    "    bio = BytesIO(d2d.GetDrawingText())\n",
    "    return Image.open(bio)\n",
    "\n",
    "num_unique_clusters = identity_df.index.get_level_values('cluster_id').nunique()\n",
    "print(num_unique_clusters)\n",
    "n_cols = 4\n",
    "n_rows = int(np.ceil(num_unique_clusters/n_cols))\n",
    "\n",
    "fig,ax = plt.subplots(nrows=n_rows,ncols=4,figsize=(n_rows*4,n_cols*14),sharex=True,sharey=False)\n",
    "                      \n",
    "ax = ax.flatten()\n",
    "for _,d in identity_df.reset_index(drop=False).groupby('cluster_id'):\n",
    "    \n",
    "    sns.heatmap(d[cols].values,ax=ax[c],cmap='viridis')\n",
    "    ax[c].set_title('Cluster %d'%d.iloc[0]['cluster_id'])\n",
    "    counter = 0\n",
    "    for i, row in d.iterrows():\n",
    "\n",
    "        mol = Chem.MolFromSmiles(row['smiles_identity'])\n",
    "        d2d = Draw.MolDraw2DCairo(120, 120)\n",
    "        dopts = d2d.drawOptions()\n",
    "        dopts.setBackgroundColour((0, 0, 0, 0))\n",
    "        im = show_mol(d2d, mol)\n",
    "\n",
    "        imagebox = OffsetImage(im, zoom=0.8)\n",
    "        ax[c].add_artist(AnnotationBbox(imagebox, (-1, counter+0.5), frameon=False,annotation_clip=False))#,box_alignment=(0,0)))\n",
    "        \n",
    "        counter += 1\n",
    "    c += 1\n",
    "    # if c>6:\n",
    "        # break\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "# Set the x-axis labels for the bottom row of subplots\n",
    "for i in range(n_cols):\n",
    "    ax[-n_cols + i].set_xticklabels(x_labels, rotation=45, ha='right', fontsize=14)\n",
    "\n",
    "# Set the x-axis label for the entire figure\n",
    "# fig.text(0.5, 0.04, 'Identity', ha='center', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('../data/identity_heatmap_RR-Sunshine-priming-WaveStab3.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# Calculate the pairwise distances between rows\n",
    "# cols = [c for c in cluster_df.columns if ('deciduousforests' in c) | ('coniferousforests' in c)]\n",
    "distances = cluster_df.values\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "Z = linkage(distances, method='average', metric='euclidean')\n",
    "\n",
    "# Get the order of the rows based on the clustering\n",
    "order = dendrogram(Z, no_plot=True)['leaves']\n",
    "\n",
    "# Reorder the rows of cluster_df\n",
    "cluster_df = cluster_df.iloc[order]\n",
    "\n",
    "# Show the reordered cluster_df\n",
    "# cluster_df[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "# Create the polar plot\n",
    "def make_polar_structural_cluster_plot(cluster_df,file_str=''):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 15), subplot_kw={'projection': 'polar'})\n",
    "\n",
    "    # Define the angles for each side of the polygons\n",
    "    angles = np.linspace(0, 2 * np.pi, cluster_df.shape[0] + 1)[:-1]\n",
    "    shift_amount = np.diff(angles)[0] / cluster_df.shape[1]\n",
    "    for iii in range(cluster_df.shape[1]):\n",
    "        # Define the lengths of the bars\n",
    "        bar_lengths = cluster_df.values[:, iii]\n",
    "        # Plot the bars\n",
    "        ax.bar(angles + iii * shift_amount, bar_lengths*1, width=shift_amount, align='edge', alpha=0.74, label=cluster_df.columns[iii].replace('Quant: ',''))\n",
    "\n",
    "    # Set the labels for each side of the polygons\n",
    "    ax.set_xticks(angles)\n",
    "    ax.set_xticklabels(['' for i in range(cluster_df.shape[0])])\n",
    "\n",
    "    # Set the title of the plot\n",
    "    tick_labels = ax.xaxis.get_ticklabels()\n",
    "\n",
    "    m = ax.get_ylim()[1]*1.1\n",
    "    ax.set_ylim(0, m)\n",
    "\n",
    "    from PIL import Image\n",
    "    from io import BytesIO\n",
    "    def show_mol(d2d,mol,legend='',highlightAtoms=[]):\n",
    "        d2d.DrawMolecule(mol)#,legend=legend, highlightAtoms=highlightAtoms)\n",
    "        d2d.FinishDrawing()\n",
    "        bio = BytesIO(d2d.GetDrawingText())\n",
    "        return Image.open(bio)\n",
    "\n",
    "    counter = 0\n",
    "    for i, row in cluster_df.iterrows():\n",
    "        mol = Chem.MolFromSmarts(i[1])\n",
    "        s = Chem.MolToSmiles(mol)\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        d2d = Draw.MolDraw2DCairo(120, 120)\n",
    "        dopts = d2d.drawOptions()\n",
    "        dopts.setBackgroundColour((0, 0, 0, 0))\n",
    "        im = show_mol(d2d, mol)\n",
    "\n",
    "        imagebox = OffsetImage(im, zoom=0.8)\n",
    "        ax.add_artist(AnnotationBbox(imagebox, (angles[counter] + shift_amount*cluster_df.shape[1]/2, m * 1), frameon=False))\n",
    "        # if np.cos(angles[counter]) < 0:\n",
    "        # my_angle = angles[counter] + shift_amount*2 - np.pi\n",
    "        # else:\n",
    "        my_angle = angles[counter] + shift_amount* cluster_df.shape[1]/2\n",
    "        # my_angle = angles[counter] + shift_amount*2\n",
    "        my_angle = my_angle * 180/np.pi - 90\n",
    "        ax.text(angles[counter] + shift_amount*cluster_df.shape[1]/2, m * 1.11, 'Cluster %d'%i[0], ha='center', va='center', fontsize=16, rotation=my_angle)#angles[counter]*180/4/np.pi)\n",
    "\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    ax.spines['polar'].set_visible(False)\n",
    "    ax.grid(color='black')\n",
    "    # Hide the y tick labels\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    # Position the legend outside of the plot area\n",
    "    ax.legend(bbox_to_anchor=(0.85, 1.1), loc='upper left',fontsize=16,frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig('lcms-structural_cluster_plot_%s.pdf'%file_str)#, dpi=300)\n",
    "\n",
    "# cols = [c for c in cluster_df.columns if ('deciduousforests' in c) | ('coniferousforests' in c)]\n",
    "# cols = [c for c in cluster_df.columns if ('OMT' in c) | ('CMT' in c)]\n",
    "cols = [c for c in cluster_df.columns if ('Quant' in c)]\n",
    "# cols = [c for c in cluster_df.columns if ('UNBDP' in c) | ('OMT' in c)]\n",
    "make_polar_structural_cluster_plot(cluster_df[cols],'Century-Sunshine-Priming-Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
